<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.290">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mark Spillman">
<meta name="dcterms.date" content="2024-02-11">
<meta name="description" content="Using convolutional neural networks to index PXRD data">

<title>Mark Spillman - Indexing PXRD data with Convolutional Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-EVMREB130B"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-EVMREB130B', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Mark Spillman</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../publications.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../subscribe.html" rel="" target="">
 <span class="menu-text">Subscribe</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mspillman" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/mspillman" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Indexing PXRD data with Convolutional Neural Networks</h1>
                  <div>
        <div class="description">
          Using convolutional neural networks to index PXRD data
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">PXRD</div>
                <div class="quarto-category">Machine learning</div>
                <div class="quarto-category">Synthetic data</div>
                <div class="quarto-category">Indexing</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Mark Spillman </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 11, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#replicating-results" id="toc-replicating-results" class="nav-link" data-scroll-target="#replicating-results">Replicating results</a></li>
  <li><a href="#training-a-convnext-v2-model" id="toc-training-a-convnext-v2-model" class="nav-link" data-scroll-target="#training-a-convnext-v2-model">Training a ConvNeXt v2 model</a></li>
  <li><a href="#replacing-the-linear-layers-with-transformer-blocks" id="toc-replacing-the-linear-layers-with-transformer-blocks" class="nav-link" data-scroll-target="#replacing-the-linear-layers-with-transformer-blocks">Replacing the linear layers with Transformer blocks</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>In a <a href="https://mspillman.github.io/blog/posts/2023-09-08-Generating-synthetic-PXRD-data.html">previous post</a>, I covered how to make relatively realistic looking PXRD data from the pre-processed Miller indices, d-spacings and intensity values stored in the <a href="https://www.researchgate.net/publication/273507012_QUALX20_A_qualitative_phase_analysis_software_using_the_freely_available_database_POW-COD">PowCod database</a>. We ended up with <a href="https://github.com/mspillman/powcodgen/">some code</a> that can be used to rapidly generate synthetic PXRD data using a GPU, via the PyTorch library.</p>
<p>In this post, we’ll make use of this code to train convolutional neural networks (CNNs) to determine the lattice parameters directly from the PXRD data. This has been done before, notably by <a href="https://pubs.acs.org/doi/10.1021/jp0310596">Habershorn et al in 2004</a> and more recently in this open access paper by <a href="https://scripts.iucr.org/cgi-bin/paper?vb5020">Chitturi et al</a> in 2021.</p>
<p>As such, a good starting point will be to replicate the results by Chitturi and coworkers before trying more modern neural network architectures to see if we can improve the performance a bit!</p>
<p>Let’s start by loading the data, and taking a look at how things stack up against the dataset used by Chitturi et al.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> nn</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> Dataset, DataLoader</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>base_name <span class="op">=</span> <span class="st">"4-44-CuKa1-data_4000_"</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>root_dir <span class="op">=</span> <span class="st">"./"</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>cs <span class="op">=</span> np.load(root_dir<span class="op">+</span>base_name<span class="op">+</span><span class="st">"crystal_systems_numeric.npy"</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>cs_key <span class="op">=</span>  {</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Cubic"</span> : <span class="dv">0</span>,</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Hexagonal"</span> :  <span class="dv">1</span>,</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Monoclinic"</span> : <span class="dv">2</span>,</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Orthorhombic"</span> : <span class="dv">3</span>,</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Tetragonal"</span> : <span class="dv">4</span>,</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Triclinic"</span> : <span class="dv">5</span>,</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Trigonal (hexagonal axes)"</span> : <span class="dv">6</span>,</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Trigonal (rhombohedral axes)"</span> : <span class="dv">7</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>crystal_systems <span class="op">=</span> np.unique(cs, return_counts<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>plt.bar(height<span class="op">=</span>crystal_systems[<span class="dv">1</span>], x<span class="op">=</span>[<span class="ss">f"</span><span class="sc">{</span>x<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="dv">100</span><span class="op">*</span><span class="bu">float</span>(crystal_systems[<span class="dv">1</span>][i])<span class="op">/</span>cs<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">%)"</span> <span class="cf">for</span> i, x <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">list</span>(cs_key.keys()))])</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">75</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f"POW-COD data : </span><span class="sc">{</span><span class="bu">len</span>(cs)<span class="sc">}</span><span class="ss"> entries"</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>cubic_data <span class="op">=</span> np.where(cs <span class="op">==</span> <span class="dv">0</span>)[<span class="dv">0</span>]</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>hex_data <span class="op">=</span> np.where(cs <span class="op">==</span> <span class="dv">1</span>)[<span class="dv">0</span>]</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>monoclinic_data <span class="op">=</span> np.where(cs <span class="op">==</span> <span class="dv">2</span>)[<span class="dv">0</span>]</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>orthorhombic_data <span class="op">=</span> np.where(cs <span class="op">==</span> <span class="dv">3</span>)[<span class="dv">0</span>]</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>tetrag_data <span class="op">=</span> np.where(cs <span class="op">==</span> <span class="dv">4</span>)[<span class="dv">0</span>]</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>triclinic_data <span class="op">=</span> np.where(cs <span class="op">==</span> <span class="dv">5</span>)[<span class="dv">0</span>]</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Ignore the trigonal!</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>trimonoortho <span class="op">=</span> np.sort(np.hstack([monoclinic_data, orthorhombic_data, triclinic_data]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="2024-02-11-Indexing-PXRD-data-with-Convolutional-Neural-Networks_files/figure-html/cell-3-output-1.png" class="quarto-discovered-preview-image img-fluid"></p>
</div>
</div>
<p>As we saw in the last post, we have a pretty significant imbalance in the distribution of the data into the different crystal systems. This is similar to what was found in the combined ICSD and CSD data used by Chitturi, who obtained the following plot:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/Chitturi_datasets.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Chitturi et al, 2021</figcaption><p></p>
</figure>
</div>
<p>Where (a) shows the data in the ICSD, (b) shows the data in the CSD and (c) shows the combined dataset they used to train their models.</p>
<p>It also seems like we have a substantially smaller dataset, weighing in at about 200k samples, compared to about 950k samples for Chitturi. However, I suspect that the data augmentation strategies we developed in the previous post should be able to help counteract this shortfall.</p>
</section>
<section id="replicating-results" class="level1">
<h1>Replicating results</h1>
<p>Let’s have a go at replicating the results by Chitturi et al.&nbsp;They decided to train different models for each of the crystal systems in their dataset. Given the relatively small dataset I have available, I’ll focus on the three most populated (and coincidentally most complex / lowest symmetry) crystal systems: triclinic, monoclinic and orthorhombic.</p>
<p>First, we’ll need a nice to load the data - let’s make a pytorch Dataset object to allow us to easily load the data. We can then easily define a training, and validation split from this. I’ve also added the option to specify indices of interest, which will allow us to pull out the specific crystal systems we want to train on.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DiffractionData(Dataset):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,base_name, root_dir<span class="op">=</span><span class="st">"./"</span>, idx<span class="op">=</span><span class="va">None</span>, dtype<span class="op">=</span>torch.float32):</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.crystal_system_numeric <span class="op">=</span> torch.tensor(np.load(root_dir<span class="op">+</span>base_name<span class="op">+</span><span class="st">"crystal_systems_numeric.npy"</span>)[idx], dtype<span class="op">=</span>dtype)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hkl                    <span class="op">=</span> torch.tensor(np.load(root_dir<span class="op">+</span>base_name<span class="op">+</span><span class="st">"hkl.npy"</span>)[idx], dtype<span class="op">=</span>dtype)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.intensities            <span class="op">=</span> torch.tensor(np.load(root_dir<span class="op">+</span>base_name<span class="op">+</span><span class="st">"intensities.npy"</span>)[idx], dtype<span class="op">=</span>dtype)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.unit_cell              <span class="op">=</span> torch.tensor(np.load(root_dir<span class="op">+</span>base_name<span class="op">+</span><span class="st">"unit_cell.npy"</span>)[idx], dtype<span class="op">=</span>dtype)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use extinctions rather than space groups</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.sg_number              <span class="op">=</span> torch.tensor(np.load(root_dir<span class="op">+</span>base_name<span class="op">+</span><span class="st">"extinction_number.npy"</span>)[idx], dtype<span class="op">=</span>dtype)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.crystal_system_numeric <span class="op">=</span> torch.tensor(np.load(root_dir<span class="op">+</span>base_name<span class="op">+</span><span class="st">"crystal_systems_numeric.npy"</span>), dtype<span class="op">=</span>dtype)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.hkl                    <span class="op">=</span> torch.tensor(np.load(root_dir<span class="op">+</span>base_name<span class="op">+</span><span class="st">"hkl.npy"</span>), dtype<span class="op">=</span>dtype)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.intensities            <span class="op">=</span> torch.tensor(np.load(root_dir<span class="op">+</span>base_name<span class="op">+</span><span class="st">"intensities.npy"</span>), dtype<span class="op">=</span>dtype)</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.unit_cell              <span class="op">=</span> torch.tensor(np.load(root_dir<span class="op">+</span>base_name<span class="op">+</span><span class="st">"unit_cell.npy"</span>), dtype<span class="op">=</span>dtype)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Use extinctions rather than space groups</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.sg_number              <span class="op">=</span> torch.tensor(np.load(root_dir<span class="op">+</span>base_name<span class="op">+</span><span class="st">"extinction_number.npy"</span>), dtype<span class="op">=</span>dtype)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>):</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.intensities)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.crystal_system_numeric[idx], <span class="va">self</span>.hkl[idx], <span class="va">self</span>.intensities[idx], <span class="va">self</span>.unit_cell[idx], <span class="va">self</span>.sg_number[idx]</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>base_name <span class="op">=</span> <span class="st">"4-44-CuKa1-data_4000_"</span></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>root_dir <span class="op">=</span> <span class="st">"./"</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> torch.device(<span class="st">"cuda:0"</span> <span class="cf">if</span> torch.cuda.is_available <span class="cf">else</span> <span class="st">"cpu"</span>)</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> torch.float32</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can now prepare our neural network model. I’ve <em>almost</em> replicated the one in the paper (same number of layers), but made a slight difference to the first linear layer due to the different data resolution I’m using.</p>
<p>Chitturi et al make use of data with a resolution of 0.01 degrees per step (90 degree data range, 9000 steps), but then also use an initial MaxPooling layer with pool size of 3, meaning that the average resolution being fed into the first convolutional layer is 0.03 degrees per step. In my case, I’ll opt to use a resolution of 0.019 degrees per step (via 40 degree data range / 2048 steps), and so have chosen to omit the initial pooling layer from my implementation. The difference in data resolution also means that the dimensionality of the first linear layer will be different from Chitturi’s model. However, overall the architecture is very similar, and a similar total number of parameters is obtained:</p>
<table class="table">
<thead>
<tr class="header">
<th>Model</th>
<th>Parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Chitturi et al</td>
<td>78248</td>
</tr>
<tr class="even">
<td>Replication</td>
<td>80292</td>
</tr>
</tbody>
</table>
<p>The model is fairly straightforward, with blocks of convolutional and pooling layers, followed by a stack of linear layers. I’ve implemented a module for the blocks, and then stacked them together with the linear layers to create the model.</p>
<div class="cell" data-execution_count="13">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ChitturiConvBlock(nn.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_dim, out_dim, pool<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.Sequential(</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>            nn.Conv1d(in_dim, out_dim, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>            nn.Conv1d(out_dim, out_dim, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>            nn.MaxPool1d(pool),</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.layers(x)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ChitturiModel(nn.Module):</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList([</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            <span class="co">#nn.MaxPool1d(2),</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            ChitturiConvBlock(<span class="dv">1</span>, <span class="dv">5</span>, pool<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>            ChitturiConvBlock(<span class="dv">5</span>, <span class="dv">10</span>, pool<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>            ChitturiConvBlock(<span class="dv">10</span>, <span class="dv">15</span>, pool<span class="op">=</span><span class="dv">3</span>),</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>            ChitturiConvBlock(<span class="dv">15</span>, <span class="dv">20</span>, pool<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>            ChitturiConvBlock(<span class="dv">20</span>, <span class="dv">30</span>, pool<span class="op">=</span><span class="dv">3</span>),</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>            nn.Flatten(),</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">840</span>, <span class="dv">80</span>),</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">80</span>, <span class="dv">50</span>),</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">50</span>, <span class="dv">10</span>),</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>            nn.ReLU(),</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">10</span>, <span class="dv">3</span>),</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>        ])</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, shapes<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> shapes:</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(x.shape)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> l(x)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> shapes:</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(x.shape)</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ChitturiModel().to(device)</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> model(torch.randn(<span class="dv">100</span>,<span class="dv">1</span>,<span class="dv">2048</span>).to(device), shapes<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model parameters:"</span>,<span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model parameters: 80298</code></pre>
</div>
</div>
<p>This model should give us a reasonable baseline which we can try to improve on later in this post. What we now need is a training loop which includes the following: - Data generation with augmentation strategies (see previous post) - Model predictions - Model loss calculation (and other metrics) - Model training - Validation loss calculation (and other metrics)</p>
<p>Chitturi et al trained their model using the <a href="https://en.m.wikipedia.org/wiki/Huber_loss">Huber</a> loss function. This is a loss function that is designed to be more robust against outliers compared to the mean squared error, whilst encouraging convergence using a quadratic loss when the error is low. We’ll use the same loss function, but also calculate the mean squared error (MSE), the mean absolute error (MAE) and the mean absolute percentage error (MAPE) as additional metrics for this regression task. The MAPE will be the most convenient metric to use to compare against Chitturi’s model as this is reported in the article for several different variations of model they trained.</p>
<p>Another thing to point out is that Chitturi et al trained models on “perfect” data, and then sequentially added in different augmentations (e.g.&nbsp;baseline noise, zero point errors etc). In the interest of time I’ll just dive straight in with training the model with all of the augmentation strategies employed rather than adding them incrementally.</p>
<p>Let’s define the data generation parameters</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data generation parameters</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> powcodgen <span class="im">import</span> patterns</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tqdm.auto <span class="im">import</span> tqdm</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Data generation parameters</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>ttmin <span class="op">=</span> <span class="dv">4</span>                       <span class="co"># Minimum data twotheta angle</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>ttmax <span class="op">=</span> <span class="dv">44</span>                      <span class="co"># Maximum data twotheta angle</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>peakrange <span class="op">=</span> <span class="fl">3.</span>                  <span class="co"># Buffer for peaks to move beyond the data range</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>datadim <span class="op">=</span> <span class="dv">2048</span>                  <span class="co"># Number of points in PXRD histograms</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>same_hwhm <span class="op">=</span> <span class="va">False</span>               <span class="co"># Impurity data has same peak shape &amp; hwhm as the dominant phase</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>min_impurity_intensity <span class="op">=</span> <span class="fl">0.02</span>   <span class="co"># Minimum intensity for impurity peaks</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>max_impurity_intensity <span class="op">=</span> <span class="fl">0.15</span>   <span class="co"># Maximum intensity for impurity peaks</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>add_background <span class="op">=</span> <span class="va">True</span>           <span class="co"># Include an amorphous background (Chebyshev)</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># ------------------------------------------------------------</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the tensors used as the PXRD histograms</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>full_data <span class="op">=</span> torch.linspace(ttmin<span class="op">-</span>(peakrange<span class="op">/</span><span class="dv">2</span>), ttmax<span class="op">+</span>(peakrange<span class="op">/</span><span class="dv">2</span>),</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">int</span>(np.ceil((ttmax<span class="op">-</span>ttmin<span class="op">+</span>peakrange)<span class="op">/</span>((ttmax<span class="op">-</span>ttmin)<span class="op">/</span>datadim))),</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>                        device<span class="op">=</span>device, dtype<span class="op">=</span>dtype)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> (full_data[full_data <span class="op">&lt;=</span> ttmin<span class="op">+</span>(peakrange<span class="op">/</span><span class="dv">2</span>)]).clone() <span class="op">-</span> ttmin</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now for the training loop</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Training / validation code</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_loss(dataloader, model, optimizer, epoch, train<span class="op">=</span><span class="va">True</span>, show_pbar<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Get the losses for an epoch. Toggle between the dataloaders so the same code can be recycled</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">    for the training and validation sets.</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> show_pbar:</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        pbar <span class="op">=</span> tqdm(dataloader)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        pbar <span class="op">=</span> dataloader</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    logs <span class="op">=</span> []</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> train:</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        model.<span class="bu">eval</span>()</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch_cs, batch_hkl, batch_i, batch_cell, batch_sg_number <span class="kw">in</span> pbar:</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># zero the parameter gradients</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward + backward + optimize</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(batch_cs) <span class="op">%</span> <span class="dv">2</span> <span class="op">!=</span> <span class="dv">0</span> <span class="kw">or</span> <span class="bu">len</span>(batch_cs) <span class="op">%</span> <span class="dv">3</span> <span class="op">!=</span> <span class="dv">0</span>:</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            maxlength <span class="op">=</span> <span class="dv">6</span><span class="op">*</span>(batch_cs.shape[<span class="dv">0</span>]<span class="op">//</span><span class="dv">6</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            batch_cs <span class="op">=</span> batch_cs[:maxlength]</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>            batch_hkl <span class="op">=</span> batch_hkl[:maxlength]</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>            batch_i <span class="op">=</span> batch_i[:maxlength]</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>            batch_cell <span class="op">=</span> batch_cell[:maxlength]</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>            batch_sg_number <span class="op">=</span> batch_sg_number[:maxlength]</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>        calcdata <span class="op">=</span> patterns.calculate_diffraction_patterns_with_impurities(</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                                        x,</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>                                        full_data,</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>                                        batch_cs.to(device),</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>                                        batch_hkl.to(device),</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>                                        batch_i.to(device),</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>                                        batch_cell.to(device),</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>                                        batch_sg_number.to(device),</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>                                        same_hwhm<span class="op">=</span>same_hwhm,</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>                                        min_impurity_intensity<span class="op">=</span>min_impurity_intensity,</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>                                        max_impurity_intensity<span class="op">=</span>max_impurity_intensity,</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>                                        add_background <span class="op">=</span> add_background,</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>                                    )</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        combined, pure_patterns, impure_patterns, pure_impure, cs, cell, sgs, hkls <span class="op">=</span> calcdata</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        abc_pred <span class="op">=</span> model(combined.unsqueeze(<span class="dv">1</span>))</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        sorted_cell <span class="op">=</span> torch.sort(cell[:,:<span class="dv">3</span>], dim<span class="op">=</span><span class="dv">1</span>).values</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        mse <span class="op">=</span> F.mse_loss(abc_pred, sorted_cell)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>        mae <span class="op">=</span> torch.<span class="bu">abs</span>(abc_pred <span class="op">-</span> sorted_cell).mean()</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>        huber <span class="op">=</span> F.huber_loss(abc_pred, sorted_cell)</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>        mape <span class="op">=</span> (<span class="dv">100</span><span class="op">*</span>torch.<span class="bu">abs</span>(abc_pred <span class="op">-</span> sorted_cell)<span class="op">/</span>sorted_cell).mean()</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> train:</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> huber</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>            optimizer.step()</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> show_pbar:</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>            pbar.set_description_str(<span class="ss">f"Epoch: </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> MSE : </span><span class="sc">{</span>mse<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss"> MAE : </span><span class="sc">{</span>mae<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss"> MAPE : </span><span class="sc">{</span>mape<span class="sc">:.4f}</span><span class="ss"> Huber : </span><span class="sc">{</span>huber<span class="sc">.</span>item()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>        logs.append(torch.tensor([[mse, mae, mape, huber]]))</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> train:</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>        model.train()</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logs</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(model, optimizer, idx, name, batchsize<span class="op">=</span><span class="dv">256</span>, num_epochs<span class="op">=</span><span class="dv">50</span>, show_pbar<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a><span class="co">    Train a model on a given subset of the POWCOD training data</span></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>    dataset <span class="op">=</span> DiffractionData(base_name, root_dir<span class="op">=</span>root_dir, idx<span class="op">=</span>idx)</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> torch.Generator().manual_seed(<span class="dv">42</span>)</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>    <span class="co">#train_set, val_set, test_set = torch.utils.data.random_split(dataset, [0.8, 0.1, 0.1], g)</span></span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>    train_set, val_set <span class="op">=</span> torch.utils.data.random_split(dataset, [<span class="fl">0.8</span>, <span class="fl">0.2</span>], g)</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>    train_dataloader <span class="op">=</span> DataLoader(train_set, batch_size<span class="op">=</span>batchsize, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>    val_dataloader   <span class="op">=</span> DataLoader(val_set,   batch_size<span class="op">=</span>batchsize, shuffle<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>    <span class="co">#test_dataloader  = DataLoader(test_set,  batch_size=batchsize, shuffle=True)</span></span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Training samples = </span><span class="sc">{</span><span class="bu">len</span>(train_set)<span class="sc">}</span><span class="ch">\n</span><span class="ss">Validation samples = </span><span class="sc">{</span><span class="bu">len</span>(val_set)<span class="sc">}</span><span class="ss">"</span>)<span class="co">#\nTest samples = {len(test_set)}")</span></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a>    num_steps <span class="op">=</span> (<span class="bu">len</span>(train_set)<span class="op">//</span>batchsize) <span class="op">*</span> num_epochs</span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Epochs: </span><span class="sc">{</span>num_epochs<span class="sc">}</span><span class="ss"> Total gradient updates: </span><span class="sc">{</span>num_steps<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a>    all_train_losses <span class="op">=</span> []</span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>    all_val_losses <span class="op">=</span> []</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a>    best_val_mape <span class="op">=</span> <span class="bu">float</span>(<span class="st">"inf"</span>)</span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>        train_losses <span class="op">=</span> get_loss(train_dataloader, model, optimizer, epoch, train<span class="op">=</span><span class="va">True</span>, show_pbar<span class="op">=</span>show_pbar)</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>        all_train_losses.append(train_losses)</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>        train_losses <span class="op">=</span> torch.cat(train_losses, dim<span class="op">=</span><span class="dv">0</span>).mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>        val_losses <span class="op">=</span> get_loss(val_dataloader, model, optimizer, epoch, train<span class="op">=</span><span class="va">False</span>, show_pbar<span class="op">=</span>show_pbar)</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a>        all_val_losses.append(val_losses)</span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a>        val_losses <span class="op">=</span> torch.cat(val_losses, dim<span class="op">=</span><span class="dv">0</span>).mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> show_pbar:</span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Train: MSE </span><span class="sc">{</span>train_losses[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss"> MAE </span><span class="sc">{</span>train_losses[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss"> MAPE </span><span class="sc">{</span>train_losses[<span class="dv">2</span>]<span class="sc">:.3f}</span><span class="ss"> Huber </span><span class="sc">{</span>train_losses[<span class="dv">3</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"  Val: MSE </span><span class="sc">{</span>val_losses[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss"> MAE </span><span class="sc">{</span>val_losses[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss"> MAPE </span><span class="sc">{</span>val_losses[<span class="dv">2</span>]<span class="sc">:.3f}</span><span class="ss"> Huber </span><span class="sc">{</span>val_losses[<span class="dv">3</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> val_losses[<span class="dv">2</span>] <span class="op">&lt;</span> best_val_mape:</span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a>            best_val_mape <span class="op">=</span> val_losses[<span class="dv">2</span>]</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>            torch.save({</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>                <span class="st">'epoch'</span>: epoch,</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>                <span class="st">'model_state_dict'</span>: model.state_dict(),</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>                <span class="st">'optimizer_state_dict'</span>: optimizer.state_dict(),</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>                }, <span class="ss">f"saved_models/</span><span class="sc">{</span>name<span class="sc">}</span><span class="ss">.pth"</span>)</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> all_train_losses, all_val_losses</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>One thing to point out is that the code I’m using to generate the impurity phase peaks means that the batch size needed from the dataloader is larger than the batch size seen by the model.</p>
<p>It works by taking in a batch of 3 x <em>N</em> samples. The first <em>N</em> samples is considered “pure”, the second <em>N</em> samples will be the dominant phase in the impure data, with the final <em>N</em> samples making up the impurity peaks. The impure data is then produced by summing the patterns produced by the dominant phase and a scaled-down version of the minority phase patterns. In this case, this means that the impurity phases are always guaranteed to be from the same crystal system as the dominant phase. Whilst I appreciate that this isn’t the ideal way to do things, it’s quick and convenient, and should still allow the models to be relatively robust with respect to the presence of small peaks from impurity phases. If anyone wanted to pick up on this work, then this would be a good place for improvement!</p>
<p>As a result of this impurity-generation strategy, you’ll see that I specify the batch size below to be (64*3)//2 = 96. This means that 96 samples will be obtained from the database and have their diffraction patterns generated. These will be split into 32 pure patterns, 32 dominant phase patterns and 32 minority phase patterns (which are randomly scaled down to ensure that the maximum intensity of the impurity patterns falls in the range specified in the settings defined earlier). The dominant phase and impurity phase patterns are then combined, which results in 32 pure patterns + 32 impure patterns, giving a batch size (as seen by the model) of 64. The batch size of 64 (as seen by the model) was chosen to match that used by Chitturi et al.</p>
<p>Let’s give this a go, and plot the metrics as a function of epoch for each of the three crystal systems we’re testing!</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>splits <span class="op">=</span> [<span class="st">"Triclinic"</span>, <span class="st">"Monoclinic"</span>, <span class="st">"Orthorhombic"</span>]</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> []</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, subset <span class="kw">in</span> <span class="bu">enumerate</span>([triclinic_data, monoclinic_data, orthorhombic_data]):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ChitturiModel().to(device)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(params<span class="op">=</span>model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(splits[i])</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    train_metrics, val_metrics <span class="op">=</span> train(</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>                                    model<span class="op">=</span>model,</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>                                    optimizer<span class="op">=</span>optimizer,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>                                    idx<span class="op">=</span>subset,</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>                                    name<span class="op">=</span><span class="ss">f"Chitturi_</span><span class="sc">{</span>splits[i]<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>                                    num_epochs<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>                                    batchsize<span class="op">=</span>(<span class="dv">64</span><span class="op">*</span><span class="dv">3</span>)<span class="op">//</span><span class="dv">2</span>,</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>                                    show_pbar<span class="op">=</span><span class="va">False</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>                                    )</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the minimum validation loss for each metric</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    val_min <span class="op">=</span> torch.cat([x.mean(dim<span class="op">=</span><span class="dv">0</span>,keepdim<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> x <span class="kw">in</span> [torch.cat(y, dim<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> y <span class="kw">in</span> val_metrics]], dim<span class="op">=</span><span class="dv">0</span>).<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).values</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Best </span><span class="sc">{</span>splits[i]<span class="sc">}</span><span class="ss"> val MSE: </span><span class="sc">{</span>val_min[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss"> MAE: </span><span class="sc">{</span>val_min[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss"> MAPE: </span><span class="sc">{</span>val_min[<span class="dv">2</span>]<span class="sc">:.3f}</span><span class="ss"> Huber: </span><span class="sc">{</span>val_min[<span class="dv">3</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the average loss for each epoch and plot</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    train_av <span class="op">=</span> torch.cat([x.mean(dim<span class="op">=</span><span class="dv">0</span>,keepdim<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> x <span class="kw">in</span> [torch.cat(y, dim<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> y <span class="kw">in</span> train_metrics]], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    val_av <span class="op">=</span> torch.cat([x.mean(dim<span class="op">=</span><span class="dv">0</span>,keepdim<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> x <span class="kw">in</span> [torch.cat(y, dim<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> y <span class="kw">in</span> val_metrics]], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">2</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(<span class="st">"Chitturi et al. model "</span><span class="op">+</span>splits[i])</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">0</span>].plot(train_av[:,<span class="dv">0</span>])</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">0</span>].plot(val_av[:,<span class="dv">0</span>])</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">0</span>].title.set_text(<span class="st">"MSE"</span>)</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">0</span>].legend([<span class="st">"Train"</span>, <span class="st">"Val"</span>])</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">1</span>].plot(train_av[:,<span class="dv">1</span>])</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">1</span>].plot(val_av[:,<span class="dv">1</span>])</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">1</span>].title.set_text(<span class="st">"MAE"</span>)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">1</span>].legend([<span class="st">"Train"</span>, <span class="st">"Val"</span>])</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">0</span>].plot(train_av[:,<span class="dv">2</span>])</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">0</span>].plot(val_av[:,<span class="dv">2</span>])</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">0</span>].title.set_text(<span class="st">"MAPE"</span>)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">0</span>].legend([<span class="st">"Train"</span>, <span class="st">"Val"</span>])</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">1</span>].plot(train_av[:,<span class="dv">3</span>])</span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">1</span>].plot(val_av[:,<span class="dv">3</span>])</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">1</span>].title.set_text(<span class="st">"Huber"</span>)</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">1</span>].legend([<span class="st">"Train"</span>, <span class="st">"Val"</span>])</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    models.append(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Triclinic
Training samples = 55230
Validation samples = 13807
Epochs: 50 Total gradient updates: 28750
Best Triclinic val MSE: 1.604 MAE: 0.834 MAPE: 7.896 Huber: 0.485</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2024-02-11-Indexing-PXRD-data-with-Convolutional-Neural-Networks_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Monoclinic
Training samples = 120031
Validation samples = 30007
Epochs: 50 Total gradient updates: 62500
Best Monoclinic val MSE: 9.230 MAE: 1.969 MAPE: 14.465 Huber: 1.547</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2024-02-11-Indexing-PXRD-data-with-Convolutional-Neural-Networks_files/figure-html/cell-8-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Orthorhombic
Training samples = 40522
Validation samples = 10130
Epochs: 50 Total gradient updates: 21100
Best Orthorhombic val MSE: 10.664 MAE: 2.327 MAPE: 19.489 Huber: 1.882</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2024-02-11-Indexing-PXRD-data-with-Convolutional-Neural-Networks_files/figure-html/cell-8-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code></code></pre>
</div>
</div>
<p>As we can see from the results above, all of the metrics demonstrate that the model is learning to predict the lattice parameters. These curves appear to be still decreasing once we hit 50 epochs, and the validation and training losses are not diverging. As such, the models are undertrained at this point and would likely continue to improve with additional epochs.</p>
<p>In terms of the performance obtained in each case, let’s summarise the MAPEs into a table to allow us to easily compare against Chitturi et al’s results. You’ll see in the table below, two sets of results from Chitturi et al’s work. The lower number is for models trained on augmented data and tested exclusively on “perfect” data (i.e.&nbsp;no augmentation). The larger number is for models trained on augmented data and tested exclusively on augmented data.</p>
<p>This differs to what I’ve done here in that only half of the datasets during both testing and evaluation contain impurities.</p>
<table class="table">
<colgroup>
<col style="width: 17%">
<col style="width: 32%">
<col style="width: 35%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th>Crystal system</th>
<th>Chitturi et al - (100/0)</th>
<th>Chitturi et al - (0/100)</th>
<th>Replication (50/50)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Triclinic</td>
<td>7.50</td>
<td>10.48</td>
<td>7.90</td>
</tr>
<tr class="even">
<td>Monoclinic</td>
<td>13.50</td>
<td>16.02</td>
<td>14.46</td>
</tr>
<tr class="odd">
<td>Orthorhombic</td>
<td>15.71</td>
<td>19.49</td>
<td>19.49</td>
</tr>
</tbody>
</table>
<p>For the triclinic, monoclinic and orthorhombic models, it’s nice to see that we achieve similar MAPEs to those reported in the paper despite the significantly smaller dataset.</p>
</section>
<section id="training-a-convnext-v2-model" class="level1">
<h1>Training a ConvNeXt v2 model</h1>
<p>Let’s now see if a more modern architecture can be used to improve on these baselines. We’ll base our new neural network on a recently published convolutional neural network architecture that gives state of the art performance on image classification tasks: <a href="https://arxiv.org/abs/2301.00808">ConvNeXt v2</a>.</p>
<p>The building block of this architecture is the ConvNeXt v2 block, shown in the figure below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ConvNeXtV2_block.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">ConvNeXt v2 block</figcaption><p></p>
</figure>
</div>
<p>This block begins with a <a href="https://paperswithcode.com/method/depthwise-convolution">depthwise convolutional layer</a> with a 7x7 kernel size followed by layer normalisation. This is then followed by a <a href="https://paperswithcode.com/method/pointwise-convolution">pointwise (i.e.&nbsp;1x1 kernel)</a> convolutional layer, which expands the dimensionality of the input signal by a factor of 4. The <a href="https://paperswithcode.com/method/gelu">GELU</a> non-linearity and the novel <em>Global Response Layer</em> (GRN) are applied. GRN was introduced to aid in stability during pretraining by ensuring that the feature maps produced by the model are diverse and free from redundancies.</p>
<p>Following the GRN, the final pointwise convolution projects feature maps back to the same dimensionality as the input which enables a <a href="https://paperswithcode.com/method/residual-connection">residual/skip connection</a> to be applied, adding the input to the block to the output of the various layers we have discussed.</p>
<p>Various combinations of these blocks with different sizes are combined, interspersed with with downsampling layers, before global average pooling is applied over the spatial dimensions of the final ConvNeXt v2 block, followed by a linear layer to compute the outputs of the model.</p>
<p>In a slight departure from the published ConvNeXt v2 architecture, our model will flatten the output of the last convolutional layer before processing it via linear layers to compute the outputs. In my testing, the global average pooling approach didn’t work too well (discussed more below). We will also of course be converting the model to use 1D rather than 2D convolutional and normalisation layers.</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GRN(nn.Module):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" Global Response Normalization, proposed in ConvNeXt v2 paper """</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps <span class="op">=</span> <span class="fl">1e-6</span>):</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gamma <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, dim, <span class="dv">1</span>))</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>, dim, <span class="dv">1</span>))</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># x = (B, C, T)</span></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Want to average first over length (T), then divide by average channel (i.e. average of C)</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Divide the L2 norms by the average for each channel</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        Gx <span class="op">=</span> x.norm(p<span class="op">=</span><span class="dv">2</span>, dim<span class="op">=</span><span class="dv">2</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># (B, C, T) --&gt; (B, C, 1)</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        Nx <span class="op">=</span> Gx <span class="op">/</span> Gx.mean(dim<span class="op">=</span><span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>).clamp(<span class="bu">min</span><span class="op">=</span><span class="va">self</span>.eps) <span class="co"># (B, C, 1) / (B, 1, 1) --&gt; (B, C, 1)</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.gamma <span class="op">*</span> (x <span class="op">*</span> Nx) <span class="op">+</span> <span class="va">self</span>.beta <span class="op">+</span> x</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DropPath(nn.Module):</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" DropPath regularisation can be used if needed, as described here:</span></span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a><span class="co">    https://arxiv.org/abs/1605.07648v4</span></span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, p: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.5</span>, inplace: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.p <span class="op">=</span> p</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.inplace <span class="op">=</span> inplace</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> drop_path(<span class="va">self</span>, x, keep_prob: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>, inplace: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>):</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>        mask <span class="op">=</span> x.new_empty(x.shape[<span class="dv">0</span>], <span class="dv">1</span>, <span class="dv">1</span>).bernoulli_(keep_prob)</span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>        mask.div_(keep_prob)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> inplace:</span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a>            x.mul_(mask)</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb13-33"><a href="#cb13-33" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x <span class="op">*</span> mask</span>
<span id="cb13-34"><a href="#cb13-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb13-35"><a href="#cb13-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-36"><a href="#cb13-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-37"><a href="#cb13-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.training <span class="kw">and</span> <span class="va">self</span>.p <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-38"><a href="#cb13-38" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.drop_path(x, <span class="va">self</span>.p, <span class="va">self</span>.inplace)</span>
<span id="cb13-39"><a href="#cb13-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb13-40"><a href="#cb13-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-41"><a href="#cb13-41" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb13-42"><a href="#cb13-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>__class__<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss">(p=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>p<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb13-43"><a href="#cb13-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-44"><a href="#cb13-44" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNeXtBlock(nn.Module):</span>
<span id="cb13-45"><a href="#cb13-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># A 1D ConvNeXt v2 block</span></span>
<span id="cb13-46"><a href="#cb13-46" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, drop_path_prob<span class="op">=</span><span class="fl">0.0</span>):</span>
<span id="cb13-47"><a href="#cb13-47" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-48"><a href="#cb13-48" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dwconv <span class="op">=</span> nn.Conv1d(in_channels<span class="op">=</span>dim, out_channels<span class="op">=</span>dim, kernel_size<span class="op">=</span><span class="dv">7</span>, groups<span class="op">=</span>dim, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb13-49"><a href="#cb13-49" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb13-50"><a href="#cb13-50" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pwconv_1 <span class="op">=</span> nn.Conv1d(dim, <span class="dv">4</span><span class="op">*</span>dim, kernel_size<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-51"><a href="#cb13-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act <span class="op">=</span> nn.GELU()</span>
<span id="cb13-52"><a href="#cb13-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.GRN <span class="op">=</span> GRN(<span class="dv">4</span><span class="op">*</span>dim)</span>
<span id="cb13-53"><a href="#cb13-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pwconv_2 <span class="op">=</span> nn.Conv1d(<span class="dv">4</span><span class="op">*</span>dim, dim, kernel_size<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-54"><a href="#cb13-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.droppath <span class="op">=</span> DropPath(p<span class="op">=</span>drop_path_prob)</span>
<span id="cb13-55"><a href="#cb13-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-56"><a href="#cb13-56" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, inputs):</span>
<span id="cb13-57"><a href="#cb13-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Inputs has shape (B, C, T)</span></span>
<span id="cb13-58"><a href="#cb13-58" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dwconv(inputs)</span>
<span id="cb13-59"><a href="#cb13-59" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x.permute(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>))</span>
<span id="cb13-60"><a href="#cb13-60" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.permute(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>) <span class="co"># Layernorm expects channels last</span></span>
<span id="cb13-61"><a href="#cb13-61" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pwconv_1(x)</span>
<span id="cb13-62"><a href="#cb13-62" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.act(x)</span>
<span id="cb13-63"><a href="#cb13-63" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.GRN(x)</span>
<span id="cb13-64"><a href="#cb13-64" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.pwconv_2(x)</span>
<span id="cb13-65"><a href="#cb13-65" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> inputs <span class="op">+</span> <span class="va">self</span>.droppath(x)</span>
<span id="cb13-66"><a href="#cb13-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-67"><a href="#cb13-67" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DownSample(nn.Module):</span>
<span id="cb13-68"><a href="#cb13-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_dim, out_dim):</span>
<span id="cb13-69"><a href="#cb13-69" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-70"><a href="#cb13-70" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.norm <span class="op">=</span> nn.LayerNorm(in_dim)</span>
<span id="cb13-71"><a href="#cb13-71" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.down <span class="op">=</span> nn.Conv1d(in_dim, out_dim, kernel_size<span class="op">=</span><span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb13-72"><a href="#cb13-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-73"><a href="#cb13-73" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb13-74"><a href="#cb13-74" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.norm(x.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)).permute(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb13-75"><a href="#cb13-75" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.down(x)</span>
<span id="cb13-76"><a href="#cb13-76" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb13-77"><a href="#cb13-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-78"><a href="#cb13-78" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNeXt(nn.Module):</span>
<span id="cb13-79"><a href="#cb13-79" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, out_dim<span class="op">=</span><span class="dv">3</span>, depths<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">2</span>], dims<span class="op">=</span>[<span class="dv">40</span>, <span class="dv">80</span>, <span class="dv">160</span>, <span class="dv">320</span>], drop_path_prob<span class="op">=</span><span class="fl">0.5</span>, dropout<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb13-80"><a href="#cb13-80" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb13-81"><a href="#cb13-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.depths <span class="op">=</span> depths</span>
<span id="cb13-82"><a href="#cb13-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.downsample_layers <span class="op">=</span> nn.ModuleList() <span class="co"># stem and 3 intermediate downsampling conv layers</span></span>
<span id="cb13-83"><a href="#cb13-83" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.initial_conv <span class="op">=</span> nn.Conv1d(<span class="dv">1</span>, dims[<span class="dv">0</span>], kernel_size<span class="op">=</span><span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb13-84"><a href="#cb13-84" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.initial_norm <span class="op">=</span> nn.LayerNorm(dims[<span class="dv">0</span>])</span>
<span id="cb13-85"><a href="#cb13-85" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> nn.ModuleList()</span>
<span id="cb13-86"><a href="#cb13-86" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, dd <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(depths, dims)):</span>
<span id="cb13-87"><a href="#cb13-87" aria-hidden="true" tabindex="-1"></a>            depth, dim <span class="op">=</span> dd</span>
<span id="cb13-88"><a href="#cb13-88" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(depth):</span>
<span id="cb13-89"><a href="#cb13-89" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.layers.append(ConvNeXtBlock(dim, drop_path_prob<span class="op">=</span>drop_path_prob))</span>
<span id="cb13-90"><a href="#cb13-90" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i<span class="op">+</span><span class="dv">1</span> <span class="op">!=</span> <span class="bu">len</span>(dims):</span>
<span id="cb13-91"><a href="#cb13-91" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.layers.append(DownSample(in_dim<span class="op">=</span>dim, out_dim<span class="op">=</span>dims[i<span class="op">+</span><span class="dv">1</span>]))</span>
<span id="cb13-92"><a href="#cb13-92" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb13-93"><a href="#cb13-93" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.layers.append(DownSample(in_dim<span class="op">=</span>dim, out_dim<span class="op">=</span>dims[i]))</span>
<span id="cb13-94"><a href="#cb13-94" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.final_norm <span class="op">=</span> nn.LayerNorm(<span class="dv">1024</span>)</span>
<span id="cb13-95"><a href="#cb13-95" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flatten <span class="op">=</span> nn.Flatten()</span>
<span id="cb13-96"><a href="#cb13-96" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output <span class="op">=</span> nn.Sequential(</span>
<span id="cb13-97"><a href="#cb13-97" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">1024</span>, <span class="dv">64</span>),</span>
<span id="cb13-98"><a href="#cb13-98" aria-hidden="true" tabindex="-1"></a>            nn.GELU(),</span>
<span id="cb13-99"><a href="#cb13-99" aria-hidden="true" tabindex="-1"></a>            nn.Dropout(dropout),</span>
<span id="cb13-100"><a href="#cb13-100" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">64</span>, out_dim)</span>
<span id="cb13-101"><a href="#cb13-101" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb13-102"><a href="#cb13-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-103"><a href="#cb13-103" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, shapes<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb13-104"><a href="#cb13-104" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.initial_conv(x)</span>
<span id="cb13-105"><a href="#cb13-105" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.initial_norm(x.permute(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>)).permute(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb13-106"><a href="#cb13-106" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> shapes:</span>
<span id="cb13-107"><a href="#cb13-107" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(x.shape)</span>
<span id="cb13-108"><a href="#cb13-108" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb13-109"><a href="#cb13-109" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> l(x)</span>
<span id="cb13-110"><a href="#cb13-110" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> shapes:</span>
<span id="cb13-111"><a href="#cb13-111" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(x.shape)</span>
<span id="cb13-112"><a href="#cb13-112" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.final_norm(F.gelu(<span class="va">self</span>.flatten(x)))<span class="co">#.permute(0,2,1)).permute(0,2,1) # global average pooling, (B, C, T) -&gt; (B, C)</span></span>
<span id="cb13-113"><a href="#cb13-113" aria-hidden="true" tabindex="-1"></a>        <span class="co">#x = self.flatten(x)</span></span>
<span id="cb13-114"><a href="#cb13-114" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> shapes:</span>
<span id="cb13-115"><a href="#cb13-115" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(x.shape)</span>
<span id="cb13-116"><a href="#cb13-116" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output(x)</span>
<span id="cb13-117"><a href="#cb13-117" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb13-118"><a href="#cb13-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-119"><a href="#cb13-119" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ConvNeXt(depths<span class="op">=</span>[<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">6</span>,<span class="dv">2</span>], dims<span class="op">=</span>[<span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">12</span>,<span class="dv">16</span>]).to(device)</span>
<span id="cb13-120"><a href="#cb13-120" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> model(torch.randn(<span class="dv">100</span>,<span class="dv">1</span>,<span class="dv">2048</span>).to(device), shapes<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-121"><a href="#cb13-121" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model parameters:"</span>,<span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model parameters: 87279</code></pre>
</div>
</div>
<div class="cell" data-execution_count="9">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>splits <span class="op">=</span> [<span class="st">"Triclinic"</span>, <span class="st">"Monoclinic"</span>, <span class="st">"Orthorhombic"</span>]</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> []</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, subset <span class="kw">in</span> <span class="bu">enumerate</span>([triclinic_data, monoclinic_data, orthorhombic_data]):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ConvNeXt(depths<span class="op">=</span>[<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">6</span>,<span class="dv">2</span>], dims<span class="op">=</span>[<span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">12</span>,<span class="dv">16</span>], drop_path_prob<span class="op">=</span><span class="dv">0</span>, dropout<span class="op">=</span><span class="dv">0</span>).to(device)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(params<span class="op">=</span>model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(splits[i])</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    train_metrics, val_metrics <span class="op">=</span> train(</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>                                    model<span class="op">=</span>model,</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>                                    optimizer<span class="op">=</span>optimizer,</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>                                    idx<span class="op">=</span>subset,</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>                                    name<span class="op">=</span><span class="ss">f"ConvNeXt_</span><span class="sc">{</span>splits[i]<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>                                    num_epochs<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>                                    batchsize<span class="op">=</span>(<span class="dv">64</span><span class="op">*</span><span class="dv">3</span>)<span class="op">//</span><span class="dv">2</span>,</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>                                    show_pbar<span class="op">=</span><span class="va">False</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>                                    )</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the minimum validation loss for each metric</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    val_min <span class="op">=</span> torch.cat([x.mean(dim<span class="op">=</span><span class="dv">0</span>,keepdim<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> x <span class="kw">in</span> [torch.cat(y, dim<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> y <span class="kw">in</span> val_metrics]], dim<span class="op">=</span><span class="dv">0</span>).<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).values</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Best </span><span class="sc">{</span>splits[i]<span class="sc">}</span><span class="ss"> val MSE: </span><span class="sc">{</span>val_min[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss"> MAE: </span><span class="sc">{</span>val_min[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss"> MAPE: </span><span class="sc">{</span>val_min[<span class="dv">2</span>]<span class="sc">:.3f}</span><span class="ss"> Huber: </span><span class="sc">{</span>val_min[<span class="dv">3</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the average loss for each epoch and plot</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    train_av <span class="op">=</span> torch.cat([x.mean(dim<span class="op">=</span><span class="dv">0</span>,keepdim<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> x <span class="kw">in</span> [torch.cat(y, dim<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> y <span class="kw">in</span> train_metrics]], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    val_av <span class="op">=</span> torch.cat([x.mean(dim<span class="op">=</span><span class="dv">0</span>,keepdim<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> x <span class="kw">in</span> [torch.cat(y, dim<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> y <span class="kw">in</span> val_metrics]], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">2</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(<span class="st">"ConvNeXt "</span><span class="op">+</span>splits[i])</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">0</span>].plot(train_av[:,<span class="dv">0</span>])</span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">0</span>].plot(val_av[:,<span class="dv">0</span>])</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">0</span>].title.set_text(<span class="st">"MSE"</span>)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">0</span>].legend([<span class="st">"Train"</span>, <span class="st">"Val"</span>])</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">1</span>].plot(train_av[:,<span class="dv">1</span>])</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">1</span>].plot(val_av[:,<span class="dv">1</span>])</span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">1</span>].title.set_text(<span class="st">"MAE"</span>)</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">1</span>].legend([<span class="st">"Train"</span>, <span class="st">"Val"</span>])</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">0</span>].plot(train_av[:,<span class="dv">2</span>])</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">0</span>].plot(val_av[:,<span class="dv">2</span>])</span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">0</span>].title.set_text(<span class="st">"MAPE"</span>)</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">0</span>].legend([<span class="st">"Train"</span>, <span class="st">"Val"</span>])</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">1</span>].plot(train_av[:,<span class="dv">3</span>])</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">1</span>].plot(val_av[:,<span class="dv">3</span>])</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">1</span>].title.set_text(<span class="st">"Huber"</span>)</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">1</span>].legend([<span class="st">"Train"</span>, <span class="st">"Val"</span>])</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>    models.append(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Triclinic
Training samples = 55230
Validation samples = 13807
Epochs: 50 Total gradient updates: 28750
Best Triclinic val MSE: 1.290 MAE: 0.697 MAPE: 6.673 Huber: 0.384</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2024-02-11-Indexing-PXRD-data-with-Convolutional-Neural-Networks_files/figure-html/cell-10-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Monoclinic
Training samples = 120031
Validation samples = 30007
Epochs: 50 Total gradient updates: 62500
Best Monoclinic val MSE: 8.420 MAE: 1.867 MAPE: 13.892 Huber: 1.450</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2024-02-11-Indexing-PXRD-data-with-Convolutional-Neural-Networks_files/figure-html/cell-10-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Orthorhombic
Training samples = 40522
Validation samples = 10130
Epochs: 50 Total gradient updates: 21100
Best Orthorhombic val MSE: 6.441 MAE: 1.588 MAPE: 12.986 Huber: 1.183</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2024-02-11-Indexing-PXRD-data-with-Convolutional-Neural-Networks_files/figure-html/cell-10-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code></code></pre>
</div>
</div>
<table class="table">
<colgroup>
<col style="width: 13%">
<col style="width: 24%">
<col style="width: 27%">
<col style="width: 17%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th>Crystal system</th>
<th>Chitturi et al - (100/0)</th>
<th>Chitturi et al - (0/100)</th>
<th>Replication (50/50)</th>
<th>ConvNeXt v2 (50/50)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Triclinic</td>
<td>7.50</td>
<td>10.48</td>
<td>7.90</td>
<td>6.67</td>
</tr>
<tr class="even">
<td>Monoclinic</td>
<td>13.50</td>
<td>16.02</td>
<td>14.46</td>
<td>13.89</td>
</tr>
<tr class="odd">
<td>Orthorhombic</td>
<td>15.71</td>
<td>19.49</td>
<td>19.49</td>
<td>12.99</td>
</tr>
</tbody>
</table>
<p>This does pretty well! The ConvNeXt v2 model has improved all of the metrics relative to our replication, with a significant improvement in the performance of the orthorhombic crystal system. It’s hard to tell with this level of training, but there may be a small amount of divergence between training and validation metrics occuring for the monoclinic data, as such some additional regularisation with DropPath and Dropout enabled may also help further improve performance.</p>
<p>However, the ConvNeXt-based model still has a relatively large number of parameters (about 5k more than our replication of Chitturi’s design). Most of these are due to the linear layers at the end of the model, which got me thinking about alternatives.</p>
<p>One option would be to take the output of the final convolutional layer and apply some kind of pooling operation, such as the global average pooling approach used in the original paper. This would reduce the dimensionality of the final linear layer(s) significantly. However, a brief test of this did not result in good performance - the model performed substantially worse than the exiting flatten approach.</p>
<p>However, I wondered if there was a sensible way to <em>learn how to pool</em> the outputs of this layer. The approach I ended up going with was to use a series of <a href="https://arxiv.org/abs/1706.03762">Transformer</a> encoder blocks with an additional “CLS token” concatenated with the input to these blocks, as was introduced by the <a href="https://arxiv.org/abs/1810.04805">BERT paper</a>. This additional token passes through the transformer blocks as normal, and as such it can attend to all of the other tokens in the sequence. The transformed output of this token is then used for the downstream tasks, such as sentiment analysis etc. It enables the transformer to learn how to efficiently aggregate information from across the sequence, via the multi-head attention mechanism which is beautifully explained by Jay Alammar <a href="https://jalammar.github.io/illustrated-transformer/">here</a>.</p>
<p>In this case, the transformed CLS token is used as the input for the final linear output layer, which gives the predicted unit cell edge lengths.</p>
</section>
<section id="replacing-the-linear-layers-with-transformer-blocks" class="level1">
<h1>Replacing the linear layers with Transformer blocks</h1>
<p>Our new model (developed below) has six additional transformer encoder blocks stacked on top of the ConvNeXt v2 blocks, and results in around 30k fewer parameters than we started with! I’ve set each transformer block to have its own learnable position embedding rather than applying a single global position embedding to the input. This is something that future work could investigate more closely, as well as alternative positional encodings such as <a href="https://arxiv.org/abs/2104.09864">RoPE</a> or <a href="https://arxiv.org/abs/2108.12409">ALiBi</a>.</p>
<p>Other changes to the original transformer encoder model published by <a href="https://arxiv.org/abs/1706.03762">Vaswani et al</a> are as follows: - Use of GEGLU layers in the feedforward blocks, as described by <a href="https://arxiv.org/abs/2002.05202">Shazeer</a>. These have been shown to improve performance of transformers with no increase in parameter count. - Use of the <a href="https://arxiv.org/abs/2002.04745">pre-LN variant</a> where layer normalisation is applied before the multi-head self attention and feedforward layers rather than after. This helps to stablise training.</p>
<p>The models now have about 57k parameters, which is a substantial reduction relative to the previous models we’ve looked at, which had around 80-87k parameters!</p>
<div class="cell" data-execution_count="10">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GLU(nn.Module):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_dim, out_dim, act<span class="op">=</span>F.gelu, bias<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear1 <span class="op">=</span> nn.Linear(in_dim, out_dim, bias<span class="op">=</span>bias)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear2 <span class="op">=</span> nn.Linear(in_dim, out_dim, bias<span class="op">=</span>bias)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.act <span class="op">=</span> act</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.act(<span class="va">self</span>.linear1(x))<span class="op">*</span><span class="va">self</span>.linear2(x)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TransformerBlock(nn.Module):</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, heads, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.heads <span class="op">=</span> heads</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.qk <span class="op">=</span> nn.Linear(dim, <span class="dv">2</span><span class="op">*</span>dim)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.v <span class="op">=</span> nn.Linear(dim, dim)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mhsa_out <span class="op">=</span> nn.Linear(dim, dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.GLU <span class="op">=</span> GLU(dim, (dim<span class="op">*</span><span class="dv">3</span>)<span class="op">//</span><span class="dv">2</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_out <span class="op">=</span> nn.Linear((dim<span class="op">*</span><span class="dv">3</span>)<span class="op">//</span><span class="dv">2</span>, dim, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(dim)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.pos <span class="op">=</span> nn.Embedding(<span class="dv">256</span>, embedding_dim<span class="op">=</span>dim)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> mhsa(<span class="va">self</span>, x):</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> x.shape</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>        q, k <span class="op">=</span> <span class="va">self</span>.qk(x <span class="op">+</span> <span class="va">self</span>.pos(torch.arange(x.shape[<span class="dv">1</span>], device<span class="op">=</span>x.device))).chunk(<span class="dv">2</span>, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> <span class="va">self</span>.v(x)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q.reshape(B, <span class="va">self</span>.heads, T, C<span class="op">//</span><span class="va">self</span>.heads)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> k.reshape(B, <span class="va">self</span>.heads, T, C<span class="op">//</span><span class="va">self</span>.heads)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> v.reshape(B, <span class="va">self</span>.heads, T, C<span class="op">//</span><span class="va">self</span>.heads)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.scaled_dot_product_attention(q, k, v)</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.reshape(B, T, C)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.mhsa_out(x)</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> ffwd(<span class="va">self</span>, x):</span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.GLU(x)</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.linear_out(<span class="va">self</span>.dropout(x))</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mhsa(<span class="va">self</span>.ln1(x))</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x))</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ConvNeXtTransformer(nn.Module):</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, out_dim<span class="op">=</span><span class="dv">3</span>, depths<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">6</span>, <span class="dv">2</span>], dims<span class="op">=</span>[<span class="dv">40</span>, <span class="dv">80</span>, <span class="dv">160</span>, <span class="dv">320</span>],</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>                 transformer_layers<span class="op">=</span><span class="dv">4</span>, transformer_heads<span class="op">=</span><span class="dv">2</span>, drop_path_prob<span class="op">=</span><span class="fl">0.5</span>, dropout<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.depths <span class="op">=</span> depths</span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.downsample_layers <span class="op">=</span> nn.ModuleList() <span class="co"># stem and 3 intermediate downsampling conv layers</span></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.initial_conv <span class="op">=</span> nn.Conv1d(<span class="dv">1</span>, dims[<span class="dv">0</span>], kernel_size<span class="op">=</span><span class="dv">7</span>, stride<span class="op">=</span><span class="dv">2</span>, padding<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.initial_norm <span class="op">=</span> nn.LayerNorm(dims[<span class="dv">0</span>])</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.conv_layers <span class="op">=</span> nn.ModuleList()</span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer_layers <span class="op">=</span> nn.ModuleList()</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.cls_token <span class="op">=</span> nn.Embedding(<span class="dv">1</span>, dims[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, dd <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(depths, dims)):</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>            depth, dim <span class="op">=</span> dd</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> d <span class="kw">in</span> <span class="bu">range</span>(depth):</span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.conv_layers.append(ConvNeXtBlock(dim, drop_path_prob<span class="op">=</span>drop_path_prob))</span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> i<span class="op">+</span><span class="dv">1</span> <span class="op">!=</span> <span class="bu">len</span>(dims):</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>                <span class="va">self</span>.conv_layers.append(DownSample(in_dim<span class="op">=</span>dim, out_dim<span class="op">=</span>dims[i<span class="op">+</span><span class="dv">1</span>]))</span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>            <span class="co">#else:</span></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>            <span class="co">#    self.conv_layers.append(DownSample(in_dim=dim, out_dim=dims[i]))</span></span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(transformer_layers):</span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.transformer_layers.append(TransformerBlock(dims[<span class="op">-</span><span class="dv">1</span>], transformer_heads, dropout<span class="op">=</span>dropout))</span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.final_norm <span class="op">=</span> nn.LayerNorm(dims[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.output <span class="op">=</span> nn.Sequential(</span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a>            GLU(dims[<span class="op">-</span><span class="dv">1</span>], <span class="dv">32</span>),</span>
<span id="cb20-71"><a href="#cb20-71" aria-hidden="true" tabindex="-1"></a>            nn.Linear(<span class="dv">32</span>, out_dim)</span>
<span id="cb20-72"><a href="#cb20-72" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb20-73"><a href="#cb20-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-74"><a href="#cb20-74" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x, shapes<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb20-75"><a href="#cb20-75" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.initial_conv(x)</span>
<span id="cb20-76"><a href="#cb20-76" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.initial_norm(x.permute(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>)).permute(<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>)</span>
<span id="cb20-77"><a href="#cb20-77" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> shapes:</span>
<span id="cb20-78"><a href="#cb20-78" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(x.shape)</span>
<span id="cb20-79"><a href="#cb20-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="va">self</span>.conv_layers:</span>
<span id="cb20-80"><a href="#cb20-80" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> l(x)</span>
<span id="cb20-81"><a href="#cb20-81" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> shapes:</span>
<span id="cb20-82"><a href="#cb20-82" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(x.shape)</span>
<span id="cb20-83"><a href="#cb20-83" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> F.gelu(x)</span>
<span id="cb20-84"><a href="#cb20-84" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Now concatenate the CLS token with the output of the convolutional layers</span></span>
<span id="cb20-85"><a href="#cb20-85" aria-hidden="true" tabindex="-1"></a>        cls <span class="op">=</span> <span class="va">self</span>.cls_token(torch.arange(<span class="dv">1</span>, device<span class="op">=</span>x.device)).squeeze().expand(x.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb20-86"><a href="#cb20-86" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> torch.cat([cls, x], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb20-87"><a href="#cb20-87" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb20-88"><a href="#cb20-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> l <span class="kw">in</span> <span class="va">self</span>.transformer_layers:</span>
<span id="cb20-89"><a href="#cb20-89" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> l(x)</span>
<span id="cb20-90"><a href="#cb20-90" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> shapes:</span>
<span id="cb20-91"><a href="#cb20-91" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(x.shape)</span>
<span id="cb20-92"><a href="#cb20-92" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.final_norm(x[:,<span class="dv">0</span>,:]) <span class="co"># Extract only the CLS token representation</span></span>
<span id="cb20-93"><a href="#cb20-93" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> shapes:</span>
<span id="cb20-94"><a href="#cb20-94" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(x.shape)</span>
<span id="cb20-95"><a href="#cb20-95" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.output(x)</span>
<span id="cb20-96"><a href="#cb20-96" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb20-97"><a href="#cb20-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-98"><a href="#cb20-98" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ConvNeXtTransformer(depths<span class="op">=</span>[<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">6</span>,<span class="dv">2</span>], dims<span class="op">=</span>[<span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">12</span>,<span class="dv">16</span>], transformer_layers<span class="op">=</span><span class="dv">6</span>, transformer_heads<span class="op">=</span><span class="dv">2</span>).to(device)</span>
<span id="cb20-99"><a href="#cb20-99" aria-hidden="true" tabindex="-1"></a>_ <span class="op">=</span> model(torch.randn(<span class="dv">100</span>,<span class="dv">1</span>,<span class="dv">2048</span>).to(device), shapes<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-100"><a href="#cb20-100" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Model parameters:"</span>,<span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> model.parameters() <span class="cf">if</span> p.requires_grad))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Model parameters: 57135</code></pre>
</div>
</div>
<p>Let’s train this new model and see how the results compare. Just for the sake of comparison, we’ll also train a model on all three of the cyrstal systems of interest simultaneously to see how it compares.</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>splits <span class="op">=</span> [<span class="st">"Triclinic"</span>, <span class="st">"Monoclinic"</span>, <span class="st">"Orthorhombic"</span>, <span class="st">"All_Three"</span>]</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>models <span class="op">=</span> []</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, subset <span class="kw">in</span> <span class="bu">enumerate</span>([triclinic_data, monoclinic_data, orthorhombic_data, trimonoortho]):</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> ConvNeXtTransformer(depths<span class="op">=</span>[<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">6</span>,<span class="dv">2</span>], dims<span class="op">=</span>[<span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">12</span>,<span class="dv">16</span>], transformer_layers<span class="op">=</span><span class="dv">6</span>, transformer_heads<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>                                drop_path_prob<span class="op">=</span><span class="dv">0</span>, dropout<span class="op">=</span><span class="dv">0</span>).to(device)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> torch.optim.Adam(params<span class="op">=</span>model.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(splits[i])</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    train_metrics, val_metrics <span class="op">=</span> train(</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>                                    model<span class="op">=</span>model,</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>                                    optimizer<span class="op">=</span>optimizer,</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>                                    idx<span class="op">=</span>subset,</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>                                    name<span class="op">=</span><span class="ss">f"ConvNeXtTransformer</span><span class="sc">{</span>splits[i]<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>                                    num_epochs<span class="op">=</span><span class="dv">50</span>,</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>                                    batchsize<span class="op">=</span>(<span class="dv">64</span><span class="op">*</span><span class="dv">3</span>)<span class="op">//</span><span class="dv">2</span>,</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>                                    show_pbar<span class="op">=</span><span class="va">False</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>                                    )</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the minimum validation loss for each metric</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    val_min <span class="op">=</span> torch.cat([x.mean(dim<span class="op">=</span><span class="dv">0</span>,keepdim<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> x <span class="kw">in</span> [torch.cat(y, dim<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> y <span class="kw">in</span> val_metrics]], dim<span class="op">=</span><span class="dv">0</span>).<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).values</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Best </span><span class="sc">{</span>splits[i]<span class="sc">}</span><span class="ss"> val MSE: </span><span class="sc">{</span>val_min[<span class="dv">0</span>]<span class="sc">:.3f}</span><span class="ss"> MAE: </span><span class="sc">{</span>val_min[<span class="dv">1</span>]<span class="sc">:.3f}</span><span class="ss"> MAPE: </span><span class="sc">{</span>val_min[<span class="dv">2</span>]<span class="sc">:.3f}</span><span class="ss"> Huber: </span><span class="sc">{</span>val_min[<span class="dv">3</span>]<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Calculate the average loss for each epoch and plot</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    train_av <span class="op">=</span> torch.cat([x.mean(dim<span class="op">=</span><span class="dv">0</span>,keepdim<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> x <span class="kw">in</span> [torch.cat(y, dim<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> y <span class="kw">in</span> train_metrics]], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>    val_av <span class="op">=</span> torch.cat([x.mean(dim<span class="op">=</span><span class="dv">0</span>,keepdim<span class="op">=</span><span class="va">True</span>) <span class="cf">for</span> x <span class="kw">in</span> [torch.cat(y, dim<span class="op">=</span><span class="dv">0</span>) <span class="cf">for</span> y <span class="kw">in</span> val_metrics]], dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">2</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">6</span>))</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>    fig.suptitle(<span class="st">"ConvNeXt Transformer "</span><span class="op">+</span>splits[i])</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">0</span>].plot(train_av[:,<span class="dv">0</span>])</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">0</span>].plot(val_av[:,<span class="dv">0</span>])</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">0</span>].title.set_text(<span class="st">"MSE"</span>)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">0</span>].legend([<span class="st">"Train"</span>, <span class="st">"Val"</span>])</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">1</span>].plot(train_av[:,<span class="dv">1</span>])</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">1</span>].plot(val_av[:,<span class="dv">1</span>])</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">1</span>].title.set_text(<span class="st">"MAE"</span>)</span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">0</span>][<span class="dv">1</span>].legend([<span class="st">"Train"</span>, <span class="st">"Val"</span>])</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">0</span>].plot(train_av[:,<span class="dv">2</span>])</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">0</span>].plot(val_av[:,<span class="dv">2</span>])</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">0</span>].title.set_text(<span class="st">"MAPE"</span>)</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">0</span>].legend([<span class="st">"Train"</span>, <span class="st">"Val"</span>])</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">1</span>].plot(train_av[:,<span class="dv">3</span>])</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">1</span>].plot(val_av[:,<span class="dv">3</span>])</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">1</span>].title.set_text(<span class="st">"Huber"</span>)</span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>    ax[<span class="dv">1</span>][<span class="dv">1</span>].legend([<span class="st">"Train"</span>, <span class="st">"Val"</span>])</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>    plt.show()</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a>    models.append(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Triclinic
Training samples = 55230
Validation samples = 13807
Epochs: 50 Total gradient updates: 28750
Best Triclinic val MSE: 1.304 MAE: 0.686 MAPE: 6.544 Huber: 0.376</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2024-02-11-Indexing-PXRD-data-with-Convolutional-Neural-Networks_files/figure-html/cell-12-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Monoclinic
Training samples = 120031
Validation samples = 30007
Epochs: 50 Total gradient updates: 62500
Best Monoclinic val MSE: 8.661 MAE: 1.844 MAPE: 13.759 Huber: 1.432</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2024-02-11-Indexing-PXRD-data-with-Convolutional-Neural-Networks_files/figure-html/cell-12-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Orthorhombic
Training samples = 40522
Validation samples = 10130
Epochs: 50 Total gradient updates: 21100
Best Orthorhombic val MSE: 6.986 MAE: 1.594 MAPE: 13.380 Huber: 1.192</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2024-02-11-Indexing-PXRD-data-with-Convolutional-Neural-Networks_files/figure-html/cell-12-output-6.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
All_Three
Training samples = 215782
Validation samples = 53945
Epochs: 50 Total gradient updates: 112350
Best All_Three val MSE: 10.544 MAE: 1.991 MAPE: 14.974 Huber: 1.581</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="2024-02-11-Indexing-PXRD-data-with-Convolutional-Neural-Networks_files/figure-html/cell-12-output-8.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code></code></pre>
</div>
</div>
<table class="table">
<colgroup>
<col style="width: 10%">
<col style="width: 19%">
<col style="width: 21%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>Crystal system</th>
<th>Chitturi et al - (100/0)</th>
<th>Chitturi et al - (0/100)</th>
<th>Replication (50/50)</th>
<th>ConvNeXt v2 (50/50)</th>
<th>ConvNeXt v2 + Transformer (50/50)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Triclinic</td>
<td>7.50</td>
<td>10.48</td>
<td>7.90</td>
<td>6.67</td>
<td>6.54</td>
</tr>
<tr class="even">
<td>Monoclinic</td>
<td>13.50</td>
<td>16.02</td>
<td>14.46</td>
<td>13.89</td>
<td>13.76</td>
</tr>
<tr class="odd">
<td>Orthorhombic</td>
<td>15.71</td>
<td>19.49</td>
<td>19.49</td>
<td>12.99</td>
<td>13.38</td>
</tr>
</tbody>
</table>
<p>We achieve effectively the same level of performance despite significantly reducing the number of parameters used by the model! The loss curves are again still decreasing, and we don’t see any significant deviations between the training and validation losses, and as such the model is likely to improve with more training epochs. We could probably also scale up the model (with regularization if needed) and achieve even better results.</p>
<p>When training on all of the crystal systems simultaneously, the MAPE achieved is 14.97, which is competitive with the performance on monoclinic and orthorhombic cells for the replicated Chitturi et al model, though a little worse for the Triclinic case.</p>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>We were able to replicate the results described by Chitturi <em>et al</em> using the PowCodGen code we developed in a previous post. Using more modern architectural choices, we were able to improve the performance of the model whilst also reducing the number of parameters it required!</p>
<p>In each case, the models were undertrained (i.e.&nbsp;losses still decreasing at the end of the 50 epochs) and as such, it should be reasonable to expect that the performance of all of the models could be improved simply by training for longer. Of course, scaling up the models (with the appropriate regularisation during training), is also likely to help things.</p>
<p>In my next post, we’ll look at a more novel use of neural networks in this context, which may be of interest to users with data that is proving difficult to index.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>
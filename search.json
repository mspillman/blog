[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Mark.\nI completed my undergraduate degree in Chemistry at the University of Oxford in 2010, before pursuing my PhD at the University of Reading under the supervision of Professor Kenneth Shankland.\nI now do scientific research in my spare time. I’m principally interested in solving the crystal structures of small molecules using powder diffraction data - a continuation of the topic of my PhD. I also like learning about and applying machine learning to interesting problems.\nWhen I’m not doing science, I enjoy BJJ, playing the drums and cooking."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Generating synthetic diffraction data from PowCod database\n\n\n\n\n\n\n\nPXRD\n\n\nMachine learning\n\n\nSynthetic data\n\n\n\n\nGenerating synthetic powder X-ray diffraction data for machine learning applications\n\n\n\n\n\n\nSep 8, 2023\n\n\nMark Spillman\n\n\n\n\n\n\n  \n\n\n\n\nSolving crystal structures with GALLOP after ill conditioning errors in DASH\n\n\n\n\n\n\n\nPXRD\n\n\nGALLOP\n\n\nProfile\n\n\n\n\nPawley refinement in DASH at too high a resolution can result in a covariance matrix with ill conditioning. Using the profile \\(\\chi^2\\), can we still solve the structure?\n\n\n\n\n\n\nJan 23, 2023\n\n\nMark Spillman\n\n\n\n\n\n\n  \n\n\n\n\nRestraints for rings and stereochemistry\n\n\n\n\n\n\n\nGALLOP\n\n\nPXRD\n\n\nPython\n\n\nRestraints\n\n\n\n\nSolving structures with unknown ring conformations or stereochemistry can be difficult and time consuming. Suitable application of restraints can help.\n\n\n\n\n\n\nMay 30, 2022\n\n\nMark Spillman\n\n\n\n\n\n\n  \n\n\n\n\nSolving crystal structures with GALLOP and profile \\(\\chi^2\\)\n\n\n\n\n\n\n\nPXRD\n\n\nGALLOP\n\n\nProfile\n\n\n\n\nWith CPU-based code, intensity \\(\\chi^2\\) is more efficient, but does this hold for GPU-based code?\n\n\n\n\n\n\nJan 8, 2022\n\n\nMark Spillman\n\n\n\n\n\n\n  \n\n\n\n\nInstalling and running GALLOP on cloud GPUs\n\n\n\n\n\n\n\nGALLOP\n\n\nCloud\n\n\nGCE\n\n\n\n\nA guide to setting up GALLOP on a GPU-equipped virtual machine using Google Compute Engine\n\n\n\n\n\n\nNov 18, 2021\n\n\nMark Spillman\n\n\n\n\n\n\n  \n\n\n\n\nEstimating molecular volumes to aid in powder X-ray diffraction indexing\n\n\n\n\n\n\n\nPXRD\n\n\nIndexing\n\n\n\n\nAn overview of using database-derived atomic volumes to aid PXRD indexing.\n\n\n\n\n\n\nNov 10, 2021\n\n\nMark Spillman\n\n\n\n\n\n\n  \n\n\n\n\nVisualising hypersurfaces and optimisation trajectories\n\n\n\n\n\n\n\nGALLOP\n\n\nPXRD\n\n\nPython\n\n\n\n\nUsing low-dimensional slices, it’s possible to visualise both the hypersurface and trajectories taken by local-optimisers\n\n\n\n\n\n\nNov 7, 2021\n\n\nMark Spillman\n\n\n\n\n\n\n  \n\n\n\n\nSolving structures with the GALLOP Python API - basic use\n\n\n\n\n\n\n\nGALLOP\n\n\nPXRD\n\n\nPython\n\n\n\n\nA walkthrough for using the GALLOP Python API to solve the crystal structures of small molecules\n\n\n\n\n\n\nNov 3, 2021\n\n\nMark Spillman\n\n\n\n\n\n\n  \n\n\n\n\nSolving structures with the GALLOP browser interface\n\n\n\n\n\n\n\nGALLOP\n\n\nPXRD\n\n\nPython\n\n\n\n\nA walkthrough for using the GALLOP browser interface to solve the crystal structures of small molecules\n\n\n\n\n\n\nNov 2, 2021\n\n\nMark Spillman\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to GALLOP\n\n\n\n\n\n\n\nGALLOP\n\n\nPXRD\n\n\nPython\n\n\n\n\nAn introduction to GALLOP, a recently published method for solving the crystal structures of small molecules\n\n\n\n\n\n\nOct 30, 2021\n\n\nMark Spillman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-10-30-introduction-to-gallop.html",
    "href": "posts/2021-10-30-introduction-to-gallop.html",
    "title": "Introduction to GALLOP",
    "section": "",
    "text": "This post is intended to give an overview of GALLOP, an algorithm I recently published alongside my friend, PhD supervisor and colleague, Prof. Kenneth Shankland. If you aren’t familiar with global optimisation (GO) methods for crystal structure determination from powder diffraction data (SDPD), I recommend the following resources: - Experimental Analysis of Powder Diﬀraction Data - An overview of currently used structure determination methods for powder diffraction data - The principles underlying the use of powder diffraction data in solving pharmaceutical crystal structures\nAbbreviations I’ll be using: - SDPD = Crystal structure determination from powder diffraction data - PXRD = Powder X-ray diffraction data - GO = Global optimisation - LO = Local optimisation - GPU = Graphics processing unit - CPU = Central processing unit - SA = Simulated annealing - PSO = Particle swarm optimisation - ML = Machine learning\n\n\nGALLOP is the culmination of several years of work, which kicked off back in 2010 with an article published by Kenneth Shankland and co-workers, who showed that contrary to the wisdom at the time, local optimisation (LO) algorithms were capable of solving the crystal structures of small molecules, provided that several tens of thousands of attempts from random starting positions on the \\(\\chi^{2}\\) hypersurface were performed. In addition to solving the crystal structure, this also gives the locations of the stationary points on the hypersurface.\nInterestingly, they showed that using this method, the global minimum on the hypersurface was located more frequently than any other minimum. This indicates that “the topology of the surface is such that the net ‘catchment’ area of stationary points with very low values of \\(\\chi^{2}\\) is significantly larger than that of the vast majority of stationary points.” The figure below, taken from the article, shows the distribution of \\(\\chi^{2}\\) values for stationary points on the 15-dimensional hypersurface for capsaicin.\n\n\n\nShankland et al, 2010\n\n\nI carried on investigating this method as part of my PhD, and my results confirmed that this approach is effective at solving crystal structures, even of high-complexity (up to 42 degrees of freedom!). However, despite the intriguing results, the approach was not adopted on a wide scale by the SDPD community, perhaps because the performance it offers is approximately the same existing GO-based programs. The code I was using was written in C++, a language I do not know at all well, so I was unable to contribute much to its further development.\nA few years after finishing my PhD, I decided I wanted to try writing my own SDPD code in Python. Whilst Python is notoriously slow, my rationale was that Python is much easier to learn than C++, so should provide a lower barrier to entry for people seeking to try out new ideas for SDPD. My first prototype used numpy to try to speed up the code, and borrowed heavily from pymatgen, a fantastic open-source library with lots of crystallographic functionality. Eventually with some help from Kenneth, I had a system which allowed me to easily try out lots of different algorithms, such as those included in scipy.optimize, which features a variety of local and global optimisation algorithms.\nIn parallel to this, it seemed like every day incredible new results from the field of deep learning were coming out, showing state-of-the-art performance in wide variety of domains. Most neural networks are trained using backpropagation, an algorithm which makes use of automatic differentiation to calculate the gradient of the cost function (which provides a measure of how well the neural network is performing its task) with respect to the parameters of the neural network. Variants of stochastic gradient descent are then used to modify the parameters of the neural network in order to improve the performance of the neural network as measured by the cost function. Whilst neural networks have been in use for over half a century, part of the reason for the explosion in activity was the availability of GPUs and tools to leverage their parallel processing capabilities.\nI took an interest in this, and quickly realised that most of the libraries used for this work had well supported python APIs. Some of them, such as PyTorch, are so similar to numpy that it seemed logical to try to port my code to make use of these libraries. This would give both GPU-acceleration and automatic differentiation capabilities for relatively little effort!\n\n\n\nWith my code now capable of running on GPUs, it might seem obvious to implement GPU-versions of commonly used existing algorithms for SDPD such as simulated annealing (SA), parallel tempering and others. However, despite the parallel processing capabilities of GPUs, I found that the performance with GO methods is not particularly good (at least with my code!). Using SA as an example, then yes, it’s possible to run thousands of simultaneous runs on a single GPU, but the number of iterations that can be performed per second is laughably slow in comparison to performance on a CPU. Therefore, because algorithms like SA take a large number of iterations converge, the performance benefits of parallel processing are offset by the amount of time needed to process the large number of iterations required to reach the global minimum.\nIn contrast to GO algorithms, LO algorithms with access to gradients converge much more rapidly. The automatic differentiation capabilities provided by PyTorch allow gradients to be calculated rapidly, without any additional code to be written. The gradients so obtained are exactly equivalent to the analytical gradient, and are obtained much more rapidly than the approximate gradient that would be obtained via the method of finite differences. Therefore, when processing large numbers of LO runs on a GPU, because they converge much more rapidly than GO methods, you don’t need to wait for a long time to know if any of the runs have been successful!\nThe next piece of the puzzle is the idea that even if the global minimum is not located (i.e. the structure hasn’t yet been solved), the previously optimised positions may contain some useful information about the crystal structure. This might not be obvious at first, but let’s try to convince ourselves by taking a look at the hypersurface of verapamil hydrochloride, a structure with 23 degrees of freedom. This interactive figure shows a 2D slice through the hypersurface with all degrees of freedom set to their correct crystallographic values, apart from the fractional coordinates of the verapamil molecule along a and b, which form the axes plotted here.\n\n\nCode\n# This cell generates the Plotly html figure\nimport numpy as np\nfrom plotly.offline import init_notebook_mode\nfrom IPython.display import HTML\nimport plotly.graph_objects as go\nimport torch\nimport pandas as pd\nimport os\n\ndisplay(HTML('''&lt;script src=\"/static/components/requirejs/require.js\"&gt;&lt;/script&gt;'''))\ninit_notebook_mode(connected=False)\n\nnum = 151 # dimensionality of grid for the plot\nx = np.linspace(0,1,num)\ny = np.linspace(0,1,num)\n\nxx, yy = np.meshgrid(x, y)\n\nif not os.path.exists(\"chisquared.csv\"):\n\n    from gallop.structure import Structure\n    from gallop import tensor_prep\n    from gallop import zm_to_cart\n    from gallop import intensities\n    from gallop import chi2\n    from gallop import files\n\n\n\n    struct = Structure(name=\"Verap\", ignore_H_atoms=True)\n    struct.add_data(\"./files/Verap.sdi\", source=\"DASH\")\n    struct.add_zmatrix(\"./files/CURHOM_1.zmatrix\", verbose=False)\n    struct.add_zmatrix(\"./files/CURHOM_2.zmatrix\", verbose=False)\n    struct.get_total_degrees_of_freedom(verbose=False)\n\n    # Coordinates of global minimum\n    g_external = np.array([0.68394345,0.45950916,-0.114383094,0.2456631,-0.29722878,\n                            -0.10422839,0.3358067,0.7402369,0.7596887,-0.16798618])\n\n    g_internal = np.array([-1.2276717,2.9165819,-0.96692395,-1.2034712,3.2261908,\n                            2.1036072,-3.2097023,-3.0328763,1.5661795,-0.0071008434,\n                            -0.1824131,-0.05715108,0.27950087])\n\n    global_min = np.hstack([g_external,g_internal])\n\n    vectors = np.zeros((2, struct.total_degrees_of_freedom)).astype(np.float32)\n    dims = (3,4) # This corresponds to verap along a, b\n    vectors[0][dims[0]] = 1\n    vectors[1][dims[1]] = 1\n\n    points = []\n    for x_point in x:\n        for y_point in y:\n            temp = np.copy(global_min)\n            temp[dims[0]] = 0\n            temp[dims[1]] = 0\n            points.append(temp + (vectors[0]*x_point) + (vectors[1]*y_point))\n\n    points = np.array(points)\n    external = points[:,:struct.total_external_degrees_of_freedom]\n    internal = points[:,struct.total_external_degrees_of_freedom:]\n\n    tensors = tensor_prep.get_all_required_tensors(struct, external=external,\n                                            internal=internal, requires_grad=False)\n\n    asymmetric_frac_coords = zm_to_cart.get_asymmetric_coords(**tensors[\"zm\"])\n\n    calculated_intensities = intensities.calculate_intensities(asymmetric_frac_coords,\n                                                         **tensors[\"int_tensors\"])\n\n    chisquared = chi2.calc_int_chisqd(calculated_intensities, **tensors[\"chisqd_tensors\"])\n\n    chisquared = chisquared.cpu().numpy()\n\n    np.savetxt(\"chisquared.csv\", chisquared, delimiter=\",\")\n\nelse:\n    df = pd.read_csv(\"chisquared.csv\", header=None)\n    chisquared = df.values\n\n# create figure\nfig = go.Figure()\n\n# Add surface trace\nfig.add_trace(go.Surface(z=chisquared.reshape(xx.shape).T, x=x, y=y, colorscale=\"Inferno\"))\n\n# Update plot sizing\nfig.update_layout(\n    width=700,\n    height=600,\n    autosize=False,\n    margin=dict(t=0, b=0, l=0, r=0),\n    template=\"plotly_white\",\n)\n\n# Update 3D scene options\nfig.update_scenes(\n    aspectratio=dict(x=1, y=1, z=0.8),\n    aspectmode=\"manual\"\n)\n\n# Add dropdown\nfig.update_layout(\n    updatemenus=[\n        dict(\n            buttons=list([\n                dict(\n                    args=[\"type\", \"surface\"],\n                    label=\"3D Surface\",\n                    method=\"restyle\"\n                ),\n                dict(\n                    args=[\"type\", \"contour\"],\n                    label=\"Contour\",\n                    method=\"restyle\"\n                )\n            ]),\n            direction=\"down\",\n            pad={\"r\": 10, \"t\": 10},\n            showactive=True,\n            x=0.1,\n            xanchor=\"left\",\n            y=1.1,\n            yanchor=\"top\"\n        ),\n    ]\n)\n\n# Add annotation\nfig.update_layout(\n    annotations=[\n        dict(text=\"Plot type:\", showarrow=False,\n        x=0, y=1.085, yref=\"paper\", align=\"left\")\n    ],\n    xaxis=dict(\n        title='a',\n    ),\n    yaxis=dict(\n        title='b',\n    ),\n    scene_camera_eye=dict(x=-1, y=-3, z=0.9),\n)\n\nfig.show()\n\n\n\n\n\n\n                                                \n\n\nI’ll write another blog post in the future showing how to use the GALLOP code to generate plots like this.\nThere are several local minima present, each of which represents an incorrect structure that has either 21 or 22 of its 23 degrees of freedom correctly determined. Despite this, the \\(\\chi^{2}\\) at each local minimum gives no indication that the result is so close to correct. This means that even failed runs with high values of \\(\\chi^{2}\\) may be close to the global minimum. If we could accumulate and exploit this information to influence where the next set of local optimisations start, then it might be possible to save time in searching for the global minimum.\nWith this idea in mind, I tried a few things to attempt to recycle information from “failed” local optimisation attempts, including using kernel density estimation to try to resample various degrees of freedom depending based on the density of solutions that ended up with particular coordinates. This definitely showed an improvement relative to random sampling, but proved inconsistent in terms of the level of improvement obtained. Perhaps I’ll revisit this in a future blog post as I still think there’s something there that may be of use.\nEventually, I ended up trying particle swarm optimisation (PSO) to attempt to recycle the optimised positions. A few things attracted me to PSO: 1. It’s a simple algorithm to implement - just a few lines of code got it working as a proof of concept 2. The algorithm maintains a memory of “good” solutions so there’s less risk of the algorithm moving in a bad direction and getting stuck there 3. It’s shown great performance in a wide variety of domains\nThe performance improvement with PSO included was immediately obvious.\nThe last thing that was needed was a name. GPU-Accelerated LocaL Optimisation and Particle swarm provides both a description of the algorithm and an acronym that gives a hat-tip to DASH. Perfect!"
  },
  {
    "objectID": "posts/2021-10-30-introduction-to-gallop.html#background",
    "href": "posts/2021-10-30-introduction-to-gallop.html#background",
    "title": "Introduction to GALLOP",
    "section": "",
    "text": "GALLOP is the culmination of several years of work, which kicked off back in 2010 with an article published by Kenneth Shankland and co-workers, who showed that contrary to the wisdom at the time, local optimisation (LO) algorithms were capable of solving the crystal structures of small molecules, provided that several tens of thousands of attempts from random starting positions on the \\(\\chi^{2}\\) hypersurface were performed. In addition to solving the crystal structure, this also gives the locations of the stationary points on the hypersurface.\nInterestingly, they showed that using this method, the global minimum on the hypersurface was located more frequently than any other minimum. This indicates that “the topology of the surface is such that the net ‘catchment’ area of stationary points with very low values of \\(\\chi^{2}\\) is significantly larger than that of the vast majority of stationary points.” The figure below, taken from the article, shows the distribution of \\(\\chi^{2}\\) values for stationary points on the 15-dimensional hypersurface for capsaicin.\n\n\n\nShankland et al, 2010\n\n\nI carried on investigating this method as part of my PhD, and my results confirmed that this approach is effective at solving crystal structures, even of high-complexity (up to 42 degrees of freedom!). However, despite the intriguing results, the approach was not adopted on a wide scale by the SDPD community, perhaps because the performance it offers is approximately the same existing GO-based programs. The code I was using was written in C++, a language I do not know at all well, so I was unable to contribute much to its further development.\nA few years after finishing my PhD, I decided I wanted to try writing my own SDPD code in Python. Whilst Python is notoriously slow, my rationale was that Python is much easier to learn than C++, so should provide a lower barrier to entry for people seeking to try out new ideas for SDPD. My first prototype used numpy to try to speed up the code, and borrowed heavily from pymatgen, a fantastic open-source library with lots of crystallographic functionality. Eventually with some help from Kenneth, I had a system which allowed me to easily try out lots of different algorithms, such as those included in scipy.optimize, which features a variety of local and global optimisation algorithms.\nIn parallel to this, it seemed like every day incredible new results from the field of deep learning were coming out, showing state-of-the-art performance in wide variety of domains. Most neural networks are trained using backpropagation, an algorithm which makes use of automatic differentiation to calculate the gradient of the cost function (which provides a measure of how well the neural network is performing its task) with respect to the parameters of the neural network. Variants of stochastic gradient descent are then used to modify the parameters of the neural network in order to improve the performance of the neural network as measured by the cost function. Whilst neural networks have been in use for over half a century, part of the reason for the explosion in activity was the availability of GPUs and tools to leverage their parallel processing capabilities.\nI took an interest in this, and quickly realised that most of the libraries used for this work had well supported python APIs. Some of them, such as PyTorch, are so similar to numpy that it seemed logical to try to port my code to make use of these libraries. This would give both GPU-acceleration and automatic differentiation capabilities for relatively little effort!"
  },
  {
    "objectID": "posts/2021-10-30-introduction-to-gallop.html#rationale-for-gallop",
    "href": "posts/2021-10-30-introduction-to-gallop.html#rationale-for-gallop",
    "title": "Introduction to GALLOP",
    "section": "",
    "text": "With my code now capable of running on GPUs, it might seem obvious to implement GPU-versions of commonly used existing algorithms for SDPD such as simulated annealing (SA), parallel tempering and others. However, despite the parallel processing capabilities of GPUs, I found that the performance with GO methods is not particularly good (at least with my code!). Using SA as an example, then yes, it’s possible to run thousands of simultaneous runs on a single GPU, but the number of iterations that can be performed per second is laughably slow in comparison to performance on a CPU. Therefore, because algorithms like SA take a large number of iterations converge, the performance benefits of parallel processing are offset by the amount of time needed to process the large number of iterations required to reach the global minimum.\nIn contrast to GO algorithms, LO algorithms with access to gradients converge much more rapidly. The automatic differentiation capabilities provided by PyTorch allow gradients to be calculated rapidly, without any additional code to be written. The gradients so obtained are exactly equivalent to the analytical gradient, and are obtained much more rapidly than the approximate gradient that would be obtained via the method of finite differences. Therefore, when processing large numbers of LO runs on a GPU, because they converge much more rapidly than GO methods, you don’t need to wait for a long time to know if any of the runs have been successful!\nThe next piece of the puzzle is the idea that even if the global minimum is not located (i.e. the structure hasn’t yet been solved), the previously optimised positions may contain some useful information about the crystal structure. This might not be obvious at first, but let’s try to convince ourselves by taking a look at the hypersurface of verapamil hydrochloride, a structure with 23 degrees of freedom. This interactive figure shows a 2D slice through the hypersurface with all degrees of freedom set to their correct crystallographic values, apart from the fractional coordinates of the verapamil molecule along a and b, which form the axes plotted here.\n\n\nCode\n# This cell generates the Plotly html figure\nimport numpy as np\nfrom plotly.offline import init_notebook_mode\nfrom IPython.display import HTML\nimport plotly.graph_objects as go\nimport torch\nimport pandas as pd\nimport os\n\ndisplay(HTML('''&lt;script src=\"/static/components/requirejs/require.js\"&gt;&lt;/script&gt;'''))\ninit_notebook_mode(connected=False)\n\nnum = 151 # dimensionality of grid for the plot\nx = np.linspace(0,1,num)\ny = np.linspace(0,1,num)\n\nxx, yy = np.meshgrid(x, y)\n\nif not os.path.exists(\"chisquared.csv\"):\n\n    from gallop.structure import Structure\n    from gallop import tensor_prep\n    from gallop import zm_to_cart\n    from gallop import intensities\n    from gallop import chi2\n    from gallop import files\n\n\n\n    struct = Structure(name=\"Verap\", ignore_H_atoms=True)\n    struct.add_data(\"./files/Verap.sdi\", source=\"DASH\")\n    struct.add_zmatrix(\"./files/CURHOM_1.zmatrix\", verbose=False)\n    struct.add_zmatrix(\"./files/CURHOM_2.zmatrix\", verbose=False)\n    struct.get_total_degrees_of_freedom(verbose=False)\n\n    # Coordinates of global minimum\n    g_external = np.array([0.68394345,0.45950916,-0.114383094,0.2456631,-0.29722878,\n                            -0.10422839,0.3358067,0.7402369,0.7596887,-0.16798618])\n\n    g_internal = np.array([-1.2276717,2.9165819,-0.96692395,-1.2034712,3.2261908,\n                            2.1036072,-3.2097023,-3.0328763,1.5661795,-0.0071008434,\n                            -0.1824131,-0.05715108,0.27950087])\n\n    global_min = np.hstack([g_external,g_internal])\n\n    vectors = np.zeros((2, struct.total_degrees_of_freedom)).astype(np.float32)\n    dims = (3,4) # This corresponds to verap along a, b\n    vectors[0][dims[0]] = 1\n    vectors[1][dims[1]] = 1\n\n    points = []\n    for x_point in x:\n        for y_point in y:\n            temp = np.copy(global_min)\n            temp[dims[0]] = 0\n            temp[dims[1]] = 0\n            points.append(temp + (vectors[0]*x_point) + (vectors[1]*y_point))\n\n    points = np.array(points)\n    external = points[:,:struct.total_external_degrees_of_freedom]\n    internal = points[:,struct.total_external_degrees_of_freedom:]\n\n    tensors = tensor_prep.get_all_required_tensors(struct, external=external,\n                                            internal=internal, requires_grad=False)\n\n    asymmetric_frac_coords = zm_to_cart.get_asymmetric_coords(**tensors[\"zm\"])\n\n    calculated_intensities = intensities.calculate_intensities(asymmetric_frac_coords,\n                                                         **tensors[\"int_tensors\"])\n\n    chisquared = chi2.calc_int_chisqd(calculated_intensities, **tensors[\"chisqd_tensors\"])\n\n    chisquared = chisquared.cpu().numpy()\n\n    np.savetxt(\"chisquared.csv\", chisquared, delimiter=\",\")\n\nelse:\n    df = pd.read_csv(\"chisquared.csv\", header=None)\n    chisquared = df.values\n\n# create figure\nfig = go.Figure()\n\n# Add surface trace\nfig.add_trace(go.Surface(z=chisquared.reshape(xx.shape).T, x=x, y=y, colorscale=\"Inferno\"))\n\n# Update plot sizing\nfig.update_layout(\n    width=700,\n    height=600,\n    autosize=False,\n    margin=dict(t=0, b=0, l=0, r=0),\n    template=\"plotly_white\",\n)\n\n# Update 3D scene options\nfig.update_scenes(\n    aspectratio=dict(x=1, y=1, z=0.8),\n    aspectmode=\"manual\"\n)\n\n# Add dropdown\nfig.update_layout(\n    updatemenus=[\n        dict(\n            buttons=list([\n                dict(\n                    args=[\"type\", \"surface\"],\n                    label=\"3D Surface\",\n                    method=\"restyle\"\n                ),\n                dict(\n                    args=[\"type\", \"contour\"],\n                    label=\"Contour\",\n                    method=\"restyle\"\n                )\n            ]),\n            direction=\"down\",\n            pad={\"r\": 10, \"t\": 10},\n            showactive=True,\n            x=0.1,\n            xanchor=\"left\",\n            y=1.1,\n            yanchor=\"top\"\n        ),\n    ]\n)\n\n# Add annotation\nfig.update_layout(\n    annotations=[\n        dict(text=\"Plot type:\", showarrow=False,\n        x=0, y=1.085, yref=\"paper\", align=\"left\")\n    ],\n    xaxis=dict(\n        title='a',\n    ),\n    yaxis=dict(\n        title='b',\n    ),\n    scene_camera_eye=dict(x=-1, y=-3, z=0.9),\n)\n\nfig.show()\n\n\n\n\n\n\n                                                \n\n\nI’ll write another blog post in the future showing how to use the GALLOP code to generate plots like this.\nThere are several local minima present, each of which represents an incorrect structure that has either 21 or 22 of its 23 degrees of freedom correctly determined. Despite this, the \\(\\chi^{2}\\) at each local minimum gives no indication that the result is so close to correct. This means that even failed runs with high values of \\(\\chi^{2}\\) may be close to the global minimum. If we could accumulate and exploit this information to influence where the next set of local optimisations start, then it might be possible to save time in searching for the global minimum.\nWith this idea in mind, I tried a few things to attempt to recycle information from “failed” local optimisation attempts, including using kernel density estimation to try to resample various degrees of freedom depending based on the density of solutions that ended up with particular coordinates. This definitely showed an improvement relative to random sampling, but proved inconsistent in terms of the level of improvement obtained. Perhaps I’ll revisit this in a future blog post as I still think there’s something there that may be of use.\nEventually, I ended up trying particle swarm optimisation (PSO) to attempt to recycle the optimised positions. A few things attracted me to PSO: 1. It’s a simple algorithm to implement - just a few lines of code got it working as a proof of concept 2. The algorithm maintains a memory of “good” solutions so there’s less risk of the algorithm moving in a bad direction and getting stuck there 3. It’s shown great performance in a wide variety of domains\nThe performance improvement with PSO included was immediately obvious.\nThe last thing that was needed was a name. GPU-Accelerated LocaL Optimisation and Particle swarm provides both a description of the algorithm and an acronym that gives a hat-tip to DASH. Perfect!"
  },
  {
    "objectID": "posts/2021-10-30-introduction-to-gallop.html#local-optimisation",
    "href": "posts/2021-10-30-introduction-to-gallop.html#local-optimisation",
    "title": "Introduction to GALLOP",
    "section": "Local optimisation",
    "text": "Local optimisation\nThe local optimisation algorithm used in GALLOP by default is Adam. This algorithm is very popular for training neural networks, and efficient implementations are available in almost every deep learning library.\nAdam incorporates two distinct innovations that improve its performance relative to (stochastic) gradient descent.\n\nAdam has a per-parameter adaptive step size in addition to a single global step size used for all parameters. This is useful as different degrees of freedom will have different effects on \\(\\chi^2\\) for the same percentage change in the parameter value. For example, the translation of a whole molecule within a unit cell affects the position of more scattering atoms than changing a torsion angle. What’s nice is that Adam automatically adjusts the step size for each parameter as it goes, meaning that a suitable step size is used throughout optimisation.\nAdam incorporates momentum, which helps it to escape shallow local minima, pass rapidly through flat regions of the hypersurface and dampens uncesessary oscillations in the optimisation trajectory. For an excellent overview of momentum (with a focus on ML applications), see this article: https://distill.pub/2017/momentum/\n\n\nAdam\nUsing the gradient obtained by automatic differentiation, \\(\\textbf{g}_t\\), Adam stores exponentially decaying averages of the gradients, \\(\\textbf{m}_t\\), and squared gradients, \\(\\textbf{v}_{t}\\), which are then used in conjunction with the overall step size, \\(\\alpha\\), to give a suitable step size for each parameter being optimised. The parameters \\(\\beta_1\\) and \\(\\beta_2\\) are numbers less than one that control the rate at which the past gradients and squared gradients respectively decay.\n\\[ \\textbf{m}_t = \\beta_{1} \\textbf{m}_{t-1} + (1-\\beta_{1})\\textbf{g}_t \\]\n\\[\\textbf{v}_t = \\beta_{2} \\textbf{v}_{t-1} + (1-\\beta_2)\\textbf{g}_t^2\\]\nBecause \\(\\textbf{m}_t\\) and \\(\\textbf{v}_{t}\\) are initialised as vectors of zeros, the authors of Adam use the following corrective terms to reduce the effect of this biasing, which can be particularly problematic in the early stages of optimisation:\n\\[ \\hat{\\textbf{m}}_t = \\frac{\\textbf{m}_t}{1-\\beta_1}\\]\n\\[ \\hat{\\textbf{v}}_t = \\frac{\\textbf{v}_t}{1-\\beta_2}\\]\nThese bias corrected terms are then used to update the parameters to be optimised, \\(\\textbf{x}_t\\), where \\(\\epsilon\\) is included to prevent numerical errors:\n\\[ \\textbf{x}_{t+1} = \\textbf{x}_t - \\frac{\\alpha}{\\sqrt{\\hat{\\textbf{v}}_t} + \\epsilon}\\hat{\\textbf{m}}_t \\]\nThe authors of Adam suggest default parameters of \\(\\beta_1 = 0.9\\), \\(\\beta_2 = 0.999\\) and \\(\\epsilon = 1 \\times 10^{-8}\\). The step size, \\(\\alpha\\), must be set by the user. By default GALLOP sets \\(\\beta_2 = 0.9\\) which decays the past squared gradients more rapidly, and was found in our testing to be more effective than the default value.\n\n\nLearning rate finder\nTo make life easy for end users (and myself), I wanted a way to avoid having to experiment to find a suitable step size (\\(\\alpha\\)) to use in GALLOP.\nThe deep learning library, fast.ai includes a heuristic known as the learning rate finder, which is used to set the step size (referred to as the learning rate in ML-parlance) for deep learning experiments automatically. This is used in conjunction with a step-size alteration policy which is carried out during optimisation, as described here.\nAfter some testing and experimentation, GALLOP now makes use of a slightly modified version, as described below.\nA set of 200 log-linearly spaced learning rates are initialised, ranging from \\(1 \\times 10^{-4}\\) and \\(0.15\\). Starting with the smallest, GALLOP is run on the structure of interest, and the step size increased to the next value after every iteration. The sum of the \\(\\chi^2\\) values is recorded after each iteration, and subsequently plotted.\nThe minimum point on this plot, \\(\\alpha_{min}\\), is then used to give the step size. It may be scaled after considering the gradient of the line as the step size is increased beyond \\(\\alpha_{min}\\).\nTo do this, the step sizes and \\(\\chi^2\\) values are rescaled into the range 0-1. The x-axis is shifted such that \\(\\alpha_{min}\\) sits at 0, and the data plotted. If the gradient of the resultant curve (approximated by the red straight line below) is &gt; 0.5, then this is considered steep. A steep gradient implies a high sensitivity to the step size, and hence \\(\\alpha_{min}\\) is scaled by a factor of 0.5, i.e. GALLOP runs with a step size of \\(0.5\\alpha_{min}\\). A medium gradient (between 0.25 and 0.5) results in multiplication factor of 0.75, whilst a shallow gradient (less than 0.25) implies relative insensitivity to the step size, and hence results in a multiplication factor of 1.0.\nIn the GALLOP browser interface, this information is provided in a plot:\n\n\n\nStep size finder\n\n\nIn my testing, the learning rates obtained in this manner provide a good first attempt for GALLOP and provide reasonable performance over a wide variety of structures. However, this doesn’t mean that they are optimal, and if a structure isn’t solving, it might be worth looking at this parameter more closely. I tend to find that a learning rate of 0.03 - 0.05 tends to work well as a first attempt for most structures."
  },
  {
    "objectID": "posts/2021-10-30-introduction-to-gallop.html#particle-swarm-optimisation",
    "href": "posts/2021-10-30-introduction-to-gallop.html#particle-swarm-optimisation",
    "title": "Introduction to GALLOP",
    "section": "Particle Swarm optimisation",
    "text": "Particle Swarm optimisation\nParticle Swarm Optimisation has previously been used in the context of SDPD in the program PeckCryst. The algorithm used in GALLOP is different to that used in PeckCryst in a number of ways which I’ll try to highlight below.\nThe equations for the PSO are simple. The velocity of a particle at step \\(t+1\\) \\((\\textbf{v}_{t+1})\\), is calculated from the velocity at the previous step \\((\\textbf{v}_{t})\\) and the position of the particle \\((\\textbf{x}_t)\\) using the following equation:\n\\[\\textbf{v}_{t+1} = \\omega_{t} \\textbf{v}_{t} + c_{1}\\textbf{R}_1(\\textbf{g}_{best} - \\textbf{x}_t) + c_2\\textbf{R}_2(\\textbf{x}_{best} - \\textbf{x}_t)\\]\nWhere \\(\\omega_{t}\\) is the inertia of the particle (which controls how much the previous velocity influences the next velocity) and \\(c_1\\) and \\(c_2\\) control the maximum step size in the direction of the best particle in the swarm \\((\\textbf{g}_{best})\\) and best position previously visited by the particle \\((\\textbf{x}_{best})\\) respectively. In contrast to PeckCryst which uses scalars, in GALLOP, by default \\(\\textbf{R}_1\\) and \\(\\textbf{R}_2\\) are diagonal matrices with their elements drawn independently from a standard uniform distribution. This provides more variability in how the particles move which helps to improve the exploration. In addition, the maximum absolute velocity in each direction in GALLOP is limited to 1.0.\nGALLOP calculates \\(\\omega\\) for particle \\(i\\) by ranking all of the \\(N\\) particles in the swarm in terms of their \\(\\chi^2\\) value, and the calculating their inertia using the following equation:\n\\[ \\omega_i = 0.4 + \\frac{Rank_i}{2N} \\]\nwhere \\(Rank_i\\) is the position of particle \\(i\\) in a sorted list of their respective \\(\\chi^2\\) values. This gives inertia values in the range 0.4 - 0.9, and means that the best particles slow down, whilst the worst particles in the swarm have higher inertias and hence are able to continue moving more rapidly towards (hopefully) promising areas of the hypersurface.\nThe degrees of freedom are then updated using the velocity and previous parameters according to:\n\\[\\textbf{x}_{t+1} = \\textbf{x}_t + \\textbf{v}_{t+1} \\]\nAnother difference to PeckCryst is the coordinate transform that is performed in GALLOP. The fractional coordinates are transformed to account for the repeating unit cell and the fact that coordinates of -0.1 and 0.9 are equivalent. The torsion angles are also transformed to ensure that the PSO treats angles of +180 and -180 degrees as equivalent. Molecular orientations, represented in GALLOP with quaternions, do not require any transformation. For local optimisation, the degrees of freedom that are optimised are:\n\\[ DoF_{(LO)} = [\\textbf{x}_{positions}, \\textbf{x}_{quaternions}, \\textbf{x}_{torsions}] \\]\nThese are then transformed as follows for use in the PSO:\n\\[ DoF_{(PSO)} = [\\sin{2\\pi\\textbf{x}_{positions}}, \\cos{2\\pi\\textbf{x}_{positions}}, \\textbf{x}_{quaternions}, \\sin{\\textbf{x}_{torsions}}, \\cos{\\textbf{x}_{torsions}}] \\]\nFollowing the PSO update, these are transformed back for use in LO using the two-argument arctangent function which gives values in the range \\(-\\pi\\) to \\(\\pi\\), which therefore necessitates additional scaling by a factor of \\(1/2\\pi\\) for the positions."
  },
  {
    "objectID": "posts/2021-10-30-introduction-to-gallop.html#gallop",
    "href": "posts/2021-10-30-introduction-to-gallop.html#gallop",
    "title": "Introduction to GALLOP",
    "section": "GALLOP",
    "text": "GALLOP\nBringing it all together, this flow chart shows how the GALLOP algorithm operates:\n\n\n\nGallop flow chart\n\n\nTypically, 500 LO steps are performed prior to a single PSO step.\nThe results reported here demonstrate a significant improvement in performance relative to DASH. The success rate is &gt;30 times that of DASH, and the GPU-acceleration means that the time taken to process the runs is also significantly lower than can be accomplished without distributed computing for the DASH jobs.\nIn my next post, I’ll go over how to use GALLOP to solve crystal structures."
  },
  {
    "objectID": "posts/2021-11-02-solving-structures-with-gallop-browser-interface.html",
    "href": "posts/2021-11-02-solving-structures-with-gallop-browser-interface.html",
    "title": "Solving structures with the GALLOP browser interface",
    "section": "",
    "text": "In my previous post, I went over the rationale behind GALLOP and some details about the algorithm.\nIn this post, I’ll go over how to use GALLOP to solve the crystal structure of verapamil hydrochloride, a calcium channel blocker used in the treatment of arryhythmias. If you want to have a go yourself, you can download the diffraction data in xye format here or my complete set of fit files in a zip archive here. If you don’t have access to DASH, I recommend downloading the full set of fit files so you have access to the Z-matrices which may otherwise be tedious to generate by hand.\nWhilst not relevant for verapamil hydrochloride which crystallises in \\(P\\bar{1}\\), note that currently GALLOP requires the space group to be in the standard setting so if you have a known unit cell in a particular non-standard space group, you should transform it to the standard setting before fitting your diffraction data for GALLOP."
  },
  {
    "objectID": "posts/2021-11-02-solving-structures-with-gallop-browser-interface.html#upload-files",
    "href": "posts/2021-11-02-solving-structures-with-gallop-browser-interface.html#upload-files",
    "title": "Solving structures with the GALLOP browser interface",
    "section": "1. Upload files",
    "text": "1. Upload files\nThe radio button in the above screen shot already has “Upload files” selected, which then has an additional context menu to choose the Pawley refinement program you have used.\nSelect the program you used to fit the diffraction data, and then either drag and drop your DASH or GSAS-II fit files and Z-matrices (ZMs) onto the uploader widget, or select “Browse files” and navigate to the folder containing your fit files and ZMs and select them all for upload. If you wish to use MDB to bias the initial torsion angles used in GALLOP, you should also upload the .dbf file produce earlier. Note that this will only be used to set the MDB torsion angle biasing, and none of the other settings included in the MDB will be used by GALLOP.\nYou should end up with something that looks like this:"
  },
  {
    "objectID": "posts/2021-11-02-solving-structures-with-gallop-browser-interface.html#modify-gallop-parameters",
    "href": "posts/2021-11-02-solving-structures-with-gallop-browser-interface.html#modify-gallop-parameters",
    "title": "Solving structures with the GALLOP browser interface",
    "section": "2. Modify GALLOP parameters",
    "text": "2. Modify GALLOP parameters\nWith only 23 degrees of freedom, verapamil hydrochloride is a relatively simple crystal structure for GALLOP and the default settings should be sufficient to solve it. However, we will make a small change to increase our chance of success.\nClick on the Particle Swarm menu in the side bar to expand it. We will then increase the number of swarms from 10 to 20 either by using the + symbol to increment the number, or by deleting the 10 and typing in 20:\n\nWe should also decrease the number of iterations GALLOP will do - 10 should be sufficient. Open the General menu in the side bar, and change the Total number of iterations per run to 10.\n\nOnce we’ve done this, then we should be ready to solve the structure."
  },
  {
    "objectID": "posts/2021-11-02-solving-structures-with-gallop-browser-interface.html#solve-the-structure",
    "href": "posts/2021-11-02-solving-structures-with-gallop-browser-interface.html#solve-the-structure",
    "title": "Solving structures with the GALLOP browser interface",
    "section": "3. Solve the structure",
    "text": "3. Solve the structure\nOnce you are happy that all the files needed have been uploaded, and you are satisified with the settings for GALLOP to use, press the Solve button. Note that from this point forward, changing any of the settings whilst GALLOP is running will stop the run. You can still open the expandable menus to view settings or extra information provided by GALLOP.\nA number of expandable data menus will appear, and a progress bar will appear that tracks the progress of the learning rate finder discussed in my previous post. Once this has finished, the main GALLOP iterations will begin, with their own progress bar to track their progress. Once the first iteration has finished, some additional items will appear on screen. Two expandable boxes (discussed below) will appear, followed by a download link with the text “CIFs for run 1”. Lastly, a table of results for each iteration is displayed, as is an interactive figure showing the \\(\\chi^2\\) value found by each of the particles in each of the swarms. You can use your scroll wheel to zoom in, and click to drag to explore this plot without interrupting GALLOP.\n\nThe Show structure expandable item allows you to view an interactive plot of the structure found during the last iteration. Click and drag to rotate, and use your scroll wheel to zoom in and out. This figure will automatically update after each iteration, and plots the best structure found during the last iteration - note that this is not necessarily the best structure found so far.\n\nIf you are using DASH for Pawley fitting, the Show profile expandable item will also be visible. This allows you to see the fit to the diffraction data obtained in the last iteration.\n\nFor the data fit files I have provided, a solution has \\(\\chi^{2}_{int}\\) &lt; 60. If you fitted your own data, this will differ."
  },
  {
    "objectID": "posts/2021-11-02-solving-structures-with-gallop-browser-interface.html#download-solutions-and-close-gallop",
    "href": "posts/2021-11-02-solving-structures-with-gallop-browser-interface.html#download-solutions-and-close-gallop",
    "title": "Solving structures with the GALLOP browser interface",
    "section": "4. Download solutions and close GALLOP",
    "text": "4. Download solutions and close GALLOP\nYou can download CIFs at any time using the link. If you wish to stop GALLOP at any point, you can press the Stop button that appears in the top right corner of the browser window when GALLOP is running.\nThe link will give you a zip archive containing a CIF of the best solution found after each iteration, and a .json file which gives details of the settings used for the GALLOP run.\nOnce you are finished, you can safely close the browser window. If running on your local machine, you can then close down the command line window you opened earlier. If running on a cloud notebook, you may wish to shut down the notebook (if using Colab or Kaggle) to conserve your useage quota, or if using a paid service, you may wish to shut down your VM in order to reduce costs."
  },
  {
    "objectID": "posts/2021-11-03-solving-structures-with-gallop-python-api.html",
    "href": "posts/2021-11-03-solving-structures-with-gallop-python-api.html",
    "title": "Solving structures with the GALLOP Python API - basic use",
    "section": "",
    "text": "In my previous post, I went over how to use the GALLOP browser interface to solve the crystal structure of verapamil hydrochloride.\nIn this post, I’ll go over the basic way to use GALLOP Python API to solve the crystal structure of verapamil hydrochloride. The complete set of fit files I’ll be using are available as a zip archive you can download here.\nIn a future post, I’ll look at more sophisticated ways of using the GALLOP Python API to customise the optimisaition procedure."
  },
  {
    "objectID": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#introduction",
    "href": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#introduction",
    "title": "Solving structures with the GALLOP Python API - basic use",
    "section": "",
    "text": "In my previous post, I went over how to use the GALLOP browser interface to solve the crystal structure of verapamil hydrochloride.\nIn this post, I’ll go over the basic way to use GALLOP Python API to solve the crystal structure of verapamil hydrochloride. The complete set of fit files I’ll be using are available as a zip archive you can download here.\nIn a future post, I’ll look at more sophisticated ways of using the GALLOP Python API to customise the optimisaition procedure."
  },
  {
    "objectID": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#install-gallop-and-import-libraries",
    "href": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#install-gallop-and-import-libraries",
    "title": "Solving structures with the GALLOP Python API - basic use",
    "section": "Install GALLOP and import libraries",
    "text": "Install GALLOP and import libraries\nLet’s import the libraries we’ll need for our initial solution of verapamil hydrochloride.\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom gallop.structure import Structure\nfrom gallop.optim import local\nfrom gallop.optim import Swarm"
  },
  {
    "objectID": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#create-a-structure-object-and-add-the-data-to-it",
    "href": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#create-a-structure-object-and-add-the-data-to-it",
    "title": "Solving structures with the GALLOP Python API - basic use",
    "section": "Create a Structure object and add the data to it",
    "text": "Create a Structure object and add the data to it\nOur next job is to create a GALLOP structure object. The Structure class is used to store all of the information needed for the local optimisation procedure. We can specify a name for the Structure here, which will be used for any files we write out later (e.g. CIFs. We can also set the parameter that tells GALLOP to ignore the positions of hydrogen atoms during local optimisation. This significantly increases both the speed and the total number of particles that can be simulataneously evaluated, so only set this to False if you really need to!\n\nmystructure = Structure(name=\"VerapamilHCl\", ignore_H_atoms=True)\n\nNext up, we should add some diffraction data to our Structure object.\nCurrently, GALLOP accepts data that has been fitted by DASH, GSAS-II and TOPAS. In the future, I’m planning to add the ability to include SHELX-style data which may be of interest for those working with high-pressure single crystal diffraction data.\nWe need to tell the Structure object what program was used to fit the diffraction data so it knows what to look for: - If using diffraction data fitted by DASH, then we supply the filename for the .sdi and indicate that the source of the data is DASH - If using diffraction data fitted by GSAS-II, then we supply the filename for the .gpx and indicate that the source of the data is GSAS-II - If using diffraction data fitted by TOPAS, then we supply the filename for the .out and indicate that the source of the data is TOPAS\nWe can check that the data have been read in correctly by printing out the unit cell parameters and the first few peak intensities.\n\nmystructure.add_data(\"files/Verap.sdi\", source=\"DASH\")\n\nprint(\"Unit cell:\", mystructure.unit_cell)\nprint(\"Intensities 1-5:\",mystructure.intensities[:5])\n\nUnit cell: [  7.08991  10.59464  19.20684 100.1068   93.7396  101.561  ]\nIntensities 1-5: [ 85.705 235.032   0.614  -6.39  225.05 ]\n\n\nNext we need to add the Z-matrices to the structure object. The Z-matrices are expected to be in the format used by DASH. For more information on this format, see here.\nThis will automatically print out some information about the Z-matrices by default, though you can supply the argument verbose=False if you’d like to suppress that.\n\nmystructure.add_zmatrix(\"files/CURHOM_1.zmatrix\")\nmystructure.add_zmatrix(\"files/CURHOM_2.zmatrix\")\n\nAdded Z-matrix with Filename: files/CURHOM_1.zmatrix\nNon-H atoms: 1\nrefinable torsions: 0\nDegrees of freedom: 3\nAdded Z-matrix with Filename: files/CURHOM_2.zmatrix\nNon-H atoms: 33\nrefinable torsions: 13\nDegrees of freedom: 20 (7 + 13)\n\n\nYou may have noticed that the verapamil molecule is listed as having 20 degrees of freedom, with 7 external degrees of freedom and 13 torsions. The reason for the 7 in this case is because GALLOP makes use of quaternions to represent the molecular orientation. This gives an additional (redundant) degree of freedom relative to using Euler angles, and hence there are three parameters for the molecular position and four parameters for its orientation."
  },
  {
    "objectID": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#initialise-a-particle-swarm",
    "href": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#initialise-a-particle-swarm",
    "title": "Solving structures with the GALLOP Python API - basic use",
    "section": "Initialise a Particle Swarm",
    "text": "Initialise a Particle Swarm\nNext thing we’ll need is a Particle Swarm optimiser. To do this, we initialise a Swarm object, and then use it to generate the initial external and internal degrees of freedom for our structure.\nWe need to specify the total number of particles, and how many swarms these should be divided into. Verapamil hydrochloride is relatively simple for GALLOP, so let’s go for 10,000 particles split into 10 swarms (i.e. 1000 particles per swarm).\n\nswarm = Swarm(mystructure, n_particles=10000, n_swarms=10)\n\nNow let’s use the swarm to generate the initial external and internal degrees of freedom. By default, this will use Latin hypercube sampling rather than uniform sampling as it gives a more even coverage of the hypersurface. If you want to use uniform sampling you can supply the argument method=\"uniform\" to the function below.\nWe can also include Mogul Distribution Bias information to this function if available, which will bias the initial torsion angles to match the distribution obtained in the CSD. This is accomplished by using DASH to create a DASH batch file (.dbf) which we supply as an additional argument, MDB=\"filename.dbf\".\n\nexternal, internal = swarm.get_initial_positions(method=\"latin\", MDB=None)\n\n100%|██████████| 10/10 [00:00&lt;00:00, 1534.63it/s]\n\n\nThe degrees of freedom are organised as follows: - External: 1. Position (x,y,z) for ZM 1 - N 2. Quaternions (q1,q2,q3,q4) for ZM 1 - N - Internal: 1. Torsion (t1,…,tn) for ZM 1 - N\nSo for verapamil hydrochloride, we have the following structure to the external DoFs: \\[[x_{Cl},y_{Cl},z_{Cl},x_{V},y_{V},z_{V},q1_{V},q2_{V},q3_{V},q4_{V}]\\]\nOnly the verapamil Z-matrix has any torsion angles, so all DoFs in the internal array correspond to verapamil torsons.\nLet’s plot a couple of these DoFs to ensure we have the expected even distribution. Positions are generated in the range [0,1]. Quaternions are generated in the range [-1,1] and torsions are generated in the range [\\(-\\pi\\), \\(\\pi\\)]. This is particularly useful if we are using MDB to ensure the resultant distribution matches that expected.\n\n\nCode\nfig, ax = plt.subplots(1,3, figsize=(12,4))\n\nax[0].hist(external[:,0], rwidth=0.7)\nax[0].set_title(\"Chloride $x$\")\nax[0].set_xlabel(\"Fractional coordinate\")\n\nax[1].hist(external[:,7], rwidth=0.7)\nax[1].set_title(\"Verapamil $q_2$\")\nax[1].set_xlabel(\"Quaternion\")\n\nax[2].hist(np.rad2deg(internal[:,0]), rwidth=0.7)\nax[2].set_title(\"Verapamil $\\\\tau_1$\")\nax[2].set_xlabel(\"Torsion angle\")\nplt.show()\n\n\n\n\n\nThese are reassuringly boring plots!"
  },
  {
    "objectID": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#set-up-the-run-parameters-and-find-the-learning-rate",
    "href": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#set-up-the-run-parameters-and-find-the-learning-rate",
    "title": "Solving structures with the GALLOP Python API - basic use",
    "section": "Set up the run parameters and find the learning rate",
    "text": "Set up the run parameters and find the learning rate\nThe next thing we’ll need to do is set up the parameters we want to use for the runs (i.e. number of iterations etc) and also (optionally) use the learning rate finder to come up with a reasonable first attempt learning rate for this structure.\nFirst thing we’ll do is automatically generate a settings dictionary, and then modify those settings if desired. We’ll print out the keys for the dictionary and see if anything needs changing.\n\nminimiser_settings = local.get_minimiser_settings(mystructure)\n\nprint(minimiser_settings.keys())\n\ndict_keys(['n_reflections', 'include_dw_factors', 'chi2_solved', 'n_iterations', 'n_cooldown', 'learning_rate', 'learning_rate_schedule', 'verbose', 'use_progress_bar', 'print_every', 'check_min', 'dtype', 'device', 'optimizer', 'loss', 'eps', 'save_CIF', 'streamlit', 'torsion_shadowing', 'Z_prime', 'use_restraints', 'include_PO', 'PO_axis'])\n\n\nMost of these should be fine left at their default values. In some cases, you may wish to try solving with fewer reflections than are available in your dataset (perhaps in order to reduce GPU memory use). In such a scenario, you can set the number of reflections to use by modifying the ‘n_reflections’ dictionary value. You can find out about what the other parameters do in the docstring for the gallop.optim.local.minimise() function.\nHere, we’ll stick with the default values, which will use all reflections available in the data, the Adam optimiser, will run for 500 local optimisation iterations and will automatically save a CIF of the best structure found after each iteration.\nOur next task will be to find a reasonable learning rate using the learning rate finder. Here we set multiplication_factor=None so it is calculated for us (as discussed below).\n\nlearning_rate = local.find_learning_rate(mystructure, external=external,\n                internal=internal, minimiser_settings=minimiser_settings,\n                multiplication_factor=None)\n\nplt.figure(figsize=(8,6))\nplt.plot(learning_rate[0], learning_rate[1])\nplt.xlabel(\"Learning rate\")\nplt.ylabel(\"$\\\\sum{\\\\chi^2}$\")\nplt.show()\n\nGALLOP iter 0000 LO iter 0200 min chi2 641.1: 100%|██████████| 200/200 [00:11&lt;00:00, 16.99it/s]\n\n\n\n\n\nAs discussed in my introduction to GALLOP post, we will derive the learning rate from the minimum point on this curve. The learning_rate result obtained above is a list, which contains the following entries: 1. Trial learning rate values (x-axis) 2. Losses (y-axis) 3. The multiplication factor which scales the best learning rate found 4. The scaled learning rate - we can use this directly, by setting:\nminimiser_settings[\"learning_rate\"] = learning_rate[3]\nHowever, let’s do the scaling process ourselves to see what it looks like.\n\n\nCode\nlrs = learning_rate[0].copy()\nlosses = learning_rate[1].copy()\nmultiplication_factor = learning_rate[2]\nlearning_rate_to_use = learning_rate[3]\n\nlrs -= lrs.min()\nlrs /= lrs.max()\n\nlosses -= losses.min()\nlosses /= losses.max()\n\nminpoint = np.argmin(losses)\n\nplt.plot(lrs[minpoint:]-lrs[minpoint:].min(),\n    lrs[minpoint:]-lrs[minpoint:].min(),\":\",alpha=0.5,c=\"k\")\nplt.plot(lrs[minpoint:]-lrs[minpoint:].min(),\n    0.5*(lrs[minpoint:]-lrs[minpoint:].min()),\"-.\",\n    alpha=0.5,c=\"k\")\nplt.plot(lrs[minpoint:]-lrs[minpoint:].min(),\n    0.25*(lrs[minpoint:]-lrs[minpoint:].min()),\"--\",\n    alpha=0.5,c=\"k\")\nplt.plot(lrs[minpoint:]-lrs[minpoint:].min(),\n                            losses[minpoint:])\ngradient = ((losses[-1] - losses[minpoint])\n            / (lrs[-1] - lrs[minpoint]))\nplt.plot(lrs[minpoint:]-lrs[minpoint:].min(),\n            gradient*(lrs[minpoint:]-lrs[minpoint:].min()),\n            c=\"r\")\nplt.xlabel('normalised learning rate')\nplt.ylabel('rescaled sum')\nplt.legend([\"y=x\",\"y=0.5x\",\"y=0.25x\",\"rescaled sum\", \"approx\"],\n                loc=2, prop={'size': 8})\n\nplt.show()\n\n\n\n\n\nAs can be seen, the gradient of the red line approximating the blue curve is fairly shallow - less than 0.25. As a result, this tells us that this particular structure is relatively insensitive to the learning rate, so we can use a relatively large learning rate and still expect good performance.\nTherefore, we use a multiplication factor of 1.0, meaning that our learning rate will be \\(1.0 \\times \\alpha_{min}\\) where \\(\\alpha_{min}\\) is the learning rate corresponding to the minimum point on the curve in the previous plot.\n\nbest_learning_rate = learning_rate[0][minpoint]\n\nminimiser_settings[\"learning_rate\"] = best_learning_rate"
  },
  {
    "objectID": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#running-gallop",
    "href": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#running-gallop",
    "title": "Solving structures with the GALLOP Python API - basic use",
    "section": "Running GALLOP",
    "text": "Running GALLOP\nWe’ve now got everything we need sorted, all we need to do is write a very simple loop that will perform the GALLOP iterations.\nThe local.minimise() function returns a dictionary with keys external, internal, chi_2 and potentially others depending on arguments supplied. These results are read in by the Swarm object and used to generate a new set of external and internal degrees of freedom.\nLet’s have a go at running GALLOP for 10 iterations.\n\n# We'll get the time before the runs start so we have an indication of how long things have taken\nstart_time = time.time()\n\n# Now we have the GALLOP loop\nfor i in range(10):\n    # First do the local optimisation - notice the **minimiser_settings argument\n    # which takes in the dictionary we created earlier\n    result = local.minimise(mystructure, external=external, internal=internal,\n                run=i, start_time=start_time, **minimiser_settings)\n\n    # Particle swarm update generates new positions to be optimised\n    external, internal = swarm.update_position(result=result)\n\nGALLOP iter 0001 LO iter 0500 min chi2 433.5: 100%|██████████| 500/500 [00:28&lt;00:00, 17.52it/s]\nGALLOP iter 0002 LO iter 0500 min chi2 99.0: 100%|██████████| 500/500 [00:27&lt;00:00, 18.52it/s] \nGALLOP iter 0003 LO iter 0500 min chi2 56.3: 100%|██████████| 500/500 [00:28&lt;00:00, 17.70it/s] \nGALLOP iter 0004 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:28&lt;00:00, 17.72it/s] \nGALLOP iter 0005 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:28&lt;00:00, 17.78it/s] \nGALLOP iter 0006 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:28&lt;00:00, 17.76it/s] \nGALLOP iter 0007 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:28&lt;00:00, 17.80it/s]\nGALLOP iter 0008 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:28&lt;00:00, 17.67it/s] \nGALLOP iter 0009 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:28&lt;00:00, 17.69it/s]\nGALLOP iter 0010 LO iter 0500 min chi2 56.2: 100%|██████████| 500/500 [00:28&lt;00:00, 17.74it/s]"
  },
  {
    "objectID": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#analysing-the-results",
    "href": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#analysing-the-results",
    "title": "Solving structures with the GALLOP Python API - basic use",
    "section": "Analysing the results",
    "text": "Analysing the results\nLet’s re-plot the same histograms we made earlier and see how much things have changed. We’ll plot the output directly from the local optimiser as well as the suggested next positions given by the particle swarm that would be used if we were running an additional GALLOP iteration. We’ll also print out how many swarms reached a solution.\n\n\nCode\nswarm_best_chi2 = np.array(swarm.best_subswarm_chi2)\nprint(\"Number of swarms that solved the structure:\", (swarm_best_chi2 &lt; 60).sum())\n\nfig, ax = plt.subplots(1,3, figsize=(12,4))\n\nax[0].hist(result[\"external\"][:,0], rwidth=0.7)\nax[0].hist(external[:,0], rwidth=0.7)\nax[0].set_title(\"Chloride $x$\")\nax[0].set_xlabel(\"Fractional coordinate\")\n\nax[1].hist(result[\"external\"][:,7], rwidth=0.7)\nax[1].hist(external[:,7], rwidth=0.7)\nax[1].set_title(\"Verapamil $q_2$\")\nax[1].set_xlabel(\"Quaternion\")\n\nax[2].hist(np.rad2deg(result[\"internal\"][:,0]), rwidth=0.7)\nax[2].hist(np.rad2deg(internal[:,0]), rwidth=0.7)\nax[2].set_title(\"Verapamil $\\\\tau_1$\")\nax[2].set_xlabel(\"Torsion angle\")\n\nplt.legend([\"LO\",\"PSO\"], loc=\"upper right\", bbox_to_anchor=(1.4,1))\nplt.show()\n\n\nNumber of swarms that solved the structure: 2\n\n\n\n\n\nUnsurprisingly, these distributions have changed thanks to the optimisation. Also note that some of the blue bars sit outside the range of the orange bars - this is because the local optimiser is unbounded whereas the PSO is set to produce starting points within specified ranges.\nWe’ll look at the distributions in more detail in a minute, however, let’s first take a look at the structure. We can read over the CIFs of the best structure found after each iteration, and then pick one of these to visualise.\n\nimport glob\n\ncifs = glob.glob(\"VerapamilHCL_*.cif\")\nfor i, fn in enumerate(cifs):\n    print(i+1, fn)\n\n1 VerapamilHCl_0001_433.496_chisqd_252_refs_0.5_mins.cif\n2 VerapamilHCl_0002_98.965_chisqd_252_refs_0.9_mins.cif\n3 VerapamilHCl_0003_56.265_chisqd_252_refs_1.4_mins.cif\n4 VerapamilHCl_0004_56.249_chisqd_252_refs_1.9_mins.cif\n5 VerapamilHCl_0005_56.246_chisqd_252_refs_2.4_mins.cif\n6 VerapamilHCl_0006_56.244_chisqd_252_refs_2.8_mins.cif\n7 VerapamilHCl_0007_56.243_chisqd_252_refs_3.3_mins.cif\n8 VerapamilHCl_0008_56.244_chisqd_252_refs_3.8_mins.cif\n9 VerapamilHCl_0009_56.244_chisqd_252_refs_4.2_mins.cif\n10 VerapamilHCl_0010_56.244_chisqd_252_refs_4.7_mins.cif\n\n\nLet’s visualise the first solution with \\(\\chi^2 &lt; 65\\), which was obtained after iteration 3.\n\n\nCode\nimport py3Dmol\nfrom IPython.display import HTML\nhide_H = True\n\nstructure_to_display = 3\n#print(cifs[structure_to_display-1])\nwith open(cifs[structure_to_display-1], \"r\") as cif:\n        lines = []\n        for line in cif:\n            if hide_H:\n                splitline = list(filter(\n                        None,line.strip().split(\" \")))\n                if splitline[0] != \"H\":\n                    lines.append(line)\n            else:\n                lines.append(line)\ncif.close()\ncif = \"\\n\".join(lines)\nview = py3Dmol.view()\nview.addModel(cif, \"cif\",\n    {\"doAssembly\" : True,\n    \"normalizeAssembly\":True,\n    'duplicateAssemblyAtoms':True})\nview.setStyle({'sphere':{\"scale\":0.15},\n                'stick':{\"radius\":0.25}})\nview.addUnitCell()\nview.zoomTo()\nview.render()\nHTML(view.startjs + \"\\n\" + view.endjs + \"\\n\")\n\n\n\n\n        You appear to be running in JupyterLab (or JavaScript failed to load for some other reason).  You need to install the 3dmol extension: \n        jupyter labextension install jupyterlab_3dmol\n        \n\n\n\nFinally, let’s try and get a feel for how the optimised positions of the particles are distributed. The interactive plot below allows you to explore the distribution of optimised positions for each combination of the degrees of freedom. As you can see, the optimised particles tend to cluster around specific combinations of values - this isn’t too surprising. Once a swarm has located the global minimum, all of the other particles in the swarm will begin to move in that direction causing large numbers of particles to have very similar degrees of freedom after a few additional iterations.\nNote that due to a quirk of the library I’m using to generate the plots and interactive widget, the first plot you see is the x-coordinate of the chloride ion plotted against itself. This effectively gives a diagonal line which is equivalent to a histogram of the chloride x-coordinate distribution.\n\n\nCode\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode\n\ndisplay(HTML('''&lt;script src=\"/static/components/requirejs/require.js\"&gt;&lt;/script&gt;'''))\ninit_notebook_mode(connected=False)\n\n# Combine all the DoFs into a single DataFrame.\nall_df = pd.DataFrame(np.hstack([result[\"external\"], result[\"internal\"]]))\n\n# Label the columns so we know what each one is.\nall_df.columns=[\"x_cl\",\"y_cl\",\"z_cl\",\n                \"x_v\",\"y_v\",\"z_v\",\n                \"q1\",\"q2\",\"q3\",\"q4\",\n                \"t1\",\"t2\",\"t3\",\"t4\",\"t5\",\"t6\",\"t7\",\n                \"t8\",\"t9\",\"t10\",\"t11\",\"t12\",\"t13\"]\n\npositions = [\"x_cl\",\"y_cl\",\"z_cl\",\"x_v\",\"y_v\",\"z_v\"]\nquaternions = [\"q1\",\"q2\",\"q3\",\"q4\"]\ntorsions = [\"t1\",\"t2\",\"t3\",\"t4\",\"t5\",\"t6\",\"t7\",\n            \"t8\",\"t9\",\"t10\",\"t11\",\"t12\",\"t13\"]\n\n# Normalise the data so everything sits in its expected range.\n# Positions into range 0-1, quaternions set to be unit quaternions and torsions\n# into range -180 to 180.\nall_df[positions] = all_df[positions] % 1\nall_df[quaternions] /= np.sqrt(\n                    (all_df[quaternions]**2).sum(axis=1).values.reshape(-1,1))\n\nall_df[torsions] = np.rad2deg(np.arctan2(np.sin(all_df[torsions]),\n                            np.cos(all_df[torsions])))\n\n# Now generate the figure\nfig = go.Figure()\n\n# We'll use a histogram2dContour plot\nfig.add_trace(go.Histogram2dContour(\n        x=all_df[\"x_cl\"],\n        y=all_df[\"x_cl\"],\n        colorscale = 'Viridis',\n        contours_showlabels = False,\n        nbinsx=10,\n        nbinsy=10,\n        ncontours=20,\n    ))\n\n# Add the drop-down menus for selecting the data to plot\nbutton_list_x = []\nbutton_list_y = []\nfor dof in all_df.columns:\n    button_list_x.append(dict(\n                    args=[\"x\", [all_df[dof].values]],\n                    label=dof,\n                    method=\"restyle\"\n                ))\n    button_list_y.append(dict(\n                    args=[\"y\", [all_df[dof].values]],\n                    label=dof,\n                    method=\"restyle\"\n                ))\n\nfig.update_layout(\n    updatemenus=[\n        dict(\n            buttons=button_list_x,\n            direction=\"up\",\n            pad={\"r\": 10, \"t\": 10},\n            showactive=True,\n            x=0.45,\n            xanchor=\"left\",\n            y=-.05,\n            yanchor=\"top\"\n        ),\n        dict(\n            buttons=button_list_y,\n            direction=\"down\",\n            pad={\"r\": 10, \"t\": 10},\n            showactive=True,\n            x=-0.18,\n            xanchor=\"left\",\n            y=.95,\n            yanchor=\"top\"\n        ),\n    ]\n)\n\n# Add the annotations to label the drop-down menus\nfig.update_layout(\n    annotations=[\n        dict(text=\"x axis\", x=0.52, xref=\"paper\", y=-.07, yref=\"paper\",\n                            align=\"left\", showarrow=False),\n        dict(text=\"y axis\", x=-.15, xref=\"paper\", y=.98,\n                            yref=\"paper\", showarrow=False),\n    ])\n\nfig.update_layout(\n    width=700,\n    height=700,\n    autosize=False,\n    margin=dict(t=100, b=0, l=0, r=0),\n)\n\nfig.show()\n\n\n\n\n\n\n                                                \n\n\nLet’s compare one of these, say for example the x coordinate for the verapamil molecule and the y coordinate of the chloride ion, and see what the distribution looks like if we include or exclude particles with low values of \\(\\chi^2\\) from consideration.\n\n\nCode\nimport seaborn as sns\n\nxaxis = \"x_v\"\nyaxis = \"y_cl\"\nlimit = 400\nn_low = (result[\"chi_2\"]&lt;=limit).sum()\nn_high = (result[\"chi_2\"]&gt;limit).sum()\n\nfig, ax = plt.subplots(1,3,figsize=(18,6))\n\nsns.kdeplot(ax=ax[0], x=all_df[xaxis], y=all_df[yaxis],)\nax[0].set_title(\"All particles\")\n\nsns.kdeplot(ax=ax[1], x=all_df[xaxis][result[\"chi_2\"]&lt;=limit], y=all_df[yaxis][result[\"chi_2\"]&lt;=limit],)\nax[1].set_title(f\"{n_low} particles with $\\\\chi^2 \\\\leq {limit}$\")\n\nsns.kdeplot(ax=ax[2], x=all_df[xaxis][result[\"chi_2\"]&gt;limit], y=all_df[yaxis][result[\"chi_2\"]&gt;limit],)\nax[2].set_title(f\"{n_high} particles with $\\\\chi^2 &gt; {limit}$\")\n\nplt.show()\n\n\n\n\n\nAs we can see, the particles with higher \\(\\chi^2\\) values are not as tightly clustered as those with low \\(\\chi^2\\) values, and are therefore it’s less likely that their swarms are stuck in deep minima. Reassuringly, there seem to be peaks in the densities at approximately the same coordinates as we see in the low \\(\\chi^2\\) distribution, which suggests that if we were to leave GALLOP running for longer, we’d be in with a good chance of obtaining more solutions."
  },
  {
    "objectID": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#conclusions",
    "href": "posts/2021-11-03-solving-structures-with-gallop-python-api.html#conclusions",
    "title": "Solving structures with the GALLOP Python API - basic use",
    "section": "Conclusions",
    "text": "Conclusions\nIn this post, we’ve been over how to use the GALLOP Python API to solve the crystal structure of verapamil hydrochloride, and done some preliminary exploration of the results.\nIn future posts, we’ll look at more advanced methods of using the Python API and spend a bit more time diving into the results."
  },
  {
    "objectID": "posts/2021-11-07-slices-of-hypersurfaces.html",
    "href": "posts/2021-11-07-slices-of-hypersurfaces.html",
    "title": "Visualising hypersurfaces and optimisation trajectories",
    "section": "",
    "text": "In this post, we’ll be using the GALLOP Python API to to enable us to visualise \\(\\chi^2\\) hypersurfaces. These hypersurfaces are functions of the molecular positions, orientations and conformations within the unit cell. As such, their dimensionality is typically high, meaning that we cant visualise them directly. However, we can visualise low-dimensional slices through them. We’ve seen an example of one of these slices an earlier post.\nWe’ll write a simple function that allows us to specify two or three vector directions, then sample \\(\\chi^2\\) at points on a grid between these directions. We’ll also have a go at visualising the trajectories taken by particles initialised at each point on the grid as they are optimised with Adam, the default local optimiser in GALLOP.\nFor ease, we’ll continue using verapamil hydrochloride as our structure of interest. You can download the DASH fit files and Z-matrices I’ll be using here.\n\n\n\n\nUsually when we are trying to solve the crystal structure of an unknown small molecule, we are most concerned with finding the global minimum of the \\(\\chi^2\\) hypersurface. Therefore, we’ll start off investigating the region around the global minimum. Due to the repeating nature of a crystallographic lattice, and symmetries within unit cells, there are infinitely many points that correspond to the global minimum of the hypersurface - therefore, the coordinates below may not be the same as coordinates you obtain in your own solutions of verapamil hydrochloride.\nWhen GALLOP is used to solve a crystal structure, the degrees of freedom corresponding to the solution are automatically added to the CIF as comments. For example, here is the header of a CIF for verapamil hydrochloride:\n# Generated using pymatgen and GALLOP\n# GALLOP External Coords = [0.68394345,0.45950916,-0.114383094,0.2456631,-0.29722878,-0.10422839,0.3358067,0.7402369,0.7596887,-0.16798618\n# GALLOP Internal Coords = -1.2276717,2.9165819,-0.96692395,-1.2034712,3.2261908,2.1036072,-3.2097023,-3.0328763,1.5661795,-0.0071008434,-0.1824131,-0.05715108,0.27950087\n# Profile chisqd = 16.741\ndata_Verap_0006_56.244_chisqd_252_refs_3.6_mins.cif\nSo as long as we’ve been able to solve the crystal structure of interest with GALLOP, we’ll have easy access to the coordinates of a global minimum point.\nWe can also plot around a random point on the hypersurface to give a comparison.\n\n\n\nIn this post, we’ll look at two options for plotting. The first and most obvious will be to plot along some of the degrees of freedom in the structure to see how they interact.\nThe other option will be to choose random unit vectors and slice the surface along them to get a feel for how all of the degrees of freedom interact. In high dimensional spaces, randomly generated pairs of vectors will be approximately orthogonal so it should be possible to get reasonable levels of independence between the axes in our plots."
  },
  {
    "objectID": "posts/2021-11-07-slices-of-hypersurfaces.html#choice-of-central-point-and-slice-directions",
    "href": "posts/2021-11-07-slices-of-hypersurfaces.html#choice-of-central-point-and-slice-directions",
    "title": "Visualising hypersurfaces and optimisation trajectories",
    "section": "",
    "text": "Usually when we are trying to solve the crystal structure of an unknown small molecule, we are most concerned with finding the global minimum of the \\(\\chi^2\\) hypersurface. Therefore, we’ll start off investigating the region around the global minimum. Due to the repeating nature of a crystallographic lattice, and symmetries within unit cells, there are infinitely many points that correspond to the global minimum of the hypersurface - therefore, the coordinates below may not be the same as coordinates you obtain in your own solutions of verapamil hydrochloride.\nWhen GALLOP is used to solve a crystal structure, the degrees of freedom corresponding to the solution are automatically added to the CIF as comments. For example, here is the header of a CIF for verapamil hydrochloride:\n# Generated using pymatgen and GALLOP\n# GALLOP External Coords = [0.68394345,0.45950916,-0.114383094,0.2456631,-0.29722878,-0.10422839,0.3358067,0.7402369,0.7596887,-0.16798618\n# GALLOP Internal Coords = -1.2276717,2.9165819,-0.96692395,-1.2034712,3.2261908,2.1036072,-3.2097023,-3.0328763,1.5661795,-0.0071008434,-0.1824131,-0.05715108,0.27950087\n# Profile chisqd = 16.741\ndata_Verap_0006_56.244_chisqd_252_refs_3.6_mins.cif\nSo as long as we’ve been able to solve the crystal structure of interest with GALLOP, we’ll have easy access to the coordinates of a global minimum point.\nWe can also plot around a random point on the hypersurface to give a comparison.\n\n\n\nIn this post, we’ll look at two options for plotting. The first and most obvious will be to plot along some of the degrees of freedom in the structure to see how they interact.\nThe other option will be to choose random unit vectors and slice the surface along them to get a feel for how all of the degrees of freedom interact. In high dimensional spaces, randomly generated pairs of vectors will be approximately orthogonal so it should be possible to get reasonable levels of independence between the axes in our plots."
  },
  {
    "objectID": "posts/2021-11-07-slices-of-hypersurfaces.html#d-slices",
    "href": "posts/2021-11-07-slices-of-hypersurfaces.html#d-slices",
    "title": "Visualising hypersurfaces and optimisation trajectories",
    "section": "2D slices",
    "text": "2D slices\nNow that all of our functions are ready, let’s first have a go at plotting some 2D slices through the surface, in the region of the global minimum.\nIn an earlier post, we looked at the verapamil position along \\(a\\) and \\(c\\). In this post, let’s take a look at some of the quaternion components for the verapamil: q1 and q2.\n\n\nCode\nexternal, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(6,7), n_points=100)\nchisquared = get_chi_squared(mystructure, external, internal)\n\nfig = get_plot(chisquared, grid, gridpoints, dim1=\"Verapamil q1\", dim2=\"Verapamil q2\")\n\nfig.show()\n\n\n\n                                                \n\n\nNow let’s take a look at two of the torsion angles in verapamil, how about torsion 1 and 2, which correspond to the following torsions (using the CURHOM atom-labels): 1. N1 C9 C10 C11 2. C12 C11 C10 C9\n\n\nCode\nexternal, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(10,11), n_points=100)\n\nchisquared = get_chi_squared(mystructure, external, internal)\n\nfig = get_plot(chisquared, grid, gridpoints, dim1=\"N1 C9 C10 C11\", dim2=\"C12 C11 C10 C9\")\nfig.show()\n\n\n\n                                                \n\n\nIn the examples above, the surfaces that result are relatively smooth and have relatively few local minima.\nAs a comparison point, let’s regenerate the plots above, but this time instead of visualising around the global minimum, let’s use random coordinates instead.\n\n\nCode\nnp.random.seed(314159)\nrandom_pos = np.random.uniform(0,1,mystructure.total_position_degrees_of_freedom)\nrandom_rot = np.random.uniform(-1,1,mystructure.total_rotation_degrees_of_freedom)\nrandom_rot /= np.linalg.norm(random_rot)\nrandom_tors = np.random.uniform(-np.pi,np.pi,mystructure.total_internal_degrees_of_freedom)\n\nrandom_point = np.hstack([random_pos, random_rot, random_tors])\nprint(\"Random coordinates:\", random_point)\nfigs = []\nfor dims in [[6,7, \"Verapamil q1\", \"Verapamil q2\"],\n            [10,11, \"N1 C9 C10 C11\", \"C12 C11 C10 C9\"]]:\n    external, internal, grid, gridpoints = get_points(mystructure, np.copy(random_point), dims=dims[:2], n_points=100)\n\n    chisquared = get_chi_squared(mystructure, external, internal)\n\n    figs.append(get_plot(chisquared, grid, gridpoints, dim1=dims[2], dim2=dims[3]))\n\nfor fig in figs:\n    fig.show()\n\n\nRandom coordinates: [ 0.81792331  0.5510463   0.41977535  0.09869185  0.81102075  0.9673564\n -0.77218306  0.58012779  0.20160524  0.16291222 -0.04343231 -0.1460866\n -1.48303189 -1.19629038  2.56325063 -1.97929866 -1.95884954 -2.58621942\n  2.3512614   0.89477677  2.21686588  0.20772398  2.38385342]\n\n\n\n                                                \n\n\n\n                                                \n\n\nThings aren’t looking quite so smooth as they were before!\nThis may partly be due to the scaling effect of no longer having the (very deep) global minimum present. To test that, let’s replot our earlier torsion angle plot, but limit the minimum \\(\\chi^2\\) to the 5th percentile value. With this, we get about the same range of \\(\\chi^2\\) values as in the torsion angle plot around the random point. As we can see, the surface around the global minimum still looks more smooth, albeit with a few more shallow local minima now visible.\n\n\nCode\nexternal, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(10,11), n_points=100)\n\nchisquared = get_chi_squared(mystructure, external, internal)\nchisquared[chisquared &lt; np.percentile(chisquared, 5)] = np.percentile(chisquared, 5)\n\nfig = get_plot(chisquared, grid, gridpoints, dim1=\"N1 C9 C10 C11\", dim2=\"C12 C11 C10 C9\")\nfig.show()"
  },
  {
    "objectID": "posts/2021-11-07-slices-of-hypersurfaces.html#d-slices-1",
    "href": "posts/2021-11-07-slices-of-hypersurfaces.html#d-slices-1",
    "title": "Visualising hypersurfaces and optimisation trajectories",
    "section": "3D slices",
    "text": "3D slices\nNow let’s turn our attention to 3D slices through the surface. We’ll use a 3D volume plot from the plotly library to visualise three different dimensions at the same time. Due to the exponential increase in number of points we’ll have to evaluate, the grid resolution will be coming down a bit! To make things a bit easier to see, I’ve reversed the colourscale used, so now orange and yellow represent regions of low \\(\\chi^2\\).\nLet’s see the position of the chloride ion within the unit cell, with all other degrees of freedom fixed at the global minimum. For clarity, we’ll only visualise the isosurfaces below the fiftieth percentile of the \\(\\chi^2\\) values.\n\n\nCode\nexternal, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(3,4,5), n_points=25)\n\nchisquared = get_chi_squared(mystructure, external, internal)\n\nfig = get_plot(chisquared, grid, gridpoints, percentile=50)\nfig.show()\n\n\n\n                                                \n\n\nLet’s do the same thing for another selection of degrees of freedom, in this case, verapamil along \\(a\\), the third quaternion component and the 4th torsion angle, which corresponds to C13 C12 C11 C10.\n\n\nCode\nexternal, internal, grid, gridpoints = get_points(mystructure, np.copy(global_min), dims=(3,8,13), n_points=25)\n\nchisquared = get_chi_squared(mystructure, external, internal)\n\nfig = get_plot(chisquared, grid, gridpoints, percentile=50)\nfig.show()\n\n\n\n                                                \n\n\nIt’s a bit harder to tell what’s going on in these plots - we have to infer the gradient from the coloured isosurfaces. However, to my eye, they still look relatively smooth. Let’s take the same random point as before and plot the same 3D-slices and see if the slices look less smooth.\n\n\nCode\nfigs = []\nfor dims in [[3,4,5],\n            [3,8,13]]:\n    external, internal, grid, gridpoints = get_points(mystructure, np.copy(random_point), dims=dims[:3], n_points=25)\n\n    chisquared = get_chi_squared(mystructure, external, internal)\n\n    figs.append(get_plot(chisquared, grid, gridpoints, percentile=50))\n\nfor fig in figs:\n    fig.show()\n\n\n\n                                                \n\n\n\n                                                \n\n\nHard to tell! But to me they look more complex."
  },
  {
    "objectID": "posts/2021-11-07-slices-of-hypersurfaces.html#minimisation-function",
    "href": "posts/2021-11-07-slices-of-hypersurfaces.html#minimisation-function",
    "title": "Visualising hypersurfaces and optimisation trajectories",
    "section": "Minimisation function",
    "text": "Minimisation function\n\n\nCode\nimport tqdm\nimport torch\n\ndef minimise(structure, external, internal, dims, fix=True, n_iterations=100, lr=0.01):\n\n    trajectories = []\n\n    if fix:\n        tensors = tensor_prep.get_all_required_tensors(structure,\n                            external=external, internal=internal,\n                            requires_grad=False)\n        grid = torch.from_numpy(np.hstack([external, internal])[:,dims]\n                                ).type(torch.float32).cuda()\n        grid.requires_grad = True\n        alldof = torch.cat([tensors[\"zm\"][\"external\"],\n                            tensors[\"zm\"][\"internal\"]], dim=-1)\n        optimizer = torch.optim.Adam([grid], lr=lr, betas=[0.9,0.9])\n    else:\n        tensors = tensor_prep.get_all_required_tensors(structure,\n                            external=external, internal=internal,\n                            requires_grad=True)\n        optimizer = torch.optim.Adam([tensors[\"zm\"][\"external\"],\n                                    tensors[\"zm\"][\"internal\"]],\n                                    lr=lr, betas=[0.9,0.9])\n    local_iters = range(n_iterations)\n    for j in local_iters:\n        # Zero the gradients before each iteration otherwise they accumulate\n        optimizer.zero_grad()\n        if fix:\n            grid_dofs = torch.cat([alldof[:,:dims[0]],\n                                    grid[:,0].unsqueeze(1),\n                                    alldof[:,dims[0]+1:dims[1]],\n                                    grid[:,1].unsqueeze(1),\n                                    alldof[:,dims[1]+1:]\n                                    ], dim=-1)\n            tensors[\"zm\"][\"external\"] = grid_dofs[:,:structure.total_external_degrees_of_freedom]\n            tensors[\"zm\"][\"internal\"] = grid_dofs[:,structure.total_external_degrees_of_freedom:]\n        asymmetric_frac_coords = zm_to_cart.get_asymmetric_coords(**tensors[\"zm\"])\n        calculated_intensities = intensities.calculate_intensities(\n                                asymmetric_frac_coords, **tensors[\"int_tensors\"])\n        chisquared = chi2.calc_int_chisqd(calculated_intensities, **tensors[\"chisqd_tensors\"])\n\n        trajectories.append([tensors[\"zm\"][\"external\"].detach().cpu().numpy(),\n                            tensors[\"zm\"][\"internal\"].detach().cpu().numpy(),\n                            chisquared.detach().cpu().numpy()])\n        # For the last iteration, don't step the optimiser, otherwise the chi2\n        # value won't correspond to the DoFs\n        if j != n_iterations - 1:\n            L = torch.sum(chisquared)\n            L.backward()\n            optimizer.step()\n\n    return trajectories\n\n\nWe’ll also need a function to plot the resultant trajectories. It’ll be useful to compare the slice of the surface before optimisation to the starting points that reached a solution by the end.\n\n\nCode\nimport matplotlib.pyplot as plt\n\ndef plot_trajectories(mystructure, dims, global_minimum, lr=0.01, fix=True, n_iterations=100, n_points=50):\n\n    external, internal, grid, gridpoints = get_points(mystructure, np.copy(global_minimum),\n                                            dims=dims, n_points=n_points)\n\n    trajectories = minimise(mystructure, external, internal, dims, fix=fix, \n                                                n_iterations=n_iterations, lr=lr)\n\n    coords = []\n    chi2s = []\n    for t in trajectories:\n        coords.append(np.hstack(t[:2])[:,dims])\n        chi2s.append(t[2])\n    coords = np.dstack(coords)\n\n    fig, ax = plt.subplots(1,3,figsize=(44,12))\n    ax[0].set_title(\"$\\\\chi^2$ surface\")\n    ax[0].contour(gridpoints[0], gridpoints[0], chi2s[0].reshape(grid[0].shape),\n                    cmap=\"viridis\", levels=20)\n\n    chi2temp = chi2s[-1] - chi2s[-1].min()\n    col=plt.cm.viridis(chi2temp/chi2s[-1].max())\n    col\n\n    ax[1].set_title(\"Particle trajectories\")\n    ax[1].scatter(coords[:,0,0], coords[:,1,0], color=col[0], s=10, alpha=.125)\n\n    for i in range(coords.shape[0]):\n        ax[1].plot(coords[i,0,:], coords[i,1,:], color=col[i], alpha=0.125)\n\n    percent_solved = np.around(100*(chi2s[-1]&lt;60).sum()/chi2s[-1].shape[0], 1)\n    ax[2].set_title(f\"Final $\\\\chi^2$ - {percent_solved} % solved\")\n    ax[2].contourf(gridpoints[0], gridpoints[0], chi2s[-1].reshape(grid[0].shape),\n                    cmap=\"viridis\", levels=20)\n    plt.show()\n    return coords, chi2s, grid, gridpoints\n\n\nLet’s now visualise the trajectories taken if we slice along the verapamil position along \\(a\\) and \\(b\\), as well as torsion 1 and torsion 2. The upper plot in each case shows the trajectories if all of the degrees of freedom other than those being plotted are fixed. The lower plot shows the trajectories taken if everything is allowed to refine (as would be normal in GALLOP).\nThe plots might appear a little small on the blog - if you want larger views, right click on the image and open them in a new tab.\n\n\nCode\ndims=(3,4)\nprint(dims,\"others fixed\")\nfixed_trajectories_34 = plot_trajectories(mystructure, dims, np.copy(global_min), lr=0.01, fix=True, n_iterations=100, n_points=50)\n\nprint(dims, \"others free\")\nfree_trajectories_34 = plot_trajectories(mystructure, dims, np.copy(global_min), lr=0.01, fix=False, n_iterations=100, n_points=50)\n\ndims=(10,11)\nprint(dims,\"others fixed\")\nfixed_trajectories_1011 = plot_trajectories(mystructure, dims, np.copy(global_min), lr=0.01, fix=True, n_iterations=100, n_points=50)\n\nprint(dims, \"others free\")\nfree_trajectories_1011 = plot_trajectories(mystructure, dims, np.copy(global_min), lr=0.01, fix=False, n_iterations=100, n_points=50)\n\n\n(3, 4) others fixed\n\n\n\n\n\n(3, 4) others free\n\n\n\n\n\n(10, 11) others fixed\n\n\n\n\n\n(10, 11) others free\n\n\n\n\n\nAs we can see, as we allow the other degrees of freedom to refine, we end up with different behaviour. In the example of the fractional coordinates (dims = 3,4) can now see two points within the unit cell that constitute a global minimum after refinement - this is because in the second plot, the chloride ion is free to move to accommodate the different positions of the verapamil molecule.\nAnother point of interest is the effects of momentum in the Adam optimiser are clearly visible - the trajectories in some cases backtrack after moving in a particular direction for a while. This is because momentum has carried them “uphill”. This property can allow the local optimisation algorithm in GALLOP to escape shallow local minima, as well as pass quickly through flat regions of the hypersurface.\nLastly, let’s animate the trajectories!\n\n\nCode\nfrom matplotlib import animation\n\ndef generate_animation(coords, chi2s, grid, gridpoints, name=\"fig.gif\", type=\"frac\"):\n    # First set up the figure, the axis, and the plot element we want to animate\n    fig = plt.figure(figsize=(10,10))\n    ax = plt.axes(xlim=(gridpoints[0].min(), gridpoints[0].max()),\n                ylim=(gridpoints[1].min(), gridpoints[1].max()))\n\n    ax.contour(gridpoints[0], gridpoints[1], chi2s[0].reshape(grid[0].shape), cmap=\"viridis\", levels=20)\n\n    scatter = ax.scatter(coords.T[0, 0, :], coords.T[0, 1, :], c=chi2s[-1],\n                        s=5, alpha=0.25)\n\n\n    # animation function.  This is called sequentially\n    def animate(i, coords, chi2s):\n        #scatter.set_offsets(coords.T[i, :, :].T)\n        if type == \"frac\":\n            scatter.set_offsets(coords.T[i, :, :].T % 1)\n        elif type == \"torsion\":\n            scatter.set_offsets(np.arctan2(np.sin(coords.T[i, :, :].T), np.cos(coords.T[i, :, :].T)))\n        #scatter.set_array(chi2s[i])\n        scatter.set_array(chi2s[-1])\n        return scatter,\n\n\n    ani = animation.FuncAnimation(fig, animate, frames=range(coords.T.shape[0]), blit=True,\n                                    fargs=(coords, chi2s), interval=100,)\n\n    ani.save(name)\n    #plt.show()\n    return None\n\ngenerate_animation(*fixed_trajectories_34, name=\"images/animation_34.gif\", type=\"frac\")\ngenerate_animation(*fixed_trajectories_1011, name=\"images/animation_1011.gif\", type=\"torsion\")\n\n\nVerapamil along a and b \nVerapamil torsion 1 and 2 \nThings seem to move much more slowly in the torsion angle example - I suspect this is because the gradient on the “flat” region is low enough that it takes a while for the particles to pick up speed!"
  },
  {
    "objectID": "posts/2021-11-10-hofcalc.html",
    "href": "posts/2021-11-10-hofcalc.html",
    "title": "Estimating molecular volumes to aid in powder X-ray diffraction indexing",
    "section": "",
    "text": "An article published in 2001 by D. W. M. Hofmann describes how crystallographic databases can be used to derive the average volume occupied by atoms of each element in crystal structures. Using his tabulated values, it’s possible to rapidly estimate the volume occupied by a given molecule, and use this to aid indexing of powder diffraction data. This is particularly useful for laboratory diffraction data, which is generally associated with lower figures of merit such as de Wolff’s \\(M_{20}\\) and Smith and Snyder’s \\(F_N\\), which can make discriminating between alternative options more challenging. Other volume estimation methods, notably the 18 Å³ rule are also commonly used, though Hofmann’s volumes give generally more accurate results.\nI’ve put together a freely available web-app, HofCalc, which can be used to conveniently obtain these estimates. It should display reasonably well on mobile devices as well as PCs/laptops. You can access it at the following address:\nhttps://hofcalc.streamlit.app\n\nThis post will explain how it works, and will look at some examples of how it can be used in practice. I’m grateful to Norman Shankland who provided invaluable feedback and assistance with debugging of the app."
  },
  {
    "objectID": "posts/2021-11-10-hofcalc.html#formulae-and-names",
    "href": "posts/2021-11-10-hofcalc.html#formulae-and-names",
    "title": "Estimating molecular volumes to aid in powder X-ray diffraction indexing",
    "section": "Formulae and names",
    "text": "Formulae and names\n\nBasic use\nThe simplest option is to enter the chemical formula or name of the material of interest. Names are resolved by querying PubChem, so common abbreviations for solvents can often be used e.g. DMF. Note that formulae can be prefixed with a multiple, e.g. 2H2O\n\n\n\nSearch term\nType\n\\(V_{Hofmann}\\)\n\n\n\n\nethanol\nname\n69.61\n\n\nCH3CH2OH\nformula\n69.61\n\n\nwater\nname\n21.55\n\n\n2H2O\nformula\n43.10\n\n\n\n\n\nMultiple search terms\nIt is also possible to search for multiple items simultaneously, and mix and match name and formulae by separating individual components with a semicolon. This means that for example, ‘amodiaquine dihydrochloride dihydrate’ can also be entered as ‘amodiaquine; 2HCl; 2H2O’.\n\n\n\nSearch term\nTotal \\(V_{Hofmann}\\)\n\n\n\n\ncarbamazepine; L-glutamic acid\n497.98\n\n\nzopiclone; 2H2O\n496.02\n\n\nC15H12N2O; CH3CH2COO-; Na+\n419.79\n\n\nsodium salicylate; water\n204.21\n\n\namodiaquine dihydrochloride dihydrate\n566.61\n\n\namodiaquine; 2HCl; 2H2O\n566.61\n\n\n\n\n\nMore complex examples - hemihydrates\nIn cases where fractional multiples of search components are required, such as with hemihydrates, care should be taken to check the evaluated chemical formula for consistency with the expected formula.\n\n\n\n\n\n\n\n\n\n\nSearch term\nEvaluated as\n\\(V_{Hofmann}\\)\nDivide by\nExpected Volume\n\n\n\n\nCalcium sulfate hemihydrate\nCa2 H2 O9 S2\n253.07\n2\n126.53\n\n\ncalcium; calcium; sulfate; sulfate; water\nCa2 H2 O9 S2\n253.07\n2\n126.53\n\n\ncalcium; sulfate; 0.5H2O\nCa1 H1.0 O4.5 S1\n126.53\n-\n126.53\n\n\nCodeine phosphate hemihydrate\nC36 H50 N2 O15 P2\n1006.77\n2\n503.38\n\n\ncodeine; codeine; phosphoric acid; phosphoric acid; water\nC36 H50 N2 O15 P2\n1006.77\n2\n503.38\n\n\ncodeine; phosphoric acid; 0.5H2O\nC18 H25.0 N1 O7.5 P1\n503.38\n-\n503.38\n\n\n\n\n\nCharged species in formulae\nCharges could potentially interfere with the parsing of chemical formulae. For example, two ways of representing an oxide ion:\n\n\n\nSearch term\nEvaluated as\n\n\n\n\nO-2\n1 x O\n\n\nO2-\n2 x O\n\n\n\nWhilst is is recommended that charges be omitted from HofCalc queries, if including charges in your queries, ensure that the correct number of atoms has been determined in the displayed atom counts or the downloadable summary file. For more information on formatting formulae, see the pyvalem documentation."
  },
  {
    "objectID": "posts/2021-11-10-hofcalc.html#temperature",
    "href": "posts/2021-11-10-hofcalc.html#temperature",
    "title": "Estimating molecular volumes to aid in powder X-ray diffraction indexing",
    "section": "Temperature",
    "text": "Temperature\nThe temperature, \\(T\\) (in kelvin) is automatically included in the volume calculation via the following equation:\n\\[V = \\sum{n_{i}v_{i}}(1 +  \\alpha(T - 298))\\]\nWhere \\(n_{i}\\) and \\(v_{i}\\) are the number and Hofmann volume (at 298 K) of the \\(i\\)th element in the chemical formula, and \\(\\alpha = 0.95 \\times 10^{-4} K^{-1}\\)."
  },
  {
    "objectID": "posts/2021-11-10-hofcalc.html#unit-cell-volume",
    "href": "posts/2021-11-10-hofcalc.html#unit-cell-volume",
    "title": "Estimating molecular volumes to aid in powder X-ray diffraction indexing",
    "section": "Unit cell volume",
    "text": "Unit cell volume\nIf the volume of a unit cell is supplied, then the unit cell volume divided by the estimated molecular volume will also be shown.\n\n\n\n\n\n\n\n\n\nSearch term\n\\(V_{cell}\\)\n\\(V_{Hofmann}\\)\n\\(\\frac{V_{cell}}{V_{Hofmann}}\\)\n\n\n\n\nzopiclone, 2H2O\n1874.61\n496.02\n3.78\n\n\nverapamil, HCl\n1382.06\n667.57\n2.07"
  },
  {
    "objectID": "posts/2021-11-10-hofcalc.html#summary-files",
    "href": "posts/2021-11-10-hofcalc.html#summary-files",
    "title": "Estimating molecular volumes to aid in powder X-ray diffraction indexing",
    "section": "Summary Files",
    "text": "Summary Files\nEach time HofCalc is used, a downloadable summary file is produced. It is designed to serve both as a record of the query for future reference and also as a method to sense-check the interpretation of the entered terms, with links to the PubChem entries where relevant. An example of the contents of the summary file for the following search terms is given below.\nSearch term = carbamazepine; indomethacin\nT = 293 K\nUnit cell volume = 2921.6 Å³\n{\n    \"combined\": {\n        \"C\": 34,\n        \"H\": 28,\n        \"N\": 3,\n        \"O\": 5,\n        \"Cl\": 1\n    },\n    \"individual\": {\n        \"carbamazepine\": {\n            \"C\": 15,\n            \"H\": 12,\n            \"N\": 2,\n            \"O\": 1\n        },\n        \"indomethacin\": {\n            \"C\": 19,\n            \"H\": 16,\n            \"Cl\": 1,\n            \"N\": 1,\n            \"O\": 4\n        }\n    },\n    \"user_input\": [\n        \"carbamazepine\",\n        \"indomethacin\"\n    ],\n    \"PubChem CIDs\": {\n        \"carbamazepine\": 2554,\n        \"indomethacin\": 3715\n    },\n    \"PubChem URLs\": {\n        \"carbamazepine\": \"https://pubchem.ncbi.nlm.nih.gov/compound/2554\",\n        \"indomethacin\": \"https://pubchem.ncbi.nlm.nih.gov/compound/3715\"\n    },\n    \"individual_volumes\": {\n        \"carbamazepine\": 303.86,\n        \"indomethacin\": 427.77\n    },\n    \"V_Cell / V_Hofmann\": 3.99,\n    \"Temperature\": 293,\n    \"Hofmann Volume\": 731.62,\n    \"Hofmann Density\": 1.35\n}"
  },
  {
    "objectID": "posts/2021-11-18-google-cloud.html",
    "href": "posts/2021-11-18-google-cloud.html",
    "title": "Installing and running GALLOP on cloud GPUs",
    "section": "",
    "text": "GALLOP is designed to run on GPUs and TPUs. Users that don’t have local access to GPUs, or who want to gain access to faster GPUs without needing to purchase their own can make use of cloud computing resources. In this post we’ll look at how to set up a GPU-equipped virtual machine on Google’s Compute Engine cloud service, and then use it to run GALLOP jobs both through the graphical browser-based interface and via python mode.\nOne feature offered by GCE is preemptible instances, which significantly lowers the cost of using resources. There are some limitations that come with this, but they are usually not a concern for GALLOP jobs that take &lt; 24 hours to complete. It’s also worth noting that new customers to Google Cloud get $300 free credits to try out their services, so it’s well worth having a go. You will need a credit/debit card to gain access to GPUs even though you won’t be charged until the free credits run out. You will also need to request an increase to your GPU quota before they can be used - this only took a few minutes to be approved for me.\nLastly, I’m not sponsored or endorsed by Google at all, I just like this service. There are many other cloud GPU providers who may be worth a look to see if they suit your requirements. If you know of any good ones, please get in contact with me, especially if you want to help with providing instructions for other users."
  },
  {
    "objectID": "posts/2021-11-18-google-cloud.html#installing-the-cuda-driver",
    "href": "posts/2021-11-18-google-cloud.html#installing-the-cuda-driver",
    "title": "Installing and running GALLOP on cloud GPUs",
    "section": "Installing the CUDA driver",
    "text": "Installing the CUDA driver\nOnce SSH’d into the VM for the first time, I typed y, then pressed enter to install the CUDA driver. This unfortunately failed (I’m not sure why, but this happens frequently! Thankfully, we can fix it quite easily). If yours doesn’t fail, proceed onto the GALLOP installation described below, otherwise follow these steps to get the CUDA driver working.\nRun the following commands one by one:\nsudo rm /var/lib/apt/lists/lock && sudo rm /var/cache/apt/archives/lock && sudo rm /var/lib/dpkg/lock*\n\nsudo dpkg --configure -a\n\nsudo /opt/deeplearning/install-driver.sh\nThe last command in particular may take a couple of minutes to run. If any errors occur during these steps, then depending on the error, you may need to run a few more commands to resolve them.\nOne possible issue is:\ndpkg: error: parsing file '/var/lib/dpkg/updates/0003' near line 0:\n newline in field name '#padding'\nI solved this issue by running sudo rm /var/lib/dpkg/0003, though you may need to replace 0003 with whatever number you have on your error.\nAnother potential issue is the second command complains about the google cloud sdk:\nErrors were encountered while processing:\n google-cloud-sdk\nIf this occurs, run the following commands and hopefully it’ll work. The first command can take a few minutes to run.\nsudo apt-get upgrade google-cloud-sdk -y\nsudo dpkg --configure -a\nsudo /opt/deeplearning/install-driver.sh\nIf you get any other errors, I found several examples of other people experiencing the same thing on Google so I suggest searching for your error message. Feel free to contact me by email or on Twitter and I can try to help."
  },
  {
    "objectID": "posts/2021-11-18-google-cloud.html#installing-gallop-and-running-in-python-mode",
    "href": "posts/2021-11-18-google-cloud.html#installing-gallop-and-running-in-python-mode",
    "title": "Installing and running GALLOP on cloud GPUs",
    "section": "Installing GALLOP and running in Python mode",
    "text": "Installing GALLOP and running in Python mode\nOnce the CUDA driver is installed, we’re ready to install GALLOP! Run the following command to grab the code from github and install it to the VM.\ngit clone https://github.com/mspillman/gallop.git && cd gallop && pip install .\nOnce these commands are finished, GALLOP is now installed. If you’re running in python mode, then you can upload a script (and diffraction data & ZMs) using the “gear” menu in the top right. Just below you’ll also see an option for downloading files which will be of use to obtain your results.\n\nYou may also be interested in using SSH to access JupyterLab\nhttps://cloud.google.com/vertex-ai/docs/workbench/user-managed/ssh-access"
  },
  {
    "objectID": "posts/2022-01-08-profile-chi-squared-optimisation.html",
    "href": "posts/2022-01-08-profile-chi-squared-optimisation.html",
    "title": "Solving crystal structures with GALLOP and profile \\(\\chi^2\\)",
    "section": "",
    "text": "The correlated integrated intensity \\(\\chi^2\\) figure of merit has been shown to be equivalent to the profile \\(\\chi^2\\), but is more efficient to calculate in CPU-based code. This is due to the sparsity of the inverse covariance matrix (typically only 1-5% of the elements will be non-zero if using DASH’s 20% correlation cut-off), which means that only a small number of non-zero elements need to be multiplied and summed. Code that exploits this property can therefore obtain very high performance - this is the approach embodied in the fortran code used in DASH.\nThe intensity \\(\\chi^2\\) figure of merit in GALLOP does not exploit the sparsity of the inverse covariance matrix; instead the matrix is treated as dense and a full matrix multiplication is performed. This is because very fast matrix multiplication routines which make effective use of the parallel processing capabilities of GPUs are available in cuBLAS. Sparse matrices are currently less well supported, and though there has been a lot of progress, performance remains generally worse.\nThis got me thinking: given that we aren’t currently able to make best use of the sparsity of the inverse covariance matrix in GALLOP and instead treat it as a dense matrix, can the profile \\(\\chi^2\\) also be implemented in a manner that is amenable to GPU acceleration, and if so, how does the performance compare to the intensity \\(\\chi^2\\) figure of merit that is currently used?\nTo tackle this, we’ll take a look at the output files produced by DASH during its Pawley fitting procedure, and work out how the full profile can be (rapidly) reconstructed."
  },
  {
    "objectID": "posts/2022-01-08-profile-chi-squared-optimisation.html#pik-file",
    "href": "posts/2022-01-08-profile-chi-squared-optimisation.html#pik-file",
    "title": "Solving crystal structures with GALLOP and profile \\(\\chi^2\\)",
    "section": "PIK file",
    "text": "PIK file\nThis file contains all of the information we’ll need to calculate the profile \\(\\chi^2\\).\nIn general, the lines follow the following structure:\n    twotheta      intensity       ESD        Number of peaks contributing to this point\nIf the number of peaks contributing to a particular point is zero, then this is just background noise, and the next line will have the same format. Here’s an example of a section of background intensity from the PIK file for verapamil hydrochloride:\n   5.016000      -323.4935       202.1800               0\n   5.031000      -382.2603       201.2000               0\n   5.045000      -315.5720       201.4600               0\n   5.060000      -250.9787       201.7000               0\nHowever, if the number of peaks that contribute to a point is greater than 0, then the next line(s) contain information about the contributing peaks. If there are N peaks that contribute intensity to a particular point, then the next line(s) will have the following structure:\n    twotheta      intensity       ESD        Number of peaks contributing to this point\n       N x [peak number         intensity for peak number]\nwhere “peak number” is the position of the particular peak in a sorted list of all of the peak \\(2\\theta\\) positions (i.e. peak number 3 is the peak with the third lowest \\(2\\theta\\) value.) The peak number and intensity information may break over multiple lines, and continues until the intensity and peak number of all N peaks contributing to a particular point have been recorded.\nFor example, here’s section where there is 1 contributing peak (which happens to be the first peak in the data):\n   4.350000       744.3560       232.3900               1\n           1   4.513631\n   4.364000       639.3544       230.9700               1\n           1   5.134259\n   4.379000       1007.128       234.2900               1\n           1   5.837606\nHere’s a section with two contributing peaks (which are the second and third peaks in the data):\n   8.653000       5611.787       179.4200               2\n           2   22.49174               3  0.1523584\n   8.668000       6297.480       185.9700               2\n           2   26.03695               3  0.1624220\n   8.682000       5904.059       181.7700               2\n           2   24.64359               3  0.1726878\nAnd here’s a section with four contributing peaks (which are the 59th, 60th, 61st and 62nd peaks in the data):\n  25.09800       883.4489       79.31000               4\n          59  0.9365445              60  0.1982842              61\n  0.1636752              62  2.1087736E-02\n   25.11300       1260.722       81.62000               4\n          59   1.462635              60  0.3181552              61\n  0.2449987              62  2.4525421E-02\n   25.12700       1757.970       84.58000               4\n          59   2.065944              60  0.5192419              61\n  0.3785602              62  2.8390534E-02\nUsing the read_DASH_pik function in GALLOP, we can parse a .pik file, and examine the individual peaks and the full diffraction profile.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom gallop import files\n\nprofile, baseline_peaks, n_contributing_peaks = files.dash.read_DASH_pik(\"Verap.pik\")\n\nThe profile numpy array contains three columns: twotheta, intensity and ESD.\n\n\nCode\nprint(\"Twotheta    Intensity   ESD\")\nprint(profile)\nplt.figure(figsize=(12,8))\nplt.plot(profile[:,0], profile[:,1])\nplt.xlabel(\"$2\\\\theta$\")\nplt.ylabel(\"Intensity\")\nplt.show()\n\n\nTwotheta    Intensity   ESD\n[[   4.35     744.356    232.39   ]\n [   4.364    639.3544   230.97   ]\n [   4.379   1007.128    234.29   ]\n ...\n [  40.167    104.9547    45.65   ]\n [  40.181    -19.50696   44.27   ]\n [  40.196     42.59625   44.96   ]]\n\n\n\n\n\nThe baseline_peaks numpy array has shape (n-peaks, n-points) where n-points is the number of points in the profile and where the Nth row contains the intensity associated with contributing peak N. Let’s plot them all on the same axes:\n\n\nCode\nprint(\"Baseline peaks array shape:\",baseline_peaks.shape)\nprint(baseline_peaks)\nplt.figure(figsize=(12,8))\nfor i in range(baseline_peaks.shape[0]):\n    plt.plot(profile[:,0],baseline_peaks[i])\nplt.xlabel(\"$2\\\\theta$\")\nplt.ylabel(\"Intensity\")\nplt.show()\n\n\nBaseline peaks array shape: (252, 2475)\n[[4.513631   5.134259   5.837606   ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n ...\n [0.         0.         0.         ... 0.01736796 0.01483574 0.01280165]\n [0.         0.         0.         ... 0.01911421 0.0161166  0.01376026]\n [0.         0.         0.         ... 0.01852708 0.01551832 0.01318077]]\n\n\n\n\n\nThis doesn’t look like the observed data!\nHowever, this is intentional. The intensities in the baseline_peaks array have been scaled to correct for peak multiplicity, Lorentz polarisation factor and form-factor fall off and then divided by their Pawley extracted intensity.\nThis then allows the calculated profile to be rapidly reconstructed by multiplying each of the rows in the baseline_peaks array by the corresponding intensity (i.e. intensities calculated during SDPD or the Pawley extracted intensities), then summing each column in the array to account for intensity contribution from multiple peaks. The profile \\(\\chi^2\\) can then be calculated from the reconstructed profile.\nLet’s have a go at reconstructing the profile from the Pawley-refined intensities:\n\n\nCode\nfrom gallop.structure import Structure\n\nmystructure = Structure(name=\"verapamil_hydrochloride\")\nmystructure.add_data(\"verap.sdi\", source=\"DASH\")\n\n# Here we reconstruct the profile by multiplying by the Pawley intensities, then summing each column\ncalc_profile = (mystructure.intensities.reshape(-1,1) * baseline_peaks).sum(axis=0)\nplt.figure(figsize=(12,8))\nplt.plot(profile[:,0], profile[:,1])\nplt.plot(profile[:,0], calc_profile)\nplt.legend([\"Obs\", \"Calc\"])\nplt.xlabel(\"$2\\\\theta$\")\nplt.ylabel(\"Intensity\")\nplt.show()\n\n\n\n\n\nIf we use intensities calculated during SDPD with GALLOP then we will need to scale the calculated pattern in order to properly calculate the \\(\\chi^2\\) value. We calculate the scaling factor using the equation below:\n\\[ c = \\frac{\\sum{y_i^{obs}}}{\\sum{y_i^{calc}}} \\]\nThis will then allow us calculate the profile \\(\\chi^2\\) value, via:\n\\[ \\chi^2_{profile} = \\frac{\\sum{\\frac{(cy^{calc}_i - y^{obs}_i)^2}{(\\sigma_i^{obs})^2}}}{N - P + C} \\]\nwhere \\(N\\) = the total number of observations (i.e. points in the profile), \\(P\\) = the number of parameters refined and \\(C\\) is the number of constraints used in the refinement. For the \\(\\chi^2\\) calculation, we will by default consider only the points which have at least one Bragg peak contributing to the intensity of that point as recommended here (pdf).\n\n\nCode\n# Generate a mask that selects only the points containing contributions from at least one Bragg peak\nsubset = n_contributing_peaks &gt; 0\n\n# Calculate the calculated pattern scaling factor\nscale = profile[:,1][subset].sum() / calc_profile[subset].sum()\n\n# The \"-2\" in the denominator is because DASH refines two background terms during the Pawley refinement by default\nprof_chisquared = ((scale*calc_profile[subset] - profile[:,1][subset])**2 / profile[:,2][subset]**2).sum() / (calc_profile[subset].shape[0] - 2)\nprint(\"Profile chi-squared =\",prof_chisquared)\n\n\nProfile chi-squared = 3.5833747277018166"
  },
  {
    "objectID": "posts/2022-01-08-profile-chi-squared-optimisation.html#performance-comparison-to-intensity-chi2",
    "href": "posts/2022-01-08-profile-chi-squared-optimisation.html#performance-comparison-to-intensity-chi2",
    "title": "Solving crystal structures with GALLOP and profile \\(\\chi^2\\)",
    "section": "Performance comparison to intensity \\(\\chi^2\\)",
    "text": "Performance comparison to intensity \\(\\chi^2\\)\nLet’s now run it again, using the standard intensity \\(\\chi^2\\) figure of merit, and plot the run times for easy comparison.\n\nstep = 1\nuse_profile_chisquared = False\ntotal_time_int = gallop_solve(mystructure, n_particles, n_swarms, learning_rate, gallop_iters, local_iterations,\n            step=step, use_profile_chisquared=use_profile_chisquared)\nprint(\"Total time (intensity):\",np.around(total_time_int / 60, 2),\"min\")\n\nGenerating start positions\n\n\n100%|██████████| 20/20 [00:00&lt;00:00, 606.07it/s]\n\n\nRunning GALLOP\n\n\nGALLOP iter 0001 LO iter 0500 min chi2 422.9: 100%|██████████| 500/500 [00:54&lt;00:00,  9.15it/s]\nGALLOP iter 0002 LO iter 0500 min chi2 247.6: 100%|██████████| 500/500 [00:55&lt;00:00,  9.06it/s]\nGALLOP iter 0003 LO iter 0500 min chi2 212.9: 100%|██████████| 500/500 [00:55&lt;00:00,  9.04it/s]\nGALLOP iter 0004 LO iter 0500 min chi2 166.9: 100%|██████████| 500/500 [00:55&lt;00:00,  9.03it/s]\nGALLOP iter 0005 LO iter 0500 min chi2 83.3: 100%|██████████| 500/500 [00:55&lt;00:00,  9.01it/s] \nGALLOP iter 0006 LO iter 0500 min chi2 58.0: 100%|██████████| 500/500 [00:55&lt;00:00,  9.02it/s]\nGALLOP iter 0007 LO iter 0500 min chi2 57.9: 100%|██████████| 500/500 [00:55&lt;00:00,  9.03it/s]\nGALLOP iter 0008 LO iter 0500 min chi2 56.7: 100%|██████████| 500/500 [00:55&lt;00:00,  9.03it/s]\nGALLOP iter 0009 LO iter 0500 min chi2 56.7: 100%|██████████| 500/500 [00:55&lt;00:00,  9.02it/s]\nGALLOP iter 0010 LO iter 0500 min chi2 57.1: 100%|██████████| 500/500 [00:55&lt;00:00,  9.01it/s]\n\n\nTotal time (intensity): 9.27 min\n\n\n\n\nCode\nplt.plot([1,2,4,8],[total_time_prof_1, total_time_prof_2, total_time_prof_4, total_time_prof_8])\nplt.plot([1,2,4,8],[total_time_int,total_time_int,total_time_int,total_time_int])\nplt.xlabel(\"Profile step\")\nplt.ylabel(\"Time (seconds)\")\nplt.legend([\"Profile $\\\\chi^2$\",\"Intensity $\\\\chi^2$\"])\nplt.show()\n\n\n\n\n\nWe see that using the implementation of profile \\(\\chi^2\\) in this notebook with the full profile incurs an approximately 10 % performance penalty relative to the intensity \\(\\chi^2\\). However, as we decrease the number of points in the profile, we see the time decrease to the extent that it ends up being about 10 % faster than the intensity \\(\\chi^2\\) figure of merit!\nHowever, given that the y-axis as plotted starts around 510 seconds, the time saving overall is not huge. This is because the biggest performance bottleneck in the GALLOP code is the internal to Cartesian coordinate conversion. So even if the figure of merit calculation follows the exponential decay function seen in the graph, then at most, we can expect around a 10 % reduction in time.\nIn terms of the quality of the results obtained, I checked the 15 molecule RMSD using Mercury - results have been plotted below. There’s not much difference between them - only at the hundredths of ångströms level.\n\n\nCode\nprofile_fifteen_mol_rmsd = [0.105, 0.108, 0.108, 0.113]\nintensity_fifteen_mol_rmsd = [0.112]\nall_data = profile_fifteen_mol_rmsd + intensity_fifteen_mol_rmsd\nlabels = [\"prof_1\", \"prof_2\", \"prof_4\", \"prof_8\", \"int\"]\nplt.bar(labels, all_data)\nplt.ylabel(\"15 molecule RMSD / Å\")\nplt.show()\n\n\n\n\n\nOne final consideration is the memory use - in my testing, I did not see an appreciable difference between the profile \\(\\chi^2\\) with different step sizes and the intensity \\(\\chi^2\\). I suspect this is because much like with the speed of the code, the internal to Cartesian coordinate conversion process contributes signficantly more to the memory use. I will look into this more in the future though!"
  },
  {
    "objectID": "posts/2022-01-08-profile-chi-squared-optimisation.html#extending-this-work",
    "href": "posts/2022-01-08-profile-chi-squared-optimisation.html#extending-this-work",
    "title": "Solving crystal structures with GALLOP and profile \\(\\chi^2\\)",
    "section": "Extending this work",
    "text": "Extending this work\nOne obvious thing to do would be to see if this can also be done easily with GSAS-II output files, or indeed other programs that are capable of Le Bail fits.\nAnother interesting possibility would be to have a go using the integrated profile strategy employed by FOX (pdf) which should afford improved performance.\nOne other thing which I really should get round to looking into is how the 20 % correlation threshold used in DASH for the intensity \\(\\chi^2\\) calculation influences the optimisation behaviour. Given that there is no performance penalty with setting this threshold in GALLOP to zero, it might be interesting to run some large-scale experiments to see if there’s anything to be gained by including such lower-level correlations into the figure of merit.\nAnyone interested in trying that for themselves can do so in GALLOP python mode easily. When reading in the data, add the percentage_cutoff argument, which defaults to 20 in order to match DASH.\nmystructure.add_data(\"filename.sdi\", source=\"DASH\", percentage_cutoff=0)\nThis also applies to GSAS-II and TOPAS derived data files."
  },
  {
    "objectID": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html",
    "href": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html",
    "title": "Restraints for rings and stereochemistry",
    "section": "",
    "text": "In this post, we’ll take a look at an approach I’ve recently been working on for refining the conformations of rings during crystal structure determination from powder diffraction data (SDPD). We’ll go over how the approach works, and how to apply it with GALLOP both via the Python API and the browser interface. The approach, which I developed in collaboration with Kenneth and Norman Shankland, was recently published in CrystEngComm, so this post is designed to give an informal summary of the paper and give some detail on how to apply it in your own work."
  },
  {
    "objectID": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#conformer-generation",
    "href": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#conformer-generation",
    "title": "Restraints for rings and stereochemistry",
    "section": "Conformer generation",
    "text": "Conformer generation\nConformer generators are often used to generate reasonable ring conformations which can then be used as the starting point for standard SDPD via GO methods. However, whilst providing a potentially straightforward solution to the ring conformation issue, such conformer generators frequently produce multiple alternative conformations, and despite much progress, often still have difficulty in reproducing sensible macrocyclic ring conformations. This can result in a large number of candidate molecular models that must be tested, with no guarantee that any of them are sufficiently close to the true conformation to obtain a solution from which subsequent Rietveld/DFT refinement could reasonably proceed."
  },
  {
    "objectID": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#other-approaches",
    "href": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#other-approaches",
    "title": "Restraints for rings and stereochemistry",
    "section": "Other approaches",
    "text": "Other approaches\nThe program FOX provides two alternative methods by which a ring conformation can be treated during SDPD: 1. Use of an alternative molecular descriptor based on the Cartesian coordinates of each atom, with restraints applied to enforce the molecular geometry 2. Use of molecular dynamics routines which allow the ring conformation to be adjusted during the SDPD procedure whilst maintaining the internal coordinate description\nBoth of these methods have been shown to work effectively. In the former, the huge increase in degrees of freedom relative to an internal coordinate based molecular descriptor is counteracted by the addition of restraints. These restrict the allowable combinations of values of the degrees of freedom to only those that maintain the molecular geometry. Where this approach doesn’t give solutions rapidly, the second approach using molecualar dynamics routines can be employed, though the computational cost of their inclusion results in run times that are approximately double that of a pure GO approach."
  },
  {
    "objectID": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#ring-breaking",
    "href": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#ring-breaking",
    "title": "Restraints for rings and stereochemistry",
    "section": "Ring breaking",
    "text": "Ring breaking\nA less commonly employed approach is to keep the Z-matrix representation, but break one of the bonds in the ring in order to convert the ring into a flexible chain of atoms. This then allows the standard Z-matrix representation to be used to refine the torsion angles within the ring.\n\n\n\nSide by side of unbroken and broken macrocycle\n\n\nHowever, this clearly comes at the expense of increasing the number of degrees of freedom which must be determined. In the figure above, we go from 8 degrees of freedom on the left (3 position, 3 orientation, 2 torsions) to 20 degrees of freedom on the right (3 position, 3 orientation, 14 torsions) - this is a pretty hefty increase!\nSo, what can we do about this? Well, the bond we broke to allow the ring conformation to adjust still exists in reality! We can therefore use a restraint that forces the distance between the atoms either side of the broken bond to be equal to the known bond length. This restraint places limits on the values that can be taken by the additional degrees of freedom to only those that reform the ring, whilst allowing any ring conformation to be adopted. The idea of using restraints to counteract the increase in the number of degrees of freedom forms the basis of our approach.\nWe were not the first people by any means to make use of restraints in this context - see for example a recent article from the Bari group who used EXPO to do essentially the same thing (incidentally, this was published after we submitted our first version of our article to CEC, so the timing is a complete coincidence). However, I think our work still provides some new insights. Firstly, the combined use of bond breaking and restraints is not limited in applicability to ring systems - all sorts of other problems could be tackled more efficiently using this method (discussed more later in this post). Secondly, we also show that GALLOP continues to provide extremely impressive performance - even with the large numbers of degrees of freedom that bond breaking introduces; results are typically obtained in tens of minutes rather than tens of hours or days!"
  },
  {
    "objectID": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#distance-restraints",
    "href": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#distance-restraints",
    "title": "Restraints for rings and stereochemistry",
    "section": "Distance restraints",
    "text": "Distance restraints\nAny two atoms in the asymmetric unit can be directed to sit a defined distance apart by adding the following penalty term to the cost function that is to be minimised:\n\\[ D = (|\\vec{u}_{ij}| - \\delta_{ij})^2 \\]\nwhere \\(D\\) is the penalty term, \\(|\\vec{u}_{ij}|\\) is the magnitude of the vector pointing from atom \\(i\\) to atom \\(j\\) and \\(\\delta_{ij}\\) is the distance provided by the user as the restraint. It can take values from 0 to ∞.\nWhilst in the context of ring conformations, the distance penalty is likely to be used to restore broken bonds, it can also be used to enforce other distance-based restraints, between any of the atoms in the asymmetric unit."
  },
  {
    "objectID": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#angle-restraints",
    "href": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#angle-restraints",
    "title": "Restraints for rings and stereochemistry",
    "section": "Angle restraints",
    "text": "Angle restraints\nAngles between any two interatomic vectors can also form the basis of a restraint. Typically this would be for a standard bond angle, though it could also be used to, for example, ensure that bonds on opposite sides of a ring are held parallel to each other. The penalty term \\(A\\) is defined as:\n\\[ A = \\left(\\frac{\\vec{u}_{ij}\\cdot\\vec{u}_{kl}}{|\\vec{u}_{ij}||\\vec{u}_{kl}|} - \\cos{\\alpha_{ijkl}}\\right)^2 \\]\nwhere \\(\\vec{u}_{ij}\\) is the vector pointing from atom \\(i\\) to atom \\(j\\), \\(\\vec{u}_{kl}\\) is the vector pointing from atom \\(k\\) to atom \\(l\\) and \\(\\alpha_{ijkl}\\) is the angle between the interatomic vectors supplied by the user. If atom \\(i\\) and atom \\(k\\) are the same atom, then this equation gives a standard bond angle restraint. In the GALLOP code, the cosine of the user supplied angle is calculated and stored in advance for efficiency. The penalty term is bounded to values in the range 0 to 4 (inclusive)."
  },
  {
    "objectID": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#torsion-angle-restraints",
    "href": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#torsion-angle-restraints",
    "title": "Restraints for rings and stereochemistry",
    "section": "Torsion angle restraints",
    "text": "Torsion angle restraints\nTorsion angles can also be used as a restraint. Whilst in general this will be a normal molecular torsion angle, it’s not a requirement - any two pairs of intersecting planes can be used as the basis for a restraint. This could be used to ensure the relative orientations of separated planes of atoms if required. The penalty term, \\(T\\) is defined as:\n\\[ T = \\left(\\frac{|\\vec{u}_{jk}|\\vec{u}_{ij}\\cdot(\\vec{u}_{jk}\\times\\vec{u}_{kl})}{|\\vec{u}_{ij}\\times\\vec{u}_{jk}||\\vec{u}_{jk}\\times\\vec{u}_{kl}|} - \\sin{\\tau_{ijkl}}\\right)^2 + \\left(\\frac{(\\vec{u}_{ij}\\times\\vec{u}_{jk})\\cdot(\\vec{u}_{jk}\\times\\vec{u}_{kl})}{|\\vec{u}_{ij}\\times\\vec{u}_{jk}||\\vec{u}_{jk}\\times\\vec{u}_{kl}|} - \\cos{\\tau_{ijkl}}\\right)^2\\]\nwhere \\(\\vec{u}_{ij}\\) is the vector pointing from atom \\(i\\) to atom \\(j\\), \\(\\vec{u}_{jk}\\) is the vector pointing from atom \\(j\\) to atom \\(k\\), \\(\\vec{u}_{kl}\\) is the vector pointing from atom \\(k\\) to atom \\(l\\) and \\(\\tau_{ijkl}\\) is the torsion angle supplied by the user. As with the angle penalty term, the sine and cosine of the angle are calculated in advance and stored. This penalty term is also bounded to values in the range 0 to 4 (inclusive)."
  },
  {
    "objectID": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#cost-function-and-restraint-weighting",
    "href": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#cost-function-and-restraint-weighting",
    "title": "Restraints for rings and stereochemistry",
    "section": "Cost function and restraint weighting",
    "text": "Cost function and restraint weighting\nDue to the generally small values of the restraint penalty terms relative to typical SDPD \\(\\chi^2\\) values, we need to weight the penalty terms so that their magnitude is sufficient to have a meaningful impact on the optimisation process. The approach that I ended up going with is to scale all of the penalty terms by the value of \\(\\chi^2\\) at every iteration so that their numerical values are comparable. I’ve also made it possible to further set the relative importance of each individual restraint by allowing users to set their own weights for each restraint. A weight of 1 indicates that the restraint has equal importance to \\(\\chi^2\\), weights less than or greater than 1 therefore enable the relative importance to be down- or up-weighted respectivly.\nOne thing that took me a little while to work out when I was implementing this (in retrospect this was an obvious thing I should have considered!) is that it’s not OK to just multiply the tensor of penalty terms by the tensor of \\(\\chi^2\\) values. To see why, let’s say that we have a structure with \\(a\\) distance restraints, \\(b\\) angle restraints and \\(c\\) torsion restraints (and their associated weights, \\(w_a\\), \\(w_b\\), and \\(w_c\\)). Let’s define the cost function that the local optimiser is trying to minimise as:\n\\[ C(\\textbf{x}) = \\chi^2(\\textbf{x}) + \\chi^2(\\textbf{x})\\left(\\sum_a w_a{D(\\textbf{x})} + \\sum_b{w_bA(\\textbf{x})} + \\sum_c{w_cT(\\textbf{x})}\\right) \\]\nwhere \\(\\textbf{x}\\) represents the structural degrees of freedom. Let’s simplify this using \\(R\\) to represent the sum of all the weighted penalty terms:\n\\[ C(\\textbf{x}) = \\chi^2(\\textbf{x})(1 + R(\\textbf{x})) \\]\nBy the product rule, the gradient of this function is:\n\\[ \\nabla C(\\textbf{x}) = (1 + R(\\textbf{x}))\\nabla \\chi^2(\\textbf{x}) + \\chi^2(\\textbf{x})\\nabla R(\\textbf{x}) \\]\nWe can see that this could lead to problems because of the scaling of the gradient of \\(\\chi^2\\) by a factor of \\(1 + R(\\textbf{x})\\). In my testing this led to some very strange runs where it looked like progress was being made but then the optimisation behaviour resulted in strange oscillations in the value of \\(\\chi^2\\) and failure to obtain a solution on even simple problems.\nThe solution I landed on is to make a gradient-free copy of the \\(\\chi^2\\) tensor, i.e. a copy that is not a function of the degrees of freedom which can act purely as a numerical scaling factor. This converts the cost function into:\n\\[ C(\\textbf{x}) = \\chi^2(\\textbf{x}) + \\chi^2_{copy}\\left(\\sum_a w_a{D(\\textbf{x})} + \\sum_b{w_bA(\\textbf{x})} + \\sum_c{w_cT(\\textbf{x})}\\right) = \\chi^2(\\textbf{x}) + \\chi^2_{copy}R(\\textbf{x})) \\]\nand hence the gradient into:\n\\[ \\nabla C(\\textbf{x}) = \\nabla \\chi^2(\\textbf{x}) + \\chi^2_{copy} \\nabla R(\\textbf{x}) \\]\nThis gives the optimisation behaviour that was expected and desired! In pytorch (the library used by GALLOP to perform the automatic differentiation), it’s very easy to make the required gradient-free copy of the \\(\\chi^2\\) tensor:\nchisquared_copy = chisquared.detach().clone()\nThe .detach() method means that the computational graph associated with the tensor is removed, thus removing the dependence on the degrees of freedom. The .clone() method then copies the contents into the new tensor which can then act purely as a scaling factor."
  },
  {
    "objectID": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#preparing-models-and-restraints",
    "href": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#preparing-models-and-restraints",
    "title": "Restraints for rings and stereochemistry",
    "section": "Preparing models and restraints",
    "text": "Preparing models and restraints\nI have been using Mercury to modify the structures and to break bonds and measure distances, angles and torsions. I then save the structure as a .mol2 file, which can then be used with DASH to generate a Z-matrix. Occasionally, there’ll be issues with the resultant Z-matrices using hydrogen atoms to define rotatable torsions. In such cases, I recommend deleting all hydrogen atoms prior to saving the mol2, then regenerating the Z-matrices. The hydrogen atoms can be added back in once the structure has solved.\nAs our example, we’ll use the structure with CSD refcode IJUXUI, a Schiff base with a 17-membered macrocyclic ring. Diffraction data and the structure can be downloaded here - I’m not sure if I can rehost the data here, but if you would like a copy of my fit files and models, feel free to message me and I’ll send them over.\nI opened the CSD entry for IJUXUI in Mercury and deleted the bond between O2 and C8. I also converted the two C=N double bonds into single bonds, and ended up with the following model, shown here with the atom labels involved with the cut bonds and restricted torsion angles, and hydrogen atoms hidden for clarity.\n\n\n\nIJUXUI cut model\n\n\nWe are going to use the following restraints for our runs:\n\nDistance restraint between O2 and C8, set to 1.44 Å\nTorsion angle restraint using atoms C3, C12, N1 and C13 set to \\(180 \\degree\\)\nTorsion angle restraint using atoms C18, C17, N2 and C16 set to \\(180 \\degree\\)\n\n\nGALLOP Learning rate\nOne thing I have found is that the learning rate finder built into GALLOP doesn’t work as well when there are restraints in play, so my recommendation is to use the learning rate finder as normal with either a cut or a non-cut model, then use the learning rate obtained there when the restraints are active. A bit of experimentation might be needed to get a reasonable learning rate, however, as a general rule I choose the larger of the two value obtained.\nFor my fit files and the un-cut model, I obtained a learning rate of 0.0325, which I’ll be using for the rest of this work."
  },
  {
    "objectID": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#browser-interface",
    "href": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#browser-interface",
    "title": "Restraints for rings and stereochemistry",
    "section": "Browser interface",
    "text": "Browser interface\nStart the GALLOP browser interface as normal (I’m using Kaggle, so my jobs are running on a P100 GPU), then upload the files needed. Modify the number of swarms and number of particles per swarm as you see fit, then we’ll need to tweak a couple of settings. You may also wish to reduce the total number of iterations as this structure will solve very quickly - 5 GALLOP iterations with 10 swarms of 1000 particles should be more than sufficient.\nLearning rate - open the Local Optimiser settings, then select “Show advanced options”. Scroll down until you see “Find learning rate”, and unselect that box. A new number input widget will appear, in here, set the learning rate to 0.0325 (or whatever value you obtain with your fit files). When you press enter, the display may show “0.03”, but rest assured that the additional decimal places are still recorded internally - not that they make much difference in practice!\n\nRestraints - continue scrolling down the advanced local optimiser settings, until you see check boxes for restraints:\n\nWe’ll be using both distance and torsion restraints, so select these. New input widgets will appear as a result:\n\nFor the distance restraint, we only need one, so all we need to do is specify the atoms involved, the distance value we want to use, and the weight to apply to this restraint - we’ll use a weight of 1.0:\nFor the torsion restraints, we’ll need two of these, so we modify the number of restraints to 2, and then the boxes allow us to specify each one:\n\nAs a brief aside, even though we aren’t using them in this case, it’s worth talking about using angle restraints in the browser interface. As GALLOP allows the flexibility to specify angle restraints between any two interatomic vectors, we can either supply 3 or 4 atom labels depending on what we want to do. For normal bond angles, we only provide three atom labels. For example, if we want the angle C2 - C1 - C3 (i.e. atom C1 in the middle) to be restrained to 120 degrees with a weight of 1.0, we would enter the following into the widget:\nC2,C1,C3,120,1.0\nAlternatively, if we have two interatomic vectors, for example C1 \\(\\rightarrow\\) C2 and C3 \\(\\rightarrow\\) C4, and we want them to be perpendicular with a weight of 0.5, we would use the following input:\nC1,C2,C3,C4,90,0.5\n\nRunning GALLOP\nOnce the restraints and other settings have been entered, we’re good to go! Start GALLOP as normal. On this structure with a P100 GPU accessed via Kaggle, this process took less than 3 minutes to complete:\n\nOne thing that GALLOP allows is to save animations of the trajectory taken by each particle. This option slows things down a fair bit, but does produce some cool animations! Click this link for an interactive animation (move, zoom etc using mouse) of the trajectory taken by the best performing particle during the first iteration of a different set of runs, with hydrogen atoms hidden for clarity. On the left, you see the whole unit cell, with the asymmetric unit shown on the right. You can see that the ring very rapidly reforms due to the action of the distance restraint.\nNote: an earlier version of this blog post had the animation embedded, but I found it runs fairly slowly on old hardware so it’s now accessible via the link above\nThe “shaking” that we see is due to the high learning rate in the middle of the local optimisation process, as per the 1-cycle learning rate policy used in GALLOP by default. This causes some oscillation due to the larger step sizes taken in the middle of the run."
  },
  {
    "objectID": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#python-api",
    "href": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#python-api",
    "title": "Restraints for rings and stereochemistry",
    "section": "Python API",
    "text": "Python API\nAs with the browser interface, we’ll set up our runs as normal then implement the restraints and fix the learning rate. I’ll run these jobs on my PC (RTX 2060 GPU) to provide a comparison against the P100 for performance. If you’re running this notebook yourself on Colab or Kaggle, you’ll need to install GALLOP first by uncommenting the code cell below (hidden on blog).\nAs usual, we start with our library imports and creating a structure object. We then use the structure methods read in the diffraction data and add Z-matrices. For a more detailed overview of using the Python API, see this post.\n\n\nCode\n# Uncomment the line below, then run this cell to install GALLOP on Colab or Kaggle\n#!git clone https://github.com/mspillman/gallop.git && cd gallop && pip install .\n\n\n\nfrom gallop.structure import Structure\nfrom gallop.optim import local\nfrom gallop.optim import Swarm\n\nmystructure = Structure(name=\"IJUXUI_cut_with_restraints\", ignore_H_atoms=True)\nmystructure.add_data(\"files/IJUXUI.sdi\", source=\"DASH\")\nmystructure.add_zmatrix(\"files/IJUXUI_cut_1.zmatrix\")\n\nAdded Z-matrix with Filename: files/IJUXUI_cut_1.zmatrix\nNon-H atoms: 29\nrefinable torsions: 14\nDegrees of freedom: 21 (7 + 14)\n\n\nNow let’s add the restraints. The below code cell shows how to do this using the unique atom-label approach that was also used in the browser interface - we pass a dictionary to the function containing key/value pairs corresponding to the atom labels, the desired value for the distance/angle/torsion and the weight for the restraint.\n\nmystructure.add_restraint({\"type\" : \"distance\",\n                        \"atom1\" : \"O2\",\n                        \"atom2\" : \"C8\",\n                        \"value\" : 1.44,\n                        \"weight\" : 1.0})\n\nmystructure.add_restraint({\"type\" : \"torsion\",\n                        \"atom1\" : \"C3\",\n                        \"atom2\" : \"C12\",\n                        \"atom3\" : \"N1\",\n                        \"atom4\" : \"C13\",\n                        \"value\" : 180.0,\n                        \"weight\" : 1.0})\n\nmystructure.add_restraint({\"type\" : \"torsion\",\n                        \"atom1\" : \"C18\",\n                        \"atom2\" : \"C17\",\n                        \"atom3\" : \"N2\",\n                        \"atom4\" : \"C16\",\n                        \"value\" : 180.0,\n                        \"weight\" : 1.0})\n\n\nAngle restraints\nWhilst we aren’t using them here, for an angle restraint, we need to specify two interatomic vectors. If we want this to be a standard bond angle, then atom1 and atom3 need to be the same atom, for example, in a structure where the angle C2 - C1 - C3 is 120 degrees, then we want the angle between the vectors C1 \\(\\rightarrow\\) C2 and C1 \\(\\rightarrow\\) C3 to be 120 degrees, so we would use the following command:\nmystructure.add_restraint({\"type\" : \"angle\",\n                        \"atom1\" : \"C1\",\n                        \"atom2\" : \"C2\",\n                        \"atom3\" : \"C1\",\n                        \"atom4\" : \"C3\",\n                        \"value\" : 120.0,\n                        \"weight\" : 1.0})\nIf we weren’t working with a normal bond angle, then to specify a restraint between two interatomic vectors, e.g. C1 \\(\\rightarrow\\) C2 and C3 \\(\\rightarrow\\) C4, then we would use something like this:\nmystructure.add_restraint({\"type\" : \"angle\",\n                        \"atom1\" : \"C1\",\n                        \"atom2\" : \"C2\",\n                        \"atom3\" : \"C3\",\n                        \"atom4\" : \"C4\",\n                        \"value\" : 120.0,\n                        \"weight\" : 1.0})\n\n\nOther input methods\nIf we didn’t have unique atom labels for each of the atoms in the asymmetric unit, we can instead specify the atoms involved by referring to their associated Z-matrix and the position of the atom within the Z-matrix. For example, a distance restraint between the 4th atom in zmatrix_1.zmatrix and the 8th atom in zmatrix_3.zmatrix, with a distance of 3.0 Å and weight of 0.5 can be added via:\nmystructure.add_restraint({\"type\" : \"distance\",\n                        \"zm1\":\"zmatrix_1.zmatrix\", \"atom1\":4,\n                        \"zm2\":\"zmatrix_3.zmatrix\", \"atom2\":8,\n                        \"value\":3,\n                        \"weight\":0.5})\nAlternatively, assuming that the zmatrices were added sequentially starting with zmatrix_1.zmatrix:\nmystructure.add_restraint({\"type\":\"distance\",\n                        \"zm1\":1, \"atom1\":4,\n                        \"zm2\":3, \"atom2\":8,\n                        \"value\":3.0,\n                        \"weight\":0.5})\nThe same principles apply to angle and torsion restraints, they just require changing the “type” key/value pair, as well as additional entries for atoms 3 and 4, and their associated ZMs. For example:\nmystructure.add_restraint({\"type\":\"torsion\",\n                        \"zm1\":2, \"atom1\":4,\n                        \"zm2\":2, \"atom2\":8,\n                        \"zm3\":2, \"atom3\":12,\n                        \"zm4\":2, \"atom4\":14,\n                        \"value\": 130.0,\n                        \"weight\": 0.5})\n\n\nRunning GALLOP\nNow that we have our restraints specified, let’s set up our GALLOP parameters and loop, then run it. We’ll need to remember to pass the use_restraints = True bool to the local optimiser function. The easiest way to do this is to modify the value in the minimiser_settings dictionary.\nAgain, we’ll go for 10 swarms of 1000 particles, and run GALLOP for 5 iterations.\n\nimport time\nswarm = Swarm(mystructure, n_particles=10000, n_swarms=10)\nexternal, internal = swarm.get_initial_positions()\n\n# Get the default minimiser settings\nminimiser_settings = local.get_minimiser_settings(mystructure)\n\n# Set the learning rate to what we obtain from a rigid ring model and learning rate finder\nminimiser_settings[\"learning_rate\"] = 0.0325\n\n# Toggle this run to use the restraints we added\nminimiser_settings[\"use_restraints\"] = True\n\n# Set the total number of iterations for the GALLOP run\ngallop_iters = 5\nstart_time = time.time()\n# The main GALLOP loop\nfor i in range(gallop_iters):\n    # Local optimisation of particle positions\n    result = local.minimise(mystructure, external=external, internal=internal,\n                run=i, start_time=start_time, **minimiser_settings)\n    # Particle swarm update generates new positions to be optimised\n    external, internal = swarm.update_position(result=result)\n\n100%|██████████| 10/10 [00:00&lt;00:00, 714.26it/s]\nGALLOP iter 0001 LO iter 0500 min chi2 159.8: 100%|██████████| 500/500 [00:48&lt;00:00, 10.32it/s]\nGALLOP iter 0002 LO iter 0500 min chi2 88.3: 100%|██████████| 500/500 [00:48&lt;00:00, 10.31it/s] \nGALLOP iter 0003 LO iter 0500 min chi2 87.0: 100%|██████████| 500/500 [00:48&lt;00:00, 10.29it/s] \nGALLOP iter 0004 LO iter 0500 min chi2 86.9: 100%|██████████| 500/500 [00:48&lt;00:00, 10.23it/s] \nGALLOP iter 0005 LO iter 0500 min chi2 86.8: 100%|██████████| 500/500 [00:48&lt;00:00, 10.37it/s] \n\n\nAs you can see, we hit approximately the same \\(\\chi^2\\) value as we did with the browser interface, though due to the use of my desktop PC rather than a cloud-based GPU, things took a little longer.\nAs a point of comparison, let’s rerun the jobs above, this time without the restraints. We should see a significant difference in the rate of convergence.\n\n# Create a new swarm object so we don't start from the end point of the last run\nswarm = Swarm(mystructure, n_particles=10000, n_swarms=10)\nexternal, internal = swarm.get_initial_positions()\n\n# We'll use the same settings as the previous run\n# However, we'll turn off the restraints for this run.\nminimiser_settings[\"use_restraints\"] = False\n\n# Set the total number of iterations for the GALLOP run\ngallop_iters = 5\nstart_time = time.time()\n# The main GALLOP loop\nfor i in range(gallop_iters):\n    # Local optimisation of particle positions\n    result = local.minimise(mystructure, external=external, internal=internal,\n                run=i, start_time=start_time, **minimiser_settings)\n    # Particle swarm update generates new positions to be optimised\n    external, internal = swarm.update_position(result=result)\n\n100%|██████████| 10/10 [00:00&lt;00:00, 625.06it/s]\nGALLOP iter 0001 LO iter 0500 min chi2 423.0: 100%|██████████| 500/500 [00:45&lt;00:00, 10.90it/s]\nGALLOP iter 0002 LO iter 0500 min chi2 147.0: 100%|██████████| 500/500 [00:46&lt;00:00, 10.78it/s]\nGALLOP iter 0003 LO iter 0500 min chi2 126.0: 100%|██████████| 500/500 [00:45&lt;00:00, 10.96it/s]\nGALLOP iter 0004 LO iter 0500 min chi2 119.2: 100%|██████████| 500/500 [00:45&lt;00:00, 10.96it/s]\nGALLOP iter 0005 LO iter 0500 min chi2 104.9: 100%|██████████| 500/500 [00:45&lt;00:00, 10.89it/s]\n\n\nAs we can see, without restraints, we don’t reach the bottom of the global minimum within the 5 iterations alloted (though we get close - the final CIF gave a relatively low RMSD of 0.204 Å against the published structure). Whilst this is a small sample size, it does seem like restraints are providing a significant benefit.\nHowever, using restraints comes at a cost: as we can see, using the restraints results in an approximately 7 % increase in the time taken to perform the runs. This is simply because there are more operations that GALLOP has to carry out in order to calculate the restraint penalty term values and their associated gradients. However, this computational cost is fairly small and hopefully it’s obvious that even for relatively simple problems like the one we are tackling here, the benefits outweigh the increased computational cost!"
  },
  {
    "objectID": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#other-applications",
    "href": "posts/2022-05-30-restraints-for-rings-and-stereochemistry.html#other-applications",
    "title": "Restraints for rings and stereochemistry",
    "section": "Other applications",
    "text": "Other applications\nRestraints in general enable the inclusion of additional information into the SDPD process, reducing the search space and (hopefully) increasing the probability of success. The information used to specify the restraints can come from a wide variety of sources. Whilst we have focussed here on using known bond lengths and expected torsion angle values, we could also include information from a multitude of other sources.\nFor example, solid state NMR experiments can be used to obtain intramolecular end-to-end distances. As was demonstrated by Middleton et al, these distances can be used to force the molecualar fragments being optimised to adopt conformations close to that at the global minimum during the early stages of optimisation, significatly reducing the time taken to solve the crystal structure.\nCrystallographic databases can also be used to derive both positional and conformational information. For example, protonated quarternary ammonium cations and chloride counter ions are commonly found approximately 3 Å apart, providing the basis of an easily applied restraint. In two previous posts, we looked at solving the crystal structure of verapamil hydrochloride, which includes this feature. I ran 20 swarms of 1000 particles for 10 iterations, both with and without the Cl1 to N1 3 Å restraint applied, with a weight of 1. The learning rate was set to 0.05 in both cases. With restraints, 6 swarms reached the global minimum (30 %), whilst without, only 2 swarms (10 %) obtained the solution. This is a very small sample size, but I still think it’s reasonable to suggest that the restraint provides some benefit for very little effort! Hydrogen-bond propensity calculations could also be used to inform restraints on the basis of likely intermolecular contact distances.\nThe use of database-derived torsion angle distribution information is well established in SDPD via the use of techniques such as the Mogul Distribution Bias (MDB) which has been shown to greatly improve success rates in DASH. It worth noting that you can also use MDB information in GALLOP. This is described in the context of the browser interface and Python API in other posts. Other torsional information could be manually included via restraints if desired.\nRecent advances in the field of protein folding allow both protein conformations and intramolecular residue contact distances to be predicted. Clearly such information could also be used to inform restraints for SDPD applications."
  },
  {
    "objectID": "posts/2023-01-23-profile-chi-squared-when-dash-pawley-fails.html",
    "href": "posts/2023-01-23-profile-chi-squared-when-dash-pawley-fails.html",
    "title": "Solving crystal structures with GALLOP after ill conditioning errors in DASH",
    "section": "",
    "text": "It’s been a long time since I updated this blog! I’m hoping to be able to update it at least a few more times this year, as I have some ideas for posts which I think some people may find useful. One post will (finally!) cover how to use TPUs with GALLOP. It’s taken this long because the free TPU resources available online up to this point have been extremely limited. Fortunately, Kaggle have given their users a generous TPU quota and importantly, access to a powerful enough VM to make the most of the performance of the TPU using pytorch, which is the library GALLOP uses for all of its calculations. I am also working on some stuff unrelated to GALLOP which I’m quite excited about, with a focus on PXRD indexing. I’m hoping to get it written up as a paper before the end of the year. If I manage that (and it’s a big if), there will be an accompanying blog post giving an informal summary of the work.\n\n\nThis is a very brief follow up to a previous post in which we looked at using GALLOP to optimise using the profile rather than intensity \\(\\chi^2\\) figure of merit. In that post, we showed that doing this potentially provides a small level of benefit in terms of the speed with which the runs are completed, though the number of solutions and frequency with which they are obtained is not significantly affected. As the bulk of the time spent by GALLOP is used for conversion of internal coordinates to Cartesian coordinates, the potential performance benefits are somewhat limited. Despite this, I’ve added the functionality to the latest version of GALLOP so you can try it for yourself. At the moment, this functionality is limited to data that has been fitted using DASH.\nIn this post, I want to talk about an idea I had recently on a situation in which this added capability in GALLOP may be useful."
  },
  {
    "objectID": "posts/2023-01-23-profile-chi-squared-when-dash-pawley-fails.html#this-post",
    "href": "posts/2023-01-23-profile-chi-squared-when-dash-pawley-fails.html#this-post",
    "title": "Solving crystal structures with GALLOP after ill conditioning errors in DASH",
    "section": "",
    "text": "This is a very brief follow up to a previous post in which we looked at using GALLOP to optimise using the profile rather than intensity \\(\\chi^2\\) figure of merit. In that post, we showed that doing this potentially provides a small level of benefit in terms of the speed with which the runs are completed, though the number of solutions and frequency with which they are obtained is not significantly affected. As the bulk of the time spent by GALLOP is used for conversion of internal coordinates to Cartesian coordinates, the potential performance benefits are somewhat limited. Despite this, I’ve added the functionality to the latest version of GALLOP so you can try it for yourself. At the moment, this functionality is limited to data that has been fitted using DASH.\nIn this post, I want to talk about an idea I had recently on a situation in which this added capability in GALLOP may be useful."
  },
  {
    "objectID": "posts/2023-09-08-Generating-synthetic-PXRD-data.html",
    "href": "posts/2023-09-08-Generating-synthetic-PXRD-data.html",
    "title": "Generating synthetic diffraction data from PowCod database",
    "section": "",
    "text": "Neural networks have become a powerful tool for automating complex tasks in crystallography, including PXRD data analysis. For example, indexing, crystallite size and scale factor determination, as well as crystal system and space group classification. There are many more examples in the literature! In future posts, we’ll look at how to train our own neural networks for analysing PXRD data. However, before we get there, we’ll need a large volume of PXRD against which they can be trained.\nTo the best of my knowledge, there is no extant database of experimental PXRD patterns. However, there are several databases, such as the COD, CCDC’s CSD and ICDD’s PDF, which provide experimental crystal structures. These databases, which contain hundreds of thousands of crystal structures can then be used to calculate diffraction patterns. The CSD has a python API which allows PXRD data to be generated for each of the entries in the database. However, this is not optimised to run quickly, and as such, PXRD data would need to be generated in advance.\nIn this post, we’ll look at how we can efficiently calculate large numbers of diffraction patterns on-the-fly whilst training neural networks. By writing our own code to generate synthetic PXRD data, we will also be able to implement various data augmentation transformations. Such transformations make the calculated PXRD data look different from the perspective of a machine learning algorithm, but could still plausibly be produced by the same underlying material. This will help to make our future neural networks more robust and able to be applied to real-world diffraction data."
  },
  {
    "objectID": "posts/2023-09-08-Generating-synthetic-PXRD-data.html#filtering-powcod-data",
    "href": "posts/2023-09-08-Generating-synthetic-PXRD-data.html#filtering-powcod-data",
    "title": "Generating synthetic diffraction data from PowCod database",
    "section": "Filtering PowCod data",
    "text": "Filtering PowCod data\nThe PowCod database, which can be obtained here, is made up of several .sql files which need to be parsed to extract the desired information. In our case, what we are after is: - Unit cells - Crystal systems - Space groups - Miller indices - Intensities\nWe’ll probably also need to use some additional information to help us along the way, for example, the d-spacing of the reflections.\nAs the files total only around 5.5 GB, I used pandas to read the database into memory and extract the information needed. I pickled the resultant dataframe to enable it to be more easily read in the future.\n\nimport pandas as pd\nimport os\n\n\ndef read_powcod_sql():\n    import sqlite3\n    con = sqlite3.connect('cod2205.sq')\n    df = pd.read_sql_query('SELECT * FROM id',con)\n    con2 = sqlite3.connect('cod2205.sq.info')\n    df2 = pd.read_sql_query('SELECT * FROM info',con2)\n\n    df.id = pd.to_numeric(df.id)\n    df2.id = pd.to_numeric(df2.id)\n\n    combined = df.merge(df2, left_on=\"id\", right_on=\"id\")\n    return combined\n\nif os.path.isfile(\"combined.pkl\"):\n    combined = pd.read_pickle(\"combined.pkl\")\nelse:\n    combined = read_powcod_sql()\n    print(combined.columns)\n    combined = combined[[\"spacegroup_x\", \"a\", \"b\", \"c\", \"alpha\", \"beta\",\n                    \"gamma\", \"volume\", \"h\", \"k\", \"l\", \"nd\", \"dvalue\",\n                    \"intensita\", \"type\", \"id\"]]\n    combined.to_pickle(\"combined.pkl\")\n\nLet’s have a look at what we’ve got:\n\n\nShow the code\ncombined\n\n\n\n\n\n\n\n\n\nspacegroup_x\na\nb\nc\nalpha\nbeta\ngamma\nvolume\nh\nk\nl\nnd\ndvalue\nintensita\ntype\nid\n\n\n\n\n0\nC m c m\n0.0000\n0.0000\n0.0000\n0.0\n0.000\n0.0\n738.078\n0\n0\n0\n1\n0.000000\n0.000000\nOrthorhombic\n1010866\n\n\n1\nI 4/m m m\n0.0000\n0.0000\n0.0000\n0.0\n0.000\n0.0\n388.473\n0\n0\n0\n1\n0.000000\n0.000000\nTetragonal\n6000382\n\n\n2\nP -1\n0.0000\n0.0000\n0.0000\n0.0\n0.000\n0.0\n429.928\n0\n0\n0\n1\n0.000000\n0.000000\nTriclinic\n6000073\n\n\n3\nP -3\n0.0000\n0.0000\n0.0000\n0.0\n0.000\n0.0\n1221.297\n0\n0\n0\n1\n0.000000\n0.000000\nTrigonal (hexagonal axes)\n6000152\n\n\n4\nP 1\n0.0000\n0.0000\n0.0000\n0.0\n0.000\n0.0\n552.983\n0\n0\n0\n1\n0.000000\n0.000000\nCubic\n6000430\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n458155\nP 1 21 1\n6.6330\n15.4171\n11.9417\n90.0\n103.335\n90.0\n1188.257\n0,0,0,1,0,1,1,1,0,0,1,1,1,1,1,0,1,0,1,1,1,1,1,...\n0,1,2,0,2,0,1,1,0,1,0,2,0,1,2,3,1,2,2,2,3,3,0,...\n1,1,0,0,1,-1,0,-1,2,2,1,0,-2,1,-1,1,-2,2,1,-2,...\n499\n11.619700,9.279300,7.708500,6.454200,6.423600,...\n1000.000000,62.638200,82.940500,3.929300,0.067...\nMonoclinic\n4344045\n\n\n458156\nC 1 2/m 1\n21.2287\n17.8117\n12.3055\n90.0\n124.759\n90.0\n3822.662\n1,1,0,2,0,2,0,2,1,3,2,2,1,1,3,3,1,2,4,2,0,4,1,...\n1,1,0,0,2,0,2,2,1,1,2,0,3,1,1,1,3,0,0,2,0,0,3,...\n0,-1,1,-1,0,0,1,-1,1,-1,0,-2,0,-2,0,-2,-1,1,-1...\n498\n12.461500,10.123800,10.109700,9.999700,8.90580...\n1000.000000,1.459300,384.189200,186.432500,88....\nMonoclinic\n4123491\n\n\n458157\nC 1 2/m 1\n21.5314\n18.0366\n12.3918\n90.0\n124.872\n90.0\n3948.216\n1,1,0,2,0,2,0,2,1,3,2,2,1,3,1,3,1,2,4,2,0,4,1,...\n1,1,0,0,2,0,2,2,1,1,2,0,3,1,1,1,3,0,0,2,0,0,3,...\n0,-1,1,-1,0,0,1,-1,1,-1,0,-2,0,0,-2,-2,-1,1,-1...\n498\n12.620300,10.213500,10.166600,10.122700,9.0183...\n1000.000000,0.425500,401.199700,131.443500,109...\nMonoclinic\n4123492\n\n\n458158\nI -4 2 m\n10.2400\n10.2400\n9.6520\n90.0\n90.000\n90.0\n1012.086\n1,1,2,0,2,1,2,2,3,3,1,2,3,3,2,4,3,0,4,3,4,1,4,...\n1,0,0,0,1,1,2,0,1,0,0,2,2,1,1,0,3,0,1,0,2,1,0,...\n0,1,0,2,1,2,0,2,0,1,3,2,1,2,3,0,0,4,1,3,0,4,2,...\n302\n7.240800,7.023700,5.120000,4.826000,4.137400,4...\n435.793700,1000.000000,156.773800,74.174400,1....\nTetragonal\n1508227\n\n\n458159\nP n a 21\n10.3130\n8.1940\n4.9930\n90.0\n90.000\n90.0\n421.932\n1,2,2,0,0,1,1,2,2,2,3,1,2,3,1,3,4,0,4,2,0,1,3,...\n1,0,1,1,2,1,2,0,1,2,1,2,2,1,3,2,0,0,1,3,3,3,2,...\n0,0,0,1,0,1,0,1,1,0,0,1,1,1,0,0,0,2,0,0,1,1,1,...\n484\n6.415500,5.156500,4.364200,4.263800,4.097000,3...\n884.735400,207.638600,175.487800,594.721200,42...\nOrthorhombic\n8103382\n\n\n\n\n458160 rows × 16 columns\n\n\n\n\n\nShow the code\ncombined.describe()\n\n\n\n\n\n\n\n\n\na\nb\nc\nalpha\nbeta\ngamma\nvolume\nnd\nid\n\n\n\n\ncount\n458160.000000\n458160.000000\n458160.000000\n458160.000000\n458160.000000\n458160.000000\n458160.000000\n458160.000000\n4.581600e+05\n\n\nmean\n12.505100\n13.372528\n16.330272\n89.503372\n95.199651\n90.906179\n3178.613746\n454.593930\n4.604041e+06\n\n\nstd\n6.874009\n6.655914\n8.092967\n8.326557\n11.649541\n10.924374\n6384.755145\n119.026662\n2.281053e+06\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n1.000000e+06\n\n\n25%\n8.282900\n9.153000\n11.279975\n90.000000\n90.000000\n90.000000\n1089.974000\n498.000000\n2.221300e+06\n\n\n50%\n10.780000\n12.130000\n15.169000\n90.000000\n92.430000\n90.000000\n1976.458000\n499.000000\n4.302406e+06\n\n\n75%\n14.703000\n16.140000\n19.860000\n90.000000\n101.688000\n90.000000\n3562.973750\n500.000000\n7.046806e+06\n\n\nmax\n189.800000\n150.000000\n475.977800\n150.172000\n173.895000\n149.900000\n740478.188000\n500.000000\n9.016730e+06\n\n\n\n\n\n\n\n\n\nShow the code\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(2, 4, figsize=(18,7))\nsns.kdeplot(combined.a, ax=ax[0][0])\nsns.kdeplot(combined.b, ax=ax[0][1])\nsns.kdeplot(combined.c, ax=ax[0][2])\nsns.kdeplot(combined.volume, ax=ax[0][3])\nsns.kdeplot(combined.alpha, ax=ax[1][0])\nsns.kdeplot(combined.beta, ax=ax[1][1])\nsns.kdeplot(combined.gamma, ax=ax[1][2])\nsns.kdeplot(combined.nd, ax=ax[1][3])\nfor i in range(2):\n    for j in range(3):\n        ax[i][j+1].set_ylabel(\"\")\nplt.show()\n\n\n\n\n\nWe’re starting off with 458k samples, but a lot of these won’t be useful for us. I’m choosing to focus on typical small molecule crystal structures, and attempting to generate realistic looking laboratory diffraction data.\nFrom what we’ve seen so far, we can see that some unit cells are listed as having zero volume, and have a value of 0 for the unit cell lengths and angles. Our goal of modifying unit cells and using them to generate the diffraction data means that these entries are not useful to us, so we’ll need to remove them. We can also see that there are some monstrous unit cells in there, with volumes of &gt;700,000 Å³!\nTo make things manageable for future machine learning projects, we’re going to set limits on the allowable unit cell dimensions and volumes. Let’s set the lower and upper limits for volume to 400 and 4000 Å³ respectively, and set the maximum unit cell length to 50 Å. We can also see some unusual looking unit cell angles, so I’m going to trim entries with cell angles below 60 and above 120 degrees (keeping 120 degrees is important to ensure we keep hexagonal unit cells).\nEventually, we’re going to generate diffraction data. I’ve chosen to focus on realistic looking laboratory diffraction data, and hence I’m choosing to work in the (arbirarily chosen!) range 4 - 44 ° \\(2\\theta\\) with Cu K-α1 radiation. We’ll want to ensure that there are at least a few easily detectable peaks within this chosen range. I’ll set the lower bound for this as 10.\n\nmin_volume = 400\nmax_volume = 4000\nmax_cell_length = 50\nmin_cell_angle = 60\nmax_cell_angle = 120\nmin_data_angle = 4\nmax_data_angle = 44\nwavelength = 1.54056\nmin_detectable_peaks_in_range = 10\n\n\nUnit cell dimensions and volume\nLet’s first filter by the unit cell restrictions on volume, lengths and angles. We can also filter by the number of d-spacings (in the column nd) though we will need to come back to this again later to check that the remaining reflections are both in the range desired and also have reasonable intensities.\n\n\nShow the code\ncombined = combined[(combined.volume &lt;= max_volume) & (combined.volume &gt;= min_volume)]\ncombined = combined[(combined.a != 0) & (combined.b != 0) & (combined.c != 0)]\ncombined = combined[(combined.a &lt;= max_cell_length) & (combined.b &lt;= max_cell_length) & (combined.c &lt;= max_cell_length)]\ncombined = combined[(combined.alpha &gt;= min_cell_angle) & (combined.alpha &lt;= max_cell_angle)]\ncombined = combined[(combined.beta &gt;= min_cell_angle) & (combined.beta &lt;= max_cell_angle)]\ncombined = combined[(combined.gamma &gt;= min_cell_angle) & (combined.gamma &lt;= max_cell_angle)]\ncombined = combined[(combined.nd &gt;= min_detectable_peaks_in_range)]\ncombined.describe()\n\n\n\n\n\n\n\n\n\na\nb\nc\nalpha\nbeta\ngamma\nvolume\nnd\nid\n\n\n\n\ncount\n324333.000000\n324333.000000\n324333.000000\n324333.000000\n324333.000000\n324333.000000\n324333.000000\n324333.000000\n3.243330e+05\n\n\nmean\n10.992746\n12.400612\n15.034673\n89.719327\n94.956107\n90.499324\n1849.638022\n477.448866\n4.549946e+06\n\n\nstd\n4.372923\n4.840500\n5.626536\n7.474114\n9.913211\n9.476333\n931.743127\n81.543710\n2.265363e+06\n\n\nmin\n2.808500\n2.820000\n2.718100\n60.019000\n60.130000\n60.000000\n400.059000\n16.000000\n1.000000e+06\n\n\n25%\n8.236400\n9.237500\n11.247500\n90.000000\n90.000000\n90.000000\n1085.857000\n498.000000\n2.219163e+06\n\n\n50%\n10.114500\n11.585500\n14.365300\n90.000000\n93.216000\n90.000000\n1699.288000\n499.000000\n4.129720e+06\n\n\n75%\n12.609000\n14.506900\n17.908600\n90.000000\n101.601000\n90.000000\n2515.215000\n500.000000\n7.047435e+06\n\n\nmax\n49.953000\n49.977000\n49.944000\n119.990000\n120.000000\n120.000000\n3999.991000\n500.000000\n9.016729e+06\n\n\n\n\n\n\n\nThat’s a fairly significant reduction in the number of samples, down by over 100k! However, hopefully we’ve gotten rid of most of the really odd or unusable samples in the dataset.\nLet’s now replot the kernel density plots we saw earlier and see how things look after the filtering.\n\n\nShow the code\nfig, ax = plt.subplots(2, 4, figsize=(18,7))\nsns.kdeplot(combined.a, ax=ax[0][0])\nsns.kdeplot(combined.b, ax=ax[0][1])\nsns.kdeplot(combined.c, ax=ax[0][2])\nsns.kdeplot(combined.volume, ax=ax[0][3])\nsns.kdeplot(combined.alpha, ax=ax[1][0])\nsns.kdeplot(combined.beta, ax=ax[1][1])\nsns.kdeplot(combined.gamma, ax=ax[1][2])\nsns.kdeplot(combined.nd, ax=ax[1][3])\nplt.show()\n\n\n\n\n\nThings look a bit nicer now! Just out of interest, we’ll take a quick look at the distribution of crystal systems and space groups. Other work looking at the CSD and other databases has shown a significant imbalance in the number of crystal structures adopting each crystal system and indeed space group.\n\n\nShow the code\ncombined.type.value_counts().sort_values().iloc[::-1].head(20).plot.bar()\nplt.show()\n\n\n\n\n\n\n\nShow the code\ncombined.spacegroup_x.value_counts().sort_values().iloc[::-1].head(20).plot.bar()\nplt.show()\n\n\n\n\n\nUnsurprisingly, we see that some space groups are much more highly represented than others. If we decide to try crystal system, space group or extinction symbol classification using this data, we will need to keep this fairly severe class imbalance in mind!\nFor convenience for future work, it’d be good to convert these labels into their international tables numbers. Let’s write a quick function to do that, getting rid of any entries that have unknown space groups, or space group labels that are otherwise difficult to interpret.\n\n\nShow the code\nfrom pymatgen.symmetry import groups\nimport numpy as np\n\nsymbol_to_num = {}\nfor i, s in enumerate(np.unique(combined.spacegroup_x.to_numpy())):\n    try:\n        g = groups.SpaceGroup(s)\n        symbol_to_num[s] = g.int_number\n    except:\n        symbol_to_num[s] = -1\n\ndef label_SGs(sg):\n    return symbol_to_num[sg]\n\ncombined[\"sg_number\"] = combined.spacegroup_x.apply(label_SGs)\ncombined = combined[combined.sg_number &gt; 0]\ncombined.sg_number.describe()\n\n\ncount    323694.000000\nmean         24.984507\nstd          44.618451\nmin           1.000000\n25%           2.000000\n50%          14.000000\n75%          15.000000\nmax         230.000000\nName: sg_number, dtype: float64\n\n\nWe lost &lt;1000 entries in that step, which is good news!\n\n\nDiffraction data\nWe can now turn our attention to the diffraction data - the intensities, Miller indices and d-spacings. Some of the entries will have a huge number of peaks, some will have very few. To start with, I’m choosing to exclude samples that have reflections below the bottom of our data range. I also want to ensure that there are still a few peaks above the top end of our data range which ensures that we aren’t missing any data inside our chosen data range.\nOur dataframe contains the d-spacings for the observed reflections, so using Bragg’s law we can easily convert the angular range bounds into d-spacings, and then apply the filtering just described.\n\n\nShow the code\nimport numpy as np\n\nd_from_tt = lambda x: wavelength/(2*np.sin(np.deg2rad(x)/2))\ntt_from_d = lambda x: 2*np.rad2deg(np.arcsin(wavelength/(2*x)))\nmax_d = d_from_tt(min_data_angle)\nmin_d = d_from_tt(max_data_angle)\nprint(\"Wavelength:\", wavelength)\nprint(f\"Minimum angle = {min_data_angle}  : Maximum d-spacing = {max_d:.3f}\")\nprint(f\"Maximum angle = {max_data_angle} : Minimum d-spacing = {min_d:.3f}\")\n\n\nWavelength: 1.54056\nMinimum angle = 4  : Maximum d-spacing = 22.071\nMaximum angle = 44 : Minimum d-spacing = 2.056\n\n\n\n\nShow the code\ndef d_spacing_limits(x, min_d_spacing=1, max_d_spacing=100):\n    d = list(filter(None, x.strip().split(\",\")))\n    d = [float(i) for i in d]\n    if len(d) == 0:\n        return False\n    else:\n        if d[-1] &gt; min_d_spacing:\n            return False\n        if d[0] &gt; max_d_spacing:\n            return False\n        return True\n\ncombined[\"d_spacing_filter\"] = combined.dvalue.apply(d_spacing_limits, args=(min_d, max_d))\n\n\n\n\nShow the code\ncombined = combined[combined[\"d_spacing_filter\"] == True]\ncombined.describe()\n\n\n\n\n\n\n\n\n\na\nb\nc\nalpha\nbeta\ngamma\nvolume\nnd\nid\nsg_number\n\n\n\n\ncount\n294047.000000\n294047.000000\n294047.000000\n294047.000000\n294047.000000\n294047.000000\n294047.000000\n294047.000000\n2.940470e+05\n294047.000000\n\n\nmean\n10.880480\n12.250730\n14.660952\n89.810688\n95.496771\n90.697710\n1761.181410\n475.436522\n4.502841e+06\n27.194466\n\n\nstd\n4.407073\n4.930788\n5.572797\n6.735834\n9.633053\n8.870661\n911.606182\n85.121247\n2.291774e+06\n46.176416\n\n\nmin\n2.808500\n2.825500\n2.718100\n60.019000\n60.130000\n60.000000\n400.059000\n16.000000\n1.000000e+06\n1.000000\n\n\n25%\n8.104550\n9.061050\n10.951000\n90.000000\n90.000000\n90.000000\n1033.725000\n498.000000\n2.214880e+06\n4.000000\n\n\n50%\n9.900700\n11.252000\n13.876500\n90.000000\n93.612000\n90.000000\n1587.395000\n499.000000\n4.122961e+06\n14.000000\n\n\n75%\n12.467950\n14.239950\n17.401100\n90.000000\n101.996000\n90.000000\n2354.773000\n500.000000\n7.047926e+06\n19.000000\n\n\nmax\n45.366000\n48.809000\n49.494500\n119.990000\n120.000000\n120.000000\n3999.991000\n500.000000\n9.016729e+06\n230.000000\n\n\n\n\n\n\n\nWe are down to just shy of 300k samples now, with one last bit of filtering to apply. Lastly, we’ll ensure that we have sufficient easily detectable peaks within our data range. Do to this, we need to look at the intensities:\n\n\nShow the code\ncombined.intensita\n\n\n11        1000.000000,293.781700,0.932900,880.099700,350...\n12        344.517500,136.663800,113.543400,41.743900,4.9...\n13        480.841100,1000.000000,345.837700,13.887000,72...\n14        6.550100,7.094000,99.801200,1000.000000,642.20...\n17        2.707700,223.647400,1000.000000,119.345700,113...\n                                ...                        \n458148    191.292600,19.724900,41.216500,169.019100,150....\n458151    1000.000000,782.379000,202.061600,29.027900,13...\n458155    1000.000000,62.638200,82.940500,3.929300,0.067...\n458158    435.793700,1000.000000,156.773800,74.174400,1....\n458159    884.735400,207.638600,175.487800,594.721200,42...\nName: intensita, Length: 294047, dtype: object\n\n\nIntensities are stored as a string, which has comma separated values. Each pattern is scaled to give a maximum intensity of 1000. We should ensure that in our chosen data range, the intensities that are present are actually strong enough to be observed! Let’s say we want the minimum intensity in our data range to be 0.5 % of the maximum intensity. As such, we should have at least \\(N\\) intensities in our range with a value greater than 5. As discussed earlier, I’ve chosen to set \\(N = 10\\).\n\n\nShow the code\ndef n_peaks_in_range(dspacing, intensity, min_intensity=5):\n    dspacing = list(filter(None, dspacing.strip().split(\",\")))\n    dspacing = np.array([float(i) for i in dspacing])\n    intensity = list(filter(None, intensity.strip().split(\",\")))\n    intensity = np.array([float(i) for i in intensity])\n    intensity = intensity[dspacing &gt;= min_d]\n    return (intensity &gt; min_intensity).sum()\n\ncombined[\"n_detectable_peaks_in_range\"] = combined.apply(lambda x: n_peaks_in_range(x.dvalue, x.intensita), axis=1)\ncombined[\"n_detectable_peaks_in_range\"].describe()\n\n\ncount    294047.000000\nmean        113.481430\nstd          59.305561\nmin           0.000000\n25%          72.000000\n50%         107.000000\n75%         149.000000\nmax         386.000000\nName: n_detectable_peaks_in_range, dtype: float64\n\n\nAs we can see, some of the patterns we might generate would have very few or no easily detectable peaks in our chosen data range. As our final filtering step, we’ll get rid of these entries which should give us our final filtered dataset.\n\n\nShow the code\ncombined = combined[combined.n_detectable_peaks_in_range &gt;= min_detectable_peaks_in_range]\ncombined.describe()\n\n\n\n\n\n\n\n\n\na\nb\nc\nalpha\nbeta\ngamma\nvolume\nnd\nid\nsg_number\nn_detectable_peaks_in_range\n\n\n\n\ncount\n288123.000000\n288123.000000\n288123.000000\n288123.000000\n288123.000000\n288123.000000\n288123.000000\n288123.000000\n2.881230e+05\n288123.000000\n288123.000000\n\n\nmean\n10.915994\n12.314144\n14.769507\n89.806797\n95.609645\n90.698470\n1780.427861\n484.062508\n4.493060e+06\n23.272384\n115.697018\n\n\nstd\n4.439603\n4.955456\n5.568620\n6.804673\n9.698929\n8.939179\n909.067811\n60.494039\n2.259233e+06\n37.387276\n57.842270\n\n\nmin\n2.808500\n2.825500\n2.760000\n60.019000\n60.130000\n60.000000\n400.059000\n30.000000\n1.000000e+06\n1.000000\n10.000000\n\n\n25%\n8.099250\n9.133100\n11.122850\n90.000000\n90.000000\n90.000000\n1054.453000\n498.000000\n2.216278e+06\n4.000000\n75.000000\n\n\n50%\n9.925000\n11.332000\n13.984900\n90.000000\n93.924000\n90.000000\n1608.534000\n499.000000\n4.122969e+06\n14.000000\n108.000000\n\n\n75%\n12.542100\n14.337000\n17.492400\n90.000000\n102.190000\n90.000000\n2378.520500\n500.000000\n7.045938e+06\n15.000000\n151.000000\n\n\nmax\n45.366000\n48.809000\n49.494500\n119.990000\n120.000000\n120.000000\n3999.991000\n500.000000\n9.016729e+06\n230.000000\n386.000000\n\n\n\n\n\n\n\n\n\nSaving the filtered data\nNow that we’ve filtered our data to our requirements, the last thing to do is save the relevant information in numpy arrays which can then be quickly and easily loaded from disk when needed.\nThe arrays to be saved will contain:\n\nUnit cells\nCrystal systems\nMiller indices\nPeak intensities\nSpace groups\nCOD IDs in case we want to look up a particular crystal structure\n\nAs the number of peaks in each pattern is different, the Miller indices and Peak intensity arrays will need padding up to the maximum number, which is 500.\n\n\nShow the code\ndef get_array(x, dtype=np.float32):\n    data = np.zeros(500, dtype=dtype)\n    x = list(filter(None, x.strip().split(\",\")))\n    x = np.array(x, dtype=dtype)\n    data[:len(x)] = x\n    return data\n\nh = combined.h.apply(get_array, args=(np.int64,))\nk = combined.k.apply(get_array, args=(np.int64,))\nl = combined.l.apply(get_array, args=(np.int64,))\nintensities = combined.intensita.apply(get_array, args=(np.float32,))\n\nintensities = np.vstack(intensities)\nh = np.vstack(h)\nk = np.vstack(k)\nl = np.vstack(l)\nhkl = np.dstack([h,k,l])\n\nunit_cell = combined[[\"a\",\"b\",\"c\",\"alpha\",\"beta\",\"gamma\"]].to_numpy()\ncrystal_systems, crystal_systems_numeric = np.unique(combined.type.to_numpy(), return_inverse=True)\nspace_group_number = combined.sg_number.to_numpy()\ncod_id = combined.id.to_numpy()\nprint(crystal_systems)\n\n\nNow we can save everything, as well as a configuration file to keep track of our settings, which we’ll save using the json format.\n\n\nShow the code\nimport json\n\nbase_name = f\"{min_data_angle}-{max_data_angle}-CuKa1-data_{max_volume}_\"\nconfig = {}\nconfig[\"base_name\"] = base_name\nconfig[\"min_volume\"] = min_volume\nconfig[\"max_volume\"] = max_volume\nconfig[\"max_cell_length\"] = max_cell_length\nconfig[\"min_cell_angle\"] = min_cell_angle\nconfig[\"max_cell_angle\"] = max_cell_angle\nconfig[\"min_data_angle\"] = min_data_angle\nconfig[\"max_data_angle\"] = max_data_angle\nconfig[\"wavelength\"] = wavelength\nconfig[\"min_detectable_peaks_in_range\"] = min_detectable_peaks_in_range\nconfig[\"crystal_systems\"] = [x[0]+\" = \"+str(x[1]) for x in zip(crystal_systems, np.arange(len(crystal_systems)))]\n\nwith open(\"data_config.json\", \"w\") as f:\n    json.dump(config, f, indent=4)\nnp.save(base_name+\"unit_cell.npy\", unit_cell)\nnp.save(base_name+\"crystal_systems_numeric.npy\", crystal_systems_numeric)\nnp.save(base_name+\"space_group_number.npy\", space_group_number)\nnp.save(base_name+\"hkl.npy\", hkl)\nnp.save(base_name+\"intensities.npy\", intensities)\nnp.save(base_name+\"cod_id.npy\", cod_id)"
  },
  {
    "objectID": "posts/2023-09-08-Generating-synthetic-PXRD-data.html#peak-positions",
    "href": "posts/2023-09-08-Generating-synthetic-PXRD-data.html#peak-positions",
    "title": "Generating synthetic diffraction data from PowCod database",
    "section": "Peak positions",
    "text": "Peak positions\nRather than using the d-spacings from the database directly, we are going to make use of the unit cells and Miller indices to calculate the d-spacings (and hence angles) for the reflections. This will allow us to add a small amount of noise to the unit cell dimensions, which serves as a form of data augmentation.\nThe d-spacing of a given reflection can be calculated using this equation:\n\\(\\frac{1}{d^{2}} = X_{hh}h^2 + X_{kk}k^2 + X_{ll}l^2 + X_{hk}hk +X_{hl}hl + X_{kl}kl\\)\nwhere \\(X_{nn}\\) are elements of the reciprocal space metric tensor.\nThis means that we will need a function to convert the unit cell dimensions into a matrix representation of the unit cell, invert this to determine the reciprocal lattice matrix and from this obtain the reciprocal lattice metric tensor. Once we have the reciprocal lattice metric tensor, we can calculate the d-spacings quite easily by multiplying with the Miller index array(s). However, one complication is that we want to do this for several samples simultaneously, so need to consider the shapes of our arrays.\nBefore dealing with that, let’s write a function to simulate the effect of temperature changes, by adding a small amount of Gaussian noise to the unit cell dimensions. We will have to consider the crystal system symmetry here in order to ensure that, for example a cubic unit cell, the perturbation to lengths \\(b\\) and \\(c\\) is equal to the pertubation to \\(a\\). The standard deviation of the random noise to be added is going to be set to a default value of 0.05, meaning that the majority of perturbed unit cells should have edges with lengths +/- 0.15 Å from the database values, and angles +/- 0.15 ° (assuming they are allowed to vary by symmetry). This seems reasonable, and should allow us to assume that the fractional coordinates within the unit cells are unaffected, so we don’t need to worry about the effect on the relative intensities.\n\n\nShow the code\nimport torch\n\ndef get_unit_cell_perturbation(crystal_systems, dtype=torch.float32, stddev=0.05):\n    \"\"\"Generate a perturbation for the unit cell lengths and angles for a given\n    crystal system.\n\n    Args:\n        crystal_systems (tensor):   Crystal systems for the unit cells.\n                                    Shape = (batch)\n        dtype (torch.dtype, optional): Defaults to torch.float32.\n        stddev (float, optional):   Standard deviation for the gaussian noise.\n                                    Defaults to 0.05.\n\n    Returns:\n        tensor: A tensors to additively adjust the unit cell lengths and angles\n                Shape = (batch, 6)\n    \"\"\"\n    batchsize = crystal_systems.shape[0]\n    device = crystal_systems.device\n    lengths, angles = torch.randn((2, batchsize, 3), device=device, dtype=dtype) * stddev\n    cubic = crystal_systems == 0\n    hexagonal = crystal_systems == 1\n    monoclinic = crystal_systems == 2\n    #orthorhombic = crystal_systems == 3 # Don't need to query data for this\n    tetragonal = crystal_systems == 4\n    triclinic = crystal_systems == 5\n    trigonal_h = crystal_systems == 6\n    trigonal_r = crystal_systems == 7\n\n    # Cubic, tetragonal, rhombohedral and hexagonal - a and b must be the same\n    lengths[:,1] = torch.where(cubic | hexagonal | tetragonal | trigonal_h | trigonal_r,\n                            lengths[:,0], lengths[:,1])\n    # Cubic and rhombohedral cells - a, b and c must be the same\n    lengths[:,2] = torch.where(cubic | trigonal_r, lengths[:,0], lengths[:,2])\n    # Rhombohedral and triclinic cells - could change their alpha values\n    angles[:,0] = torch.where((trigonal_r | triclinic), angles[:,0], 0.)\n    # Triclinic or monoclinic cells could change beta values\n    angles[:,1] = torch.where((triclinic | monoclinic), angles[:,1], 0.)\n    # Triclinc cells could change gamma\n    angles[:,2] = torch.where(triclinic, angles[:,2], 0.)\n    # Rhombohedral cells - need to ensure all angles are the same\n    angles[:,1] = torch.where(trigonal_r, angles[:,0], angles[:,1])\n    angles[:,2] = torch.where(trigonal_r, angles[:,0], angles[:,2])\n\n    return torch.concat([lengths, angles], dim=-1)\n\n\nLet’s check it does what it’s meant to!\nWhat we’re expecting is that for the unit cell lengths, the perturbation should be the same for each of the edges for cubic and rhombohedral cells, and the perturbation for \\(a\\) and \\(b\\) should be the same for hexagonal and tetragonal cells.\nWith regards to angles, where the original unit cell is an angle of 90 or 120 degrees, the perturbation should be zero. So we should see a change in the beta value for monoclinic cells, a change for all values of the triclinic unit cells and the same change for each of the angles for rhombohedral cells.\n\n\nShow the code\ncs_key =  {\n    \"Cubic\" : 0,\n    \"Hexagonal\" :  1,\n    \"Monoclinic\" : 2,\n    \"Orthorhombic\" : 3,\n    \"Tetragonal\" : 4,\n    \"Triclinic\" : 5,\n    \"Trigonal (hexagonal axes)\" : 6,\n    \"Trigonal (rhombohedral axes)\" : 7\n}\n\nfor c in cs_key.items():\n    name, number = c\n    perturb = get_unit_cell_perturbation(torch.tensor([[int(number)]]))\n    lengths = perturb[:,:3]\n    angles = perturb[:,3:]\n    print(name, \"Length perturbation:\", lengths)\n    print(name, \"Angle perturbation:\", angles,\"\\n\")\n\n\nCubic Length perturbation: tensor([[-0.0266, -0.0266, -0.0266]])\nCubic Angle perturbation: tensor([[0., 0., 0.]]) \n\nHexagonal Length perturbation: tensor([[-0.0088, -0.0088,  0.0375]])\nHexagonal Angle perturbation: tensor([[0., 0., 0.]]) \n\nMonoclinic Length perturbation: tensor([[ 0.0049, -0.0555,  0.0063]])\nMonoclinic Angle perturbation: tensor([[0.0000, 0.0258, 0.0000]]) \n\nOrthorhombic Length perturbation: tensor([[-0.0652, -0.0148, -0.0104]])\nOrthorhombic Angle perturbation: tensor([[0., 0., 0.]]) \n\nTetragonal Length perturbation: tensor([[-0.0055, -0.0055,  0.0428]])\nTetragonal Angle perturbation: tensor([[0., 0., 0.]]) \n\nTriclinic Length perturbation: tensor([[-0.0521, -0.0112,  0.0643]])\nTriclinic Angle perturbation: tensor([[ 0.0202, -0.0083,  0.0039]]) \n\nTrigonal (hexagonal axes) Length perturbation: tensor([[0.0370, 0.0370, 0.0156]])\nTrigonal (hexagonal axes) Angle perturbation: tensor([[0., 0., 0.]]) \n\nTrigonal (rhombohedral axes) Length perturbation: tensor([[-0.0299, -0.0299, -0.0299]])\nTrigonal (rhombohedral axes) Angle perturbation: tensor([[0.0540, 0.0540, 0.0540]]) \n\n\n\nLooks good to me. We now need to start writing functions to generate our lattice matrix, reciprocal lattice matrix and reciprocal lattice metric tensor. We can then use these in conjunction with the Miller indices to determine the reflection d-spacings.\nGiven that we are modifying unit cells, it’s probably sensible to check that the perturbed unit cells are valid. We can do that by calculating the volume of the unit cells, with invalid unit cells returning a volume of zero.\n\n\nShow the code\ndef get_lattice_matrix(unit_cell_dimensions):\n    \"\"\"calculate a lattice matrix from unit cell dimensions\n\n    Args:\n        unit_cell_dimensions (tensor):  The unit cell dimensions. Lengths in\n                                        angstroms, angles in degrees.\n                                        Shape = (batch, 6)\n\n    Returns:\n        tensor: The matrix representation of the unit cells.\n                Shape = (batch, 3, 3)\n    \"\"\"\n    pi_over_180=0.017453292519943295\n    a, b, c = unit_cell_dimensions[:,:3].T\n    cosal, cosbe, cosga = torch.cos(unit_cell_dimensions[:,3:]*pi_over_180).T\n    sinal, sinbe = torch.sin(unit_cell_dimensions[:,3:-1]*pi_over_180).T\n    # Sometimes rounding errors cause |values| slightly &gt; 1.\n    val = torch.clamp((cosal * cosbe - cosga) / (sinal * sinbe), min=-1, max=1)\n\n    gamma_star = torch.arccos(val)\n    zeros = torch.zeros_like(a)\n    v_a = torch.stack([a * sinbe, zeros, a*cosbe]).T\n    v_b = torch.stack([-b * sinal * torch.cos(gamma_star),\n                    b*sinal * torch.sin(gamma_star),\n                    b*cosal]).T\n    v_c = torch.stack([zeros, zeros, c]).T\n\n    matrix = torch.stack([v_a,v_b,v_c], dim=2)\n\n    # Unit cells are valid if cell volume &gt; 0\n    # The cell volume is |det(M)|, but don't need the absolute value here\n    volume = torch.linalg.det(matrix)\n    valid = volume != 0\n\n    return matrix, valid\n\n\nLet’s quickly check that this works as expected. I’ll give it a simple cubic unit cell, and an invalid unit cell.\n\n\nShow the code\ncubic_cell = torch.tensor([5.0, 5.0, 5.0, 90.0, 90.0, 90.0]).unsqueeze(0)\ninvalid_cell = torch.tensor([5.0, 6.0, 7.0, 120.0, 120.0, 120.0]).unsqueeze(0)\nprint(\"Cubic cell valid -\",get_lattice_matrix(cubic_cell)[1].item())\nprint(\"Invalid cell valid -\", get_lattice_matrix(invalid_cell)[1].item())\n\n\nCubic cell valid - True\nInvalid cell valid - False\n\n\nThe last couple of functions we need to implement then include a way to get the reciprocal lattice metric tensor. The reciprocal lattice matrix can be easily obtained by inverting the lattice matrix we calculated previously.\nWe also need a way to obtain the reflection d-spacings, and then lastly convert the d-spacings to \\(2\\theta\\).\n\n\nShow the code\ndef get_recip_lattice_metric_tensor(recip_lattice_matrix):\n    \"\"\"Calculate the reciprocal lattice metric tensor\n\n    Args:\n        recip_lattice_matrix (tensor):  Reciprocal lattice matrix\n                                        Shape = (batch, 3, 3)\n\n    Returns:\n        tensor: Reciprocal lattice metric tensor\n                Shape = (batch, 3, 3)\n    \"\"\"\n    return recip_lattice_matrix @ recip_lattice_matrix.permute(0,2,1)\n\n\ndef get_d_spacing(recip_latt_metric_tensor,hkl):\n    \"\"\"Calculate the d-spacings for the reflections from the Miller indices and\n    the reciprocal lattice metric tensor\n\n    Args:\n        recip_latt_metric_tensor (tensor):  Reciprocal lattice metric tensor\n                                            Shape = (batch, 3, 3)\n        hkl (tensor):   Miller indices\n                        Shape = (batch, number of reflections, 3)\n\n    Returns:\n        tensor: d-spacing for each of the reflections\n                Shape = (batch, number of reflections)\n    \"\"\"\n    one_over_d_squared = torch.einsum(\"bij,bji-&gt;bi\",hkl,torch.einsum(\n                                \"bij,bkj-&gt;bik\",recip_latt_metric_tensor,hkl))\n    d = 1/torch.sqrt(one_over_d_squared)\n    return d\n\n\ndef d_to_tt(d,wavelength=1.54056):\n    \"\"\"Convert d-spacings to twotheta values (in degrees)\n\n    Args:\n        d (tensor): d-spacings for each of the reflections\n                    Shape = (batch, number of reflections)\n        wavelength (float): The wavelength of the radiation. Defaults to\n                            1.54056 which is copper K-a1\n\n    Returns:\n        tensor: twotheta values for each of the reflections\n                Shape = (batch, number of reflections)\n    \"\"\"\n    two_times_180_over_pi = 114.59155902616465\n    tt = two_times_180_over_pi*torch.arcsin(wavelength/(2*d))\n    return tt\n\ndef get_zero_point_error(batchsize, device, dtype, zpemin=0.03, zpemax=0.03):\n    \"\"\"\n    Generate a random zero-point error to be applied to the peak positions\n\n    Args:\n        batchsize (int): Size of the batch dimension\n        device (torch.device): Device to generate the tensor on\n        dtype (torch.dtype): dtype to use for the ZPEs\n        zpemin (float, optional): Lower bound for zero point error in degrees\n        zpemax (float, optional): Upper bound for zero point error in degrees\n\n    Returns:\n        tensor: Zero point error to be applied to the peak positions\n                Shape = (batch, 1)\n    \"\"\"\n    zero_point_error = (torch.rand((batchsize,1), device=device, dtype=dtype)\n                        * (zpemax - zpemin)) + zpemin\n    return zero_point_error\n\n\nTo test the whole set of functions written so far, I’ll add a convenience function to calculate the peak positions from the unit cells and Miller indices.\n\n\nShow the code\ndef get_peak_positions(unit_cells, hkl, zpe=True, wavelength=1.54056):\n    lattice_matrix, valid = get_lattice_matrix(unit_cells)\n    unit_cells = unit_cells[valid]\n    hkl = hkl[valid]\n    reciprocal_lattice_matrix = torch.linalg.inv(lattice_matrix)\n    reciprocal_lattice_metric_tensor = get_recip_lattice_metric_tensor(reciprocal_lattice_matrix)\n    d_spacing = get_d_spacing(reciprocal_lattice_metric_tensor, hkl)\n    twotheta = d_to_tt(d_spacing, wavelength)\n    if zpe:\n        zero_point_error = get_zero_point_error(twotheta.shape[0],\n                                                twotheta.device,\n                                                twotheta.dtype)\n        twotheta += zero_point_error\n    return twotheta\n\n\nI’ll test that using some diffraction data for a Carbamazepine:Indomethacin cocrystal I published a while ago. The first 3 peaks have the following Miller indices and positions:\n\n\n\n\\(h\\)\n\\(k\\)\n\\(l\\)\n\\(2\\theta\\)\n\n\n\n\n1\n1\n0\n5.322\n\n\n2\n0\n0\n7.496\n\n\n0\n2\n0\n7.562\n\n\n\nAnd the unit cell is:\n\\(a\\) = 23.573194, \\(b\\) = 23.363375, \\(c\\) = 5.125218\n\\(\\alpha\\) = 90.000, \\(\\beta\\) = 88.77132, \\(\\gamma\\) = 90.000\nWe’ll run the function both with and without zero point errors applied.\n\n\nShow the code\ncoXtal_unit_cell = torch.tensor([23.573194, 23.363375, 5.125218, 90.000, 88.77132, 90.000])\ncoXtal_hkl = torch.tensor([[1,1,0],[2,0,0],[0,2,0]]).float()\n\nfor zpe in [False, True]:\n    print(\"ZPE:\",zpe)\n    coXtal_twotheta = get_peak_positions(coXtal_unit_cell.reshape(1,6), coXtal_hkl.reshape(1,3,3), zpe=zpe)\n    print(\"[h, k, l]  2θ\")\n    print(\"----------------\")\n    for i, ht in enumerate(zip(coXtal_hkl, coXtal_twotheta.squeeze())):\n        print(ht[0].int().tolist(), f\"{ht[1].item():.3f}\")\n    print(\"\")\n\n\nZPE: False\n[h, k, l]  2θ\n----------------\n[1, 1, 0] 5.322\n[2, 0, 0] 7.496\n[0, 2, 0] 7.562\n\nZPE: True\n[h, k, l]  2θ\n----------------\n[1, 1, 0] 5.352\n[2, 0, 0] 7.526\n[0, 2, 0] 7.592\n\n\n\nThat looks like it’s working nicely!"
  },
  {
    "objectID": "posts/2023-09-08-Generating-synthetic-PXRD-data.html#peak-intensities",
    "href": "posts/2023-09-08-Generating-synthetic-PXRD-data.html#peak-intensities",
    "title": "Generating synthetic diffraction data from PowCod database",
    "section": "Peak intensities",
    "text": "Peak intensities\nThe main contribution to the intensity of a diffraction peak is the position of the atoms within the unit cell. What makes the PowCod database so helpful is that this has already been calculated, and can be read in from the intensities array we saved earlier.\nHowever, there are experimental/physical effects that can modulate these intensities. Most import is the effect of preferred orientation. There are several ways to model this, I’m choosing to implement the March-Dollase method as I already have some code written to do this in GALLOP. This code is actually based on code from GSAS-II; see the GetPrefOri function in the file found here. I updated it to use pytorch, and work over a batch of multiple PXRD patterns.\nAgain, this is being implemented as a form of data augmentation, allowing our existing data to generate multiple different, but plausible, diffraction patterns.\nFuture work could extend this to use the more sophisticated spherical harmonic approach described here.\n\n\nShow the code\ndef get_MD_PO_components(hkl, reciprocal_lattice_metric_tensor, dspacing, factor_std=0.1):\n    \"\"\"Calculate the terms needed in the March-Dollase preferred orientation\n    correction.\n\n    Args:\n        hkl (tensor):   The Miller indices of the reflections.\n                        Shape = (batch, number of reflections, 3)\n        reciprocal_lattice_matrix (tensor): The reciprocal lattice matrix\n                                            Shape = (batch, 3, 3)\n        factor_std (float, optional):   The standard deviation for the normally\n                                        distributed March-Dollase factors.\n                                        Defaults to 0.1.\n\n    Returns:\n        tuple: Tuple of tensors containing the terms needed in the March-Dollase\n        preferred orientation correction function. cosP, sinP = cosine and sine\n        of the angle between the Miller indices and the PO axis. Factor = the\n        March Dollase factors, PO_axis = the preferred orientation axis.\n    \"\"\"\n    batchsize = hkl.shape[0]\n    device = hkl.device\n    dtype = hkl.dtype\n    # Randomly assign the PO axis to be either [1,0,0], [0,1,0] or [0,0,1]\n    PO_axis = torch.zeros((batchsize, 3), device=device, dtype=dtype)\n    PO_axis_select = torch.randint(0,3,(batchsize,),device=device)\n    PO_axis[torch.arange(batchsize, device=device),PO_axis_select] = 1.0\n\n\n    \"\"\"u = hkl / torch.sqrt(torch.einsum(\"bkj,bkj-&gt;bk\", hkl,\n                        torch.einsum(\"bij,bkj-&gt;bki\",\n                        reciprocal_lattice_matrix,hkl))).unsqueeze(2)\"\"\"\n    # Dividing the Miller indices by the reciprocal lattice vector lengths is\n    # equivalent to multiplying them by the d-spacings\n    u = hkl * dspacing.unsqueeze(2)\n\n    cosP = torch.einsum(\"bij,bj-&gt;bi\", u, torch.einsum(\"bij,bj-&gt;bi\",\n                    reciprocal_lattice_metric_tensor, PO_axis))\n    one_minus_cosP_sqd = 1.0-cosP**2\n    one_minus_cosP_sqd[one_minus_cosP_sqd &lt; 0.] *= 0.\n    sinP = torch.sqrt(one_minus_cosP_sqd)\n\n    # MD factor = 1 means no PO. Use a normal distribution with std given in the\n    # argument centred at 1.\n    factor = 1 + (torch.randn((batchsize,1),device=device,dtype=dtype) * factor_std)\n\n    return cosP, sinP, factor, PO_axis\n\ndef apply_MD_PO_correction(intensities, cosP, sinP, factor):\n    \"\"\"Modifies the intensities to account for preferred orientation effects\n    using the method of March and Dollase.\n\n    Args:\n        intensities (tensor):   Original intensities for the reflections.\n            Shape = (batch, number of reflections)\n        cosP (tensor):  Cosine of the angle between the Miller indices and the\n            preferred orientation axis. Calculated in get_MD_PO_components\n            Shape = (batch, number of reflections)\n        sinP (tensor):  Sine of the angle between the Miller indices and the\n            preferred orientation axis. Calculated in get_MD_PO_components\n            Shape = (batch, number of reflections)\n        factor (tensor): The March-Dollase factors. Shape = (batch,1)\n\n    Returns:\n        tensor: Modified intensities for the reflections given preferred\n        orientation. Shape = (batch, number of reflections)\n    \"\"\"\n    A_all = (1.0/torch.sqrt(((factor)*cosP)**2+sinP**2/(factor)))**3\n    return intensities * A_all"
  },
  {
    "objectID": "posts/2023-09-08-Generating-synthetic-PXRD-data.html#peak-shapes",
    "href": "posts/2023-09-08-Generating-synthetic-PXRD-data.html#peak-shapes",
    "title": "Generating synthetic diffraction data from PowCod database",
    "section": "Peak shapes",
    "text": "Peak shapes\nNow that we can calculate where the peaks will be, and modulate the intensity of the peaks if we want to, we now need to think about the shape of the peaks.\nThere are several possible peak shapes that we could go for, however, in order to try to get relatively realistic results, I decided to implement full Voigt functions, and also include the Finger, Cox and Jephcoat correction for peak asymmetry due to axial divergence. I also decided to implement peak broadening as a function of the peak position, using the well established U,V,W,X,Y,Z parameters described here and elsewhere.\nWe therefore need to calculate gaussian, lorentzian and the FCJ profile for each peak in the data, then convolve them together. Using the convolution theorem, we can do this by first Fourier transforming them, calculating their pointwise product in Fourier space, then inverse Fourier transforming the resultant product.\n\n\nShow the code\ndef gaussian(x, mu, sig):\n    \"\"\"Calculate a gaussian peak\n\n    Args:\n        x (tensor): the x-coordinates for the peak. Shape = (datapoints)\n        mu (tensor): the mean of the gaussian.\n            Shape = (batch, number of peaks, 1)\n        sig (tensor): the standard deviation of the gaussian.\n            Shape = (batch, number of peaks, 1)\n\n    Returns:\n        tensor: the gaussian peaks centered at mu with standard deviation sig.\n            Intensities scaled to max = 1.\n            Shape = (batch, number of peaks, datapoints)\n    \"\"\"\n    root_two_pi = 2.5066282746310002\n    peak = (1/(sig*root_two_pi))*torch.exp(-0.5*(x-mu)**2/sig**2)\n    return peak/peak.max(dim=-1).values.unsqueeze(2)\n\n\ndef lorentzian(x, loc, gam):\n    \"\"\"Calcualte a lorentzian peak\n\n    Args:\n        x (tensor): the x-coordinates for the peak. Shape = (datapoints)\n        loc (tensor): The centre of the peaks.\n            Shape = (batch, number of peaks, 1)\n        gam (tensor): The half width at half max for the peaks.\n             Shape = (batch, number of peaks, 1)\n\n    Returns:\n        tensor: the gaussian peaks centered at loc with HWHM gam.\n            Intensities scaled to max = 1.\n            Shape = (batch, number of peaks, datapoints)\n    \"\"\"\n    one_over_pi = 0.3183098861837907\n    peak = one_over_pi*(gam/((x - loc)**2+gam**2))\n    return peak/peak.max(dim=-1).values.unsqueeze(2)\n\n\ndef fcj(data,twotheta,shl):\n    \"\"\"Finger, Cox and Jephcoat profile function.\n    Code based on GSAS-II function found here:\n    https://gsas-ii.readthedocs.io/en/latest/_modules/GSASIIpwd.html#fcjde_gen\n\n    Args:\n        data (tensor): the data to evaluate the function on.\n            Shape = (batch, number of peaks, datapoints)\n        twotheta (tensor): The position of the peak.\n            Shape = (batch, number of peaks, 1)\n        shl (tensor): shl = sum(S/L,H/L) where:\n            S: sample height\n            H: detector opening\n            L: sample to detector opening distance\n            Shape = (batch, 1)\n\n    Returns:\n        tensor: The Finger, Cox and Jephcoat profiles for the peaks\n            Shape = (batch, number of peaks, datapoints)\n    \"\"\"\n    pi_over_180 = 0.017453292519943295\n    step = data[1] - data[0]\n    T = step*data+twotheta\n    abs_cos_T = torch.abs(torch.cos(T*pi_over_180))\n    abs_cos_T_sqd = abs_cos_T**2\n    cos_sqd_twotheta = torch.cos(twotheta*pi_over_180)**2\n    cos_sqd_twotheta = torch.where(abs_cos_T_sqd&gt;cos_sqd_twotheta,\n                                    cos_sqd_twotheta,abs_cos_T_sqd)\n    fcj_profile = torch.where(abs_cos_T_sqd&gt;cos_sqd_twotheta,\n                    (torch.sqrt(cos_sqd_twotheta/(abs_cos_T_sqd-cos_sqd_twotheta+1e-9))\n                    -1./shl)/abs_cos_T,0.0)\n    fcj_profile = torch.where(fcj_profile &gt; 0.,fcj_profile,0.)\n    # Sometimes the FCJ profile returned is all zeros. We don't want to separate\n    # these out with loops etc, so need to deal with them. We'll replace zero entry\n    # with an array like this ([1, 0, 0, ..., 0]) which gives ([1, 1, 1, ..., 1])\n    # after the Fourier transform. Then then means that the pointwise multiplication\n    # of the FT Gaussian and Lorentzian components can still occur unaffected\n    zero_sum = (fcj_profile.sum(dim=-1) == 0).type(fcj_profile.dtype)\n    fcj_profile[:,:,0] += zero_sum\n    #first_zero = torch.zeros_like(x)\n    #first_zero[0] = 1\n    #batch_first_zero = (zero_sum * first_zero.unsqueeze(0))\n    #peak_FCJ += batch_first_zero\n    return fcj_profile\n\ndef get_UVWZ(batchsize, device, dtype, U_min=0.0001, U_max=0.0004,\n            V_min=0.0001, V_max=0.0004, W_min=0.0001, W_max=0.0004,\n            Z_min=0.0001, Z_max=0.0004):\n    \"\"\"\n    Get parameters for Gaussian HWHM. Defaults should give reasonable data to\n    resemble laboratory diffraction data\n    \"\"\"\n    U = ((torch.rand(batchsize, device=device, dtype=dtype) * (U_max-U_min))\n            + U_min).unsqueeze(1)\n\n    V = ((torch.rand(batchsize, device=device, dtype=dtype) * (V_max-V_min))\n            + V_min).unsqueeze(1)\n\n    W = ((torch.rand(batchsize, device=device, dtype=dtype) * (W_max-W_min))\n            + W_min).unsqueeze(1)\n\n    Z = ((torch.rand(batchsize, device=device, dtype=dtype) * (Z_max-Z_min))\n            + Z_min).unsqueeze(1)\n\n    return U, V, W, Z\n\ndef get_XY(batchsize, device, dtype, X_min=0.001, X_max=0.035,\n            Y_min=0.001,Y_max=0.035):\n    \"\"\"\n    Get parameters for Lorentzian HWHM. Defaults should give reasonable data to\n    resemble laboratory diffraction data\n    \"\"\"\n    X = ((torch.rand(batchsize, device=device, dtype=dtype) * (X_max-X_min))\n            + X_min).unsqueeze(1)\n\n    Y = ((torch.rand(batchsize, device=device, dtype=dtype) * (Y_max-Y_min))\n            + Y_min).unsqueeze(1)\n\n    return X, Y\n\ndef get_hwhm_G(tan_twotheta, cos_twotheta, U, V, W, Z):\n    \"\"\"Calculate Gaussian HWHM as a function of peak position and U,V,W and Z params\n\n    Args:\n        tan_twotheta (tensor): tangent of the twotheta peak positions\n        cos_twotheta (tensor): tangent of the twotheta peak positions\n        U (tensor): peakshape parameter U\n        V (tensor): peakshape parameter V\n        W (tensor): peakshape parameter W\n        Z (tensor): peakshape parameter Z\n\n    Returns:\n        tensor: HWHM for the gaussian peaks\n            Shape = (batch, number of peaks)\n    \"\"\"\n    tan_twotheta = tan_twotheta.squeeze()\n    cos_twotheta = cos_twotheta.squeeze()\n    return torch.sqrt((U * tan_twotheta**2) + (V * tan_twotheta)\n                        + W + (Z/(cos_twotheta**2))).unsqueeze(2)\n\ndef get_hwhm_L(tan_twotheta, cos_twotheta, X, Y):\n    \"\"\"Calculate Lorentzian HWHM as a function of peak position and X and Y params\n\n    Args:\n        tan_twotheta (tensor): tangent of the twotheta peak positions\n        cos_twotheta (tensor): tangent of the twotheta peak positions\n        X (tensor): peakshape parameter X\n        Y (tensor): peakshape parameter Y\n\n    Returns:\n        tensor: HWHM for the lorentzian peaks\n            Shape = (batch, number of peaks)\n    \"\"\"\n    tan_twotheta = tan_twotheta.squeeze()\n    cos_twotheta = cos_twotheta.squeeze()\n    return ((X * tan_twotheta) + (Y/cos_twotheta)).unsqueeze(2)\n\ndef get_shl(batchsize, device, dtype, shlmax=0.5, rescale=True):\n    \"\"\"Generate asymmetry parameter for the FCJ profile.\n\n    Args:\n        shl = sum(S/L,H/L) where:\n            S: sample height\n            H: detector opening\n            L: sample to detector opening distance\n        This is scaled by 1/57.2958 if rescale is True, in keeping with the scaling\n        applied in the original GSAS-II code\n\n    Returns:\n        tensor: The asymmetry parameter for the FCJ profiles\n    \"\"\"\n    shl = torch.rand((batchsize, 1, 1), device=device, dtype=dtype) * shlmax\n    shl /= 57.2958\n    return shl"
  },
  {
    "objectID": "posts/2023-09-08-Generating-synthetic-PXRD-data.html#background-and-noise",
    "href": "posts/2023-09-08-Generating-synthetic-PXRD-data.html#background-and-noise",
    "title": "Generating synthetic diffraction data from PowCod database",
    "section": "Background and noise",
    "text": "Background and noise\nThe experimental noise can be modelled as gaussian noise, which we can add very easily to the calculated diffraction data. For the background shape, we can use Chebyshev polynomials.\n\n\nShow the code\ndef get_noise(calculated_patterns, noise_min = 0.0001, noise_max = 0.0025):\n    \"\"\"Get noise for the diffraction patterns to simulate experimental data\n\n    Args:\n        calculated_patterns (tensor): The diffraction patterns.\n            Shape = (batch, points in profile)\n        noise_min (float, optional): Minimum standard deviation for the\n            Gaussian noise. Defaults to 0.0001.\n        noise_max (float, optional): Maximum standard deviation for the\n            Gaussian noise. Defaults to 0.0025.\n\n    Returns:\n        tensor: Noise to be added to the diffraction patterns\n    \"\"\"\n    batchsize = calculated_patterns.shape[0]\n    device = calculated_patterns.device\n    dtype = calculated_patterns.dtype\n    noise_std = torch.rand((batchsize,1), device=device, dtype=dtype) * (noise_max - noise_min) + noise_min\n    noise = torch.randn(calculated_patterns.shape, device=device, dtype=dtype) * noise_std\n    return noise\n\ndef get_background(batchsize, data, bg_prm_max=0.025, bg_prm_min=0.0, degree=8):\n    \"\"\"Calculate background profiles using Chebyshev polynomials\n\n    Args:\n        batchsize (int): The batch size\n        data (tensor): The twotheta values for the diffraction histograms\n        bg_prm_max (float, optional): Maximum value for the standard deviation\n            of the weights for the Chebyshev polynomial components. Defaults to\n            0.025.\n        bg_prm_min (float, optional): Minimum value for the standard deviation\n            of the weights for the Chebyshev polynomial components. Defaults to\n            0.0.\n        degree (int, optional): The degree of Chebyshev polynomial to use for\n            the backgrounds. Defaults to 8.\n\n    Returns:\n        tensor: Background profiles. Shape = (batch, number of points in histogram)\n    \"\"\"\n    device = data.device\n    dtype = data.dtype\n    n = torch.arange(degree,device=device,dtype=dtype).unsqueeze(1)\n    # Scale data into range -1 to +1\n    ttstar = 2*(data - data.min())/(data.max() - data.min()) - 1\n    chebyshev_basis = torch.cos(n*torch.arccos(ttstar))\n    params = (((torch.rand((batchsize,1,1), device=device, dtype=dtype)\n                * (bg_prm_max - bg_prm_min)) + bg_prm_min)\n                * torch.randn((batchsize, chebyshev_basis.shape[0], 1),\n                device=device, dtype=dtype))\n    bg = (params * chebyshev_basis).sum(dim=1)\n    return bg\n\n\nA quick test to see if these functions work and generate sensible looking noise and backgrounds:\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\ndata = torch.linspace(4,44,2048).float()\n\nbatchsize = 4\nbgs = get_background(batchsize, data, degree=10)\nbgs -= bgs.min(dim=-1).values.unsqueeze(1)\n\nnoise = get_noise(bgs)\n\nfig, ax = plt.subplots(1,1,figsize=(10,6))\nfor bg in (bgs+noise):\n    ax.plot(data, bg)\nax.set_xlabel(\"$2\\\\theta$\")\nax.set_title(\"Randomly generated backgrounds with noise\")\nplt.show()\n\n\n\n\n\nSome funky background shapes are possible with randomly generated Chebyshev polynomials! However, these look reasonable to me.\nWe now need functions to tie everything together and start to generate some data\n\n\nShow the code\ndef get_peak_positions(crystal_systems, hkl, intensities, unit_cells,\n                    perturbation_stddev=0.05, zpemin=0.03, zpemax=0.03, wavelength=1.54056):\n    batchsize = intensities.shape[0]\n    dtype = intensities.dtype\n    device = intensities.device\n\n    cell_perturbation = get_unit_cell_perturbation(crystal_systems,\n                                    dtype=dtype, stddev=perturbation_stddev)\n    new_unit_cells = unit_cells + cell_perturbation\n    lattice_matrix, valid = get_lattice_matrix(new_unit_cells)\n\n    # Get rid of any invalid unit cells after perturbation\n    if valid.sum() != valid.shape[0]:\n        import warnings\n        warnings.warn(\"Invalid cells generated\")\n        lattice_matrix = lattice_matrix[valid]\n        hkl = hkl[valid]\n        intensities = intensities[valid]\n        batchsize = intensities.shape[0]\n    reciprocal_lattice_matrix = torch.linalg.inv(lattice_matrix)\n    reciprocal_lattice_metric_tensor = get_recip_lattice_metric_tensor(reciprocal_lattice_matrix)\n    d_spacing = get_d_spacing(reciprocal_lattice_metric_tensor, hkl)\n    zpe = get_zero_point_error(batchsize, device, dtype, zpemin=zpemin, zpemax=zpemax)\n    twotheta = zpe + d_to_tt(d_spacing, wavelength)\n\n    return twotheta, reciprocal_lattice_metric_tensor, hkl, intensities, d_spacing\n\ndef get_PO_intensities(hkl, reciprocal_lattice_metric_tensor, dspacing, intensities, PO_std=0.1):\n    # Now apply PO perturbation to the peak intensities\n    cosP, sinP, MDfactor, PO_axis = get_MD_PO_components(hkl,\n                                    reciprocal_lattice_metric_tensor, dspacing, factor_std=PO_std)\n    intensities = apply_MD_PO_correction(intensities, cosP, sinP, MDfactor)\n    return torch.nan_to_num(intensities)\n\ndef get_peak_shape_params(twotheta, U_min=0.0001, U_max=0.0004,\n                        V_min=0.0001, V_max=0.0004, W_min=0.0001, W_max=0.0004,\n                        Z_min=0.0001, Z_max=0.0004, X_min=0.001, X_max=0.035,\n                        Y_min=0.001, Y_max=0.035, shlmax=0.5):\n    batchsize = twotheta.shape[0]\n    dtype = twotheta.dtype\n    device = twotheta.device\n    tan_twotheta = torch.tan(twotheta*torch.pi/180.)\n    cos_twotheta = torch.cos(twotheta*torch.pi/180.)\n    U, V, W, Z = get_UVWZ(batchsize, device, dtype, U_min=U_min, U_max=U_max,\n                        V_min=V_min, V_max=V_max, W_min=W_min, W_max=W_max,\n                        Z_min=Z_min, Z_max=Z_max)\n    X, Y = get_XY(batchsize, device, dtype, X_min=X_min, X_max=X_max,\n                        Y_min=Y_min,Y_max=Y_max)\n    hwhm_gaussian = get_hwhm_G(tan_twotheta, cos_twotheta, U, V, W, Z)\n    hwhm_lorentzian = get_hwhm_L(tan_twotheta, cos_twotheta, X, Y)\n    shl = get_shl(batchsize, device, dtype, shlmax=shlmax, rescale=True)\n    return hwhm_gaussian, hwhm_lorentzian, shl\n\ndef calculate_peaks(x, twotheta, intensities, hwhm_gaussian, hwhm_lorentzian, shl):\n    peak_G =  gaussian(x, torch.zeros_like(twotheta), hwhm_gaussian)\n    peak_L = lorentzian(x, torch.zeros_like(twotheta), hwhm_lorentzian)\n    peak_FCJ = fcj(x,twotheta, shl)\n    peak_GLF = torch.stack([peak_G,peak_L,peak_FCJ],dim=1)\n    prod_FT_GLF = torch.fft.fft(peak_GLF).prod(dim=1)\n    peak_voigt = torch.fft.ifft(prod_FT_GLF).real\n    zero_sum = peak_FCJ.sum(dim=-1) == 1\n    peak_voigt[zero_sum.squeeze()] = torch.fft.ifftshift(peak_voigt[zero_sum.squeeze()], dim=-1)\n    peak_voigt /= peak_voigt.max(dim=2).values.unsqueeze(2)\n    peak_voigt *= intensities\n    return peak_voigt\n\ndef calculate_full_patterns(x, full_data, twotheta, peak_voigt, ttmin=4., ttmax=44.):\n    # Finally calculate the full diffraction pattern\n    twotheta[twotheta == 0] = torch.inf\n    twotheta[twotheta &lt; 4] = torch.inf\n    twotheta[twotheta &gt; 44] = torch.inf\n    peakidx = torch.abs((x[0] + twotheta) - full_data).min(dim=-1).indices\n    full_pattern = torch.zeros(list(peakidx.shape)+[full_data.shape[0]], device=device, dtype=dtype)\n    full_pattern = full_pattern.scatter_(2,\n                    peakidx.unsqueeze(2) + torch.arange(x.shape[0], device=device), peak_voigt*torch.isfinite(twotheta))\n\n    full_pattern = full_pattern.sum(dim=1)\n    full_pattern /= full_pattern.max(dim=1).values.unsqueeze(1)\n    full_pattern = full_pattern[:,(full_data &gt;= ttmin) & (full_data &lt;= ttmax)]\n    return full_pattern\n\ndef calculate_diffraction_patterns(x, full_data, crystal_systems, hkl, intensities, unit_cells, wavelength=1.54056):\n    \"\"\"\n    Expect the input tensors to have their first dimension to be of size batchsize\n    \"\"\"\n\n    twotheta, reciprocal_lattice_metric_tensor, hkl, intensities, d_spacing = get_peak_positions(crystal_systems, hkl, intensities, unit_cells,\n                    perturbation_stddev=0.05, zpemin=0.03, zpemax=0.03, wavelength=wavelength)\n\n    twotheta = twotheta.unsqueeze(2)\n\n    mod_intensities = get_PO_intensities(hkl, reciprocal_lattice_metric_tensor, d_spacing, intensities).unsqueeze(2)\n\n    hwhm_gaussian, hwhm_lorentzian, shl = get_peak_shape_params(twotheta)\n\n    peak_voigt = calculate_peaks(x, twotheta, mod_intensities, hwhm_gaussian, hwhm_lorentzian, shl)\n\n    calculated_patterns = calculate_full_patterns(x, full_data, twotheta, peak_voigt, ttmin=ttmin, ttmax=ttmax)\n\n    bgs = get_background(calculated_patterns.shape[0], \n                        full_data[(full_data &gt;= ttmin) & (full_data &lt;= ttmax)],\n                        degree=10)\n    noise = get_noise(calculated_patterns)\n\n    calculated_patterns += bgs + noise\n    calculated_patterns -= calculated_patterns.min(dim=1).values.unsqueeze(1)\n    calculated_patterns /= calculated_patterns.max(dim=1).values.unsqueeze(1)\n\n    return calculated_patterns\n\n\nLet’s test the functions we’ve written using some real diffraction data as a comparison point. We’ll first plot the actual diffraction pattern, then plot several simulated datasets calculated from the unit cell, hkls and intensities extracted by DASH. This will show the effect of the data augmentation we’ve added into the code which modulates the positions, intensities and shapes of the peaks.\nThe dataset we’ll look at is that for verapamil hydrochloride. We’ll need a function to calculate the Lorentz-polarisation factor for the intensities, as this would have been corrected for in the intensities in the DASH hcv.\n\n\nShow the code\nv_xye = []\nwith open(\"./files/Verap.xye\", \"r\") as f:\n    for line in f:\n        line = list(filter(None, line.strip().split(\" \")))\n        v_xye.append([float(x) for x in line])\nv_xye = np.array(v_xye)\nv_tt, v_i_obs = v_xye[:,0], v_xye[:,1]\nv_i_obs /= v_i_obs.max()\n\n\n\n\nShow the code\ndef lp_factor(twotheta, ttmono=torch.tensor([28])):\n    A = torch.cos(ttmono*torch.pi/180)**2\n    cossqdtt = torch.cos(twotheta*torch.pi/180)**2\n    sintt = torch.sin(twotheta*torch.pi/180)\n    return (1+A*cossqdtt) / ((1+A)*sintt)\n\n\n\n\nShow the code\nimport matplotlib.pyplot as plt\ndevice = torch.device(\"cpu\")\ndtype = torch.float32\nttmin = 4\nttmax = 44\npeakrange = 3.\ndatadim = 2048\nfull_data = torch.linspace(ttmin-(peakrange/2), ttmax+(peakrange/2), int(np.ceil((ttmax-ttmin+peakrange)/((ttmax-ttmin)/datadim))), device=device, dtype=dtype)\nplotdata = full_data[(full_data &gt;= ttmin) & (full_data &lt;= ttmax)].cpu()\nx = (full_data[full_data &lt;= ttmin+(peakrange/2)]).clone() - 4\n\nv_hkl = []\nv_i = []\nwith open(\"./files/Verap.hcv\", \"r\") as f:\n    for line in f:\n        line = list(filter(None, line.strip().split(\" \")))\n        v_hkl.append([int(x) for x in line[0:3]])\n        v_i.append(float(line[3]))\nv_hkl = torch.tensor(v_hkl).float()\nv_i = torch.tensor(v_i).float()\nv_cell = torch.tensor([7.08991, 10.59464, 19.20684, 100.1068, 93.7396, 101.5610])\nv_cs = torch.tensor([cs_key[\"Monoclinic\"]])\n\nrepeat = 4\ndiffraction_patterns = calculate_diffraction_patterns(x, full_data, v_cs.repeat(repeat), v_hkl.repeat(repeat,1,1), v_i.repeat(repeat,1), v_cell.repeat(repeat,1))\nplt.plot(v_tt[v_tt &lt;= 35], v_i_obs[v_tt &lt;= 35])\nplt.title(\"Original verapamil hydrochloride diffraction data\")\nplt.xlabel(\"$2\\\\theta$\")\nplt.ylabel(\"Intensity\")\nplt.show()\nfig, ax = plt.subplots(2,2,figsize=(16,8))\nfor i, p, in enumerate(diffraction_patterns):\n    col = i % 2\n    row = i // 2\n    corrected_i = (lp_factor(plotdata[plotdata &lt;= 35])*p[plotdata &lt;= 35]).squeeze()\n    corrected_i -= corrected_i.min()\n    corrected_i /= corrected_i.max()\n    ax[row][col].plot(plotdata[plotdata &lt;= 35], corrected_i)\n    ax[row][col].set_xlabel(\"$2\\\\theta$\")\n    ax[row][col].set_ylabel(\"Intensity\")\nfig.suptitle(\"Calculated diffraction patterns\")\nplt.show()"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Matthew J. Almond, Mark J. Spillman and Elizabeth M. Page. “Workbook in Inorganic Chemistry”, Oxford University Press (2017). ISBN: 9780198729501. Publisher\n\n\nMark J. Spillman, Daniel Nicholls and Kenneth Shankland. “Experimental Analysis of Powder Diffraction Data.” (2020). doi: 10.1142/9789811204579_0001, Chapter 1 in “Handbook on Big Data and Machine Learning in the Physical Sciences”, World Scientific (2020)"
  },
  {
    "objectID": "publications.html#books",
    "href": "publications.html#books",
    "title": "Publications",
    "section": "",
    "text": "Matthew J. Almond, Mark J. Spillman and Elizabeth M. Page. “Workbook in Inorganic Chemistry”, Oxford University Press (2017). ISBN: 9780198729501. Publisher\n\n\nMark J. Spillman, Daniel Nicholls and Kenneth Shankland. “Experimental Analysis of Powder Diffraction Data.” (2020). doi: 10.1142/9789811204579_0001, Chapter 1 in “Handbook on Big Data and Machine Learning in the Physical Sciences”, World Scientific (2020)"
  },
  {
    "objectID": "publications.html#articles",
    "href": "publications.html#articles",
    "title": "Publications",
    "section": "Articles",
    "text": "Articles\nMark J. Spillman, Norman Shankland and Kenneth Shankland. “An efficient treatment of ring conformations during molecular crystal structure determination from powder diffraction data.” CrystEngComm (2022) doi: 10.1039/D2CE00520D\nMark J. Spillman and Kenneth Shankland. “GALLOP: accelerated molecular crystal structure determination from powder diffraction data.” CrystEngComm (2021) doi: 10.1039/d1ce00978h\nOkba Al Rahal, Mridul Majumder, Mark J. Spillman, Jacco van de Streek and Kenneth Shankland. “Co-Crystal Structures of Furosemide:Urea and Carbamazepine:Indomethacin Determined from Powder X-Ray Diffraction Data.” Crystals (2020). doi: 10.3390/cryst10010042\nDaniel Nicholls, Kenneth Shankland, Mark J. Spillman and Carole J. Elleman. “Rietveld-Based Quantitative Phase Analysis of Sugars in Confectionery.” Food Analytical Methods (2018) doi: 10.1007/s12161-018-1243-9\nPhilippa B. Cranwell, Fred J. Davis, Joanne M. Elliott, John E. Mckendrick, Elizabeth M. Page and Mark J. Spillman. “Encouraging Independent Thought and Learning in First Year Practical Classes.” New directions in the teaching of physical sciences (2017) doi: 10.29311/NDTPS.V0I12.674\nAmanda R. Buist, David S. Edgeley, Elena A. Kabova, Alan R. Kennedy, Debbie Hooper, David G. Rollo, Kenneth Shankland, and Mark J. Spillman. “Salt and Ionic Cocrystalline Forms of Amides: Protonation of Carbamazepine in Aqueous Media.” Crystal Growth & Design (2015) doi: 10.1021/acs.cgd.5b01223\nMark J. Spillman, Kenneth Shankland, Adrian C. Williams and Jason C. Cole. “CDASH: a cloud-enabled program for structure solution from powder diffraction data.” Journal of Applied Crystallography (2015) doi: 10.1107/S160057671502049X\nKenneth Shankland, Mark J. Spillman, Elena A. Kabova, David S. Edgeley and Norman Shankland. “The Principles Underlying the Use of Powder Diffraction Data in Solving Pharmaceutical Crystal Structures.” Acta Crystallographica Section C (2013) doi: 10.1107/S0108270113028643\nAmanda R. Buist, Alan R. Kennedy, Kenneth Shankland, Norman Shankland and Mark J. Spillman. “Salt Forms of Amides: Protonation and Polymorphism of Carbamazepine and Cytenamide.” Crystal Growth & Design (2013) doi: 10.1021/cg401341y\nMridul Majumder, Graham Buckton, Clare F. Rawlinson-Malone, Adrian C. Williams, Mark J. Spillman, Elna Pidcock and Kenneth Shankland. “Application of Hydrogen-Bond Propensity Calculations to an Indomethacin Nicotinamide (1 : 1) Co-Crystal.” CrystEngComm (2013) doi: 10.1039/C3CE40367J\nMridul Majumder, Graham Buckton, Clare F. Rawlinson-Malone, Adrian C Williams, Mark J. Spillman, Norman Shankland and Kenneth Shankland. “A Carbamazepine-Indomethacin (1 : 1) Cocrystal Produced by Milling.” CrystEngComm (2011) doi: 10.1039/C1CE05650F"
  }
]